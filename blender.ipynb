{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blender",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM9YAgwNPgH6xPTjdJO9l+o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newcodevelop/DL-Adventures/blob/master/blender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNj-qOG90Epu",
        "colab_type": "code",
        "outputId": "16640be2-da88-4076-c487-75579ecf1228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python parlai/scripts/safe_interactive.py -t blended_skill_talk -mf zoo:blender/blender_3B/model"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                                 /@&%###%&&@@#\n",
            "                      .,*/((((##@@@&%%#%%&&@@@&%%#/*.\n",
            "             #@@&&&%%%%##(((///*****//(((###%%%&&&@@@@@&&%#%%#.\n",
            "         .%&@@@@@&&&%%%####((((////((((####%%%&&&@@@@@&&%%#%%####,\n",
            "           ./,,#(//**,,.....,,,,***////((((########%%%%%%%%###(((\n",
            "              /*(//**,,,....,,,,***////((((########%%%%%%%%###(#%*\n",
            "               (*,...      ...,,,***//////((((((///////(/*...,/#@@@(\n",
            "               **,,..         ...,,,,,,,,,,........,,*///*...*(#@@@@&&*\n",
            "               ./,,..          ...,,,,,,,,,........,,*//*,...*#/,,,,,/%#\n",
            "                (*,..          ...,,,,,,,,,........,,*//*,..,/(      .,#(\n",
            "                **,..          ...,,,,,,,,,.........,*//*,..,((       .,(#\n",
            "                 /*,..          ....,,,,,,,.....  ..,***,,,,(#         ..#&\n",
            "                 **,..          ....,,,,,,,....   ..,***,,,*#.         .,%@\n",
            "                 ./,...          B L E N D E R    ...***,,,*#          .*%@\n",
            "                  /*,..          ...,,,,,,,....    .,**,,,,/#         ..(%/\n",
            "                  /*,,..         ...,,,,,,,...    ..,*,,,,,(.         ..#&\n",
            "                  ,/*,..         ...,,,,,,,...    ..,*,,,,*#         ..*%(\n",
            "                   /*,..         ...,,,,,.....    ..,*,*,,/(         ..#&\n",
            "                   /**,..        ...,,,,.....    ...,***,*(.       ,,(%.\n",
            "                    (/*,,..      ....,,.....     ...,****(&@@@&&&#,\n",
            "                     (/*,,...   .....,,......     ..,****#@,\n",
            "                     *(/*,,/....*(###%(,(%%##(*.  ./,,**(\n",
            "                      ,//**(,........,/((#.........*,**(\n",
            "                      .(#//*,,,,,,.*.,/((%/,,.....,,*/@\n",
            "                    ((######//****,/.,/(#%#***,***(&@@@@@(\n",
            "                   *&%%#####%%%%%%%#//(#%&%%&&@@@@@@@@@@@@*\n",
            "                   &&%%%###((((((####%%%%&&&&&@@@@@@@@@@&&@.\n",
            "                  *##%%%##(((((((####%%%%%&&&&@@@@@@@@@&#/*,\n",
            "                 .(##%#/,  .,*((##%%%&&&&%%%#####%&&@&&%#(/*.\n",
            "                 /(###(,   .,*/(##%%%&&&&%%%######%&&&&%#(/*,\n",
            "                */((((*.  ..,//((##%%%%%%%%#######%&&&&%%#(/*,\n",
            "               .//(((/,   .,*//((###%%%%%%########%%&&&%%#((/,.\n",
            "              .&####(((((((((######%%%%%%%%&&&&&&&@@@@@@@@@@@@@#\n",
            "               *&#.   .*/((((#######%%%%%%&&&&&&&@@@@@#/.   (&\n",
            "[building data: /content/ParlAI/data/models/blender/blender_3B/BST3B.tgz]\n",
            "[ downloading: http://parl.ai/downloads/_models/blender/BST3B.tgz to /content/ParlAI/data/models/blender/blender_3B/BST3B.tgz ]\n",
            "Downloading BST3B.tgz:   9% 436M/4.95G [00:22<03:32, 21.2MB/s]Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/ssl.py\", line 874, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "  File \"/usr/lib/python3.6/ssl.py\", line 631, in read\n",
            "    v = self._sslobj.read(len, buffer)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"parlai/scripts/safe_interactive.py\", line 85, in <module>\n",
            "    safe_interactive(parser.parse_args(print_args=False), print_parser=parser)\n",
            "  File \"/content/ParlAI/parlai/core/params.py\", line 991, in parse_args\n",
            "    self.add_extra_args(args)\n",
            "  File \"/content/ParlAI/parlai/core/params.py\", line 829, in add_extra_args\n",
            "    model = get_model_name(parsed)\n",
            "  File \"/content/ParlAI/parlai/core/params.py\", line 123, in get_model_name\n",
            "    model_file = modelzoo_path(opt.get('datapath'), model_file)\n",
            "  File \"/content/ParlAI/parlai/core/build_data.py\", line 423, in modelzoo_path\n",
            "    my_module.download(datapath)\n",
            "  File \"/content/ParlAI/parlai/zoo/blender/blender_3B.py\", line 17, in download\n",
            "    build(datapath, 'BST3B.tgz', model_type='blender_3B', version=VERSION)\n",
            "  File \"/content/ParlAI/parlai/zoo/blender/build.py\", line 29, in build\n",
            "    download_models(opt, [fname], 'blender', version=version, use_model_type=False)\n",
            "  File \"/content/ParlAI/parlai/core/build_data.py\", line 386, in download_models\n",
            "    download(url, dpath, fname)\n",
            "  File \"/content/ParlAI/parlai/core/build_data.py\", line 196, in download\n",
            "    for chunk in response.iter_content(CHUNK_SIZE):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/requests/models.py\", line 751, in generate\n",
            "    for chunk in self.raw.stream(chunk_size, decode_content=True):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/response.py\", line 496, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/urllib3/response.py\", line 444, in read\n",
            "    data = self._fp.read(amt)\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 459, in read\n",
            "    n = self.readinto(b)\n",
            "  File \"/usr/lib/python3.6/http/client.py\", line 503, in readinto\n",
            "    n = self.fp.readinto(b)\n",
            "  File \"/usr/lib/python3.6/socket.py\", line 586, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.6/ssl.py\", line 1012, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.6/ssl.py\", line 874, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "KeyboardInterrupt\n",
            "Downloading BST3B.tgz:   9% 436M/4.95G [00:22<03:49, 19.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QLPCGdW0F2_",
        "colab_type": "code",
        "outputId": "b699a185-63ae-46d4-e50b-c63cde063e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/facebookresearch/ParlAI.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ParlAI'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 30191 (delta 0), reused 1 (delta 0), pack-reused 30187\u001b[K\n",
            "Receiving objects: 100% (30191/30191), 58.45 MiB | 36.90 MiB/s, done.\n",
            "Resolving deltas: 100% (21492/21492), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWtAnZyT0aXa",
        "colab_type": "code",
        "outputId": "49d82fef-3511-498b-cb14-e621c36a97f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd ParlAI \n",
        "!ls\n",
        "!python setup.py develop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ParlAI\n",
            "autoformat.sh\t    example_parlai_internal  parlai\t       setup.py\n",
            "codecov.yml\t    examples\t\t     projects\t       tests\n",
            "CODE_OF_CONDUCT.md  LICENSE\t\t     pyproject.toml    website\n",
            "conftest.py\t    MANIFEST.in\t\t     pytest.ini\n",
            "CONTRIBUTING.md     mypy.ini\t\t     README.md\n",
            "docs\t\t    NEWS.md\t\t     requirements.txt\n",
            "running develop\n",
            "running egg_info\n",
            "creating parlai.egg-info\n",
            "writing parlai.egg-info/PKG-INFO\n",
            "writing dependency_links to parlai.egg-info/dependency_links.txt\n",
            "writing entry points to parlai.egg-info/entry_points.txt\n",
            "writing requirements to parlai.egg-info/requires.txt\n",
            "writing top-level names to parlai.egg-info/top_level.txt\n",
            "writing manifest file 'parlai.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'parlai.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.6/dist-packages/parlai.egg-link (link to .)\n",
            "Adding parlai 0.1.20200504 to easy-install.pth file\n",
            "\n",
            "Installed /content/ParlAI\n",
            "Processing dependencies for parlai==0.1.20200504\n",
            "Searching for websocket-server==0.4\n",
            "Reading https://pypi.org/simple/websocket-server/\n",
            "Downloading https://files.pythonhosted.org/packages/74/64/e86581ee7775a2e08aca530b41e1a1e3ee6b320233b1eff301dcb86d1636/websocket_server-0.4.tar.gz#sha256=91cd4b565d1e1b00ef107abcb2840a8090868b19543f3b38e1962d5f975d0c04\n",
            "Best match: websocket-server 0.4\n",
            "Processing websocket_server-0.4.tar.gz\n",
            "Writing /tmp/easy_install-v62_qeih/websocket_server-0.4/setup.cfg\n",
            "Running websocket_server-0.4/setup.py -q bdist_egg --dist-dir /tmp/easy_install-v62_qeih/websocket_server-0.4/egg-dist-tmp-854hmdzh\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "Moving websocket_server-0.4-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding websocket-server 0.4 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/websocket_server-0.4-py3.6.egg\n",
            "Searching for websocket-client==0.56.0\n",
            "Reading https://pypi.org/simple/websocket-client/\n",
            "Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl#sha256=1151d5fb3a62dc129164292e1227655e4bbc5dd5340a5165dfae61128ec50aa9\n",
            "Best match: websocket-client 0.56.0\n",
            "Processing websocket_client-0.56.0-py2.py3-none-any.whl\n",
            "Installing websocket_client-0.56.0-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding websocket-client 0.56.0 to easy-install.pth file\n",
            "Installing wsdump.py script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/websocket_client-0.56.0-py3.6.egg\n",
            "Searching for Unidecode==1.1.1\n",
            "Reading https://pypi.org/simple/Unidecode/\n",
            "Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl#sha256=1d7a042116536098d05d599ef2b8616759f02985c85b4fef50c78a5aaf10822a\n",
            "Best match: Unidecode 1.1.1\n",
            "Processing Unidecode-1.1.1-py2.py3-none-any.whl\n",
            "Installing Unidecode-1.1.1-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding Unidecode 1.1.1 to easy-install.pth file\n",
            "Installing unidecode script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/Unidecode-1.1.1-py3.6.egg\n",
            "Searching for typing-extensions==3.7.4.1\n",
            "Reading https://pypi.org/simple/typing-extensions/\n",
            "Downloading https://files.pythonhosted.org/packages/03/92/705fe8aca27678e01bbdd7738173b8e7df0088a2202c80352f664630d638/typing_extensions-3.7.4.1-py3-none-any.whl#sha256=cf8b63fedea4d89bab840ecbb93e75578af28f76f66c35889bd7065f5af88575\n",
            "Best match: typing-extensions 3.7.4.1\n",
            "Processing typing_extensions-3.7.4.1-py3-none-any.whl\n",
            "Installing typing_extensions-3.7.4.1-py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding typing-extensions 3.7.4.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/typing_extensions-3.7.4.1-py3.6.egg\n",
            "Searching for tqdm==4.36.1\n",
            "Reading https://pypi.org/simple/tqdm/\n",
            "Downloading https://files.pythonhosted.org/packages/e1/c1/bc1dba38b48f4ae3c4428aea669c5e27bd5a7642a74c8348451e0bd8ff86/tqdm-4.36.1-py2.py3-none-any.whl#sha256=dd3fcca8488bb1d416aa7469d2f277902f26260c45aa86b667b074cd44b3b115\n",
            "Best match: tqdm 4.36.1\n",
            "Processing tqdm-4.36.1-py2.py3-none-any.whl\n",
            "Installing tqdm-4.36.1-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding tqdm 4.36.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/tqdm-4.36.1-py3.6.egg\n",
            "Searching for tokenizers==0.4.2\n",
            "Reading https://pypi.org/simple/tokenizers/\n",
            "Downloading https://files.pythonhosted.org/packages/2e/0a/096a279ee20a206a58ba9af3251c7271fb9493bed3bf29bfe4c58fcbfe4c/tokenizers-0.4.2-cp36-cp36m-manylinux1_x86_64.whl#sha256=da8e1593ef2a73cb0d2ebaf06183e8acd304ef1a272c526884a686ded9cf4d27\n",
            "Best match: tokenizers 0.4.2\n",
            "Processing tokenizers-0.4.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Installing tokenizers-0.4.2-cp36-cp36m-manylinux1_x86_64.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding tokenizers 0.4.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/tokenizers-0.4.2-py3.6-linux-x86_64.egg\n",
            "Searching for sphinx-autodoc-typehints==1.10.3\n",
            "Reading https://pypi.org/simple/sphinx-autodoc-typehints/\n",
            "Downloading https://files.pythonhosted.org/packages/00/83/87b8890a93b3994b49960716009c1effb6f7a1fef3a1ec553fda2a7c84de/sphinx_autodoc_typehints-1.10.3-py3-none-any.whl#sha256=27c9e6ef4f4451766ab8d08b2d8520933b97beb21c913f3df9ab2e59b56e6c6c\n",
            "Best match: sphinx-autodoc-typehints 1.10.3\n",
            "Processing sphinx_autodoc_typehints-1.10.3-py3-none-any.whl\n",
            "Installing sphinx_autodoc_typehints-1.10.3-py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding sphinx-autodoc-typehints 1.10.3 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/sphinx_autodoc_typehints-1.10.3-py3.6.egg\n",
            "Searching for sphinx_rtd_theme==0.4.3\n",
            "Reading https://pypi.org/simple/sphinx_rtd_theme/\n",
            "Downloading https://files.pythonhosted.org/packages/60/b4/4df37087a1d36755e3a3bfd2a30263f358d2dea21938240fa02313d45f51/sphinx_rtd_theme-0.4.3-py2.py3-none-any.whl#sha256=00cf895504a7895ee433807c62094cf1e95f065843bf3acd17037c3e9a2becd4\n",
            "Best match: sphinx-rtd-theme 0.4.3\n",
            "Processing sphinx_rtd_theme-0.4.3-py2.py3-none-any.whl\n",
            "Installing sphinx_rtd_theme-0.4.3-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding sphinx-rtd-theme 0.4.3 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/sphinx_rtd_theme-0.4.3-py3.6.egg\n",
            "Searching for Sphinx==2.2.0\n",
            "Reading https://pypi.org/simple/Sphinx/\n",
            "Downloading https://files.pythonhosted.org/packages/8e/4c/95a21788db2e1653e931420f561015a0bbc9bd4660c4520467ab9e733eb2/Sphinx-2.2.0-py3-none-any.whl#sha256=839a3ed6f6b092bb60f492024489cc9e6991360fb9f52ed6361acd510d261069\n",
            "Best match: Sphinx 2.2.0\n",
            "Processing Sphinx-2.2.0-py3-none-any.whl\n",
            "Installing Sphinx-2.2.0-py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding Sphinx 2.2.0 to easy-install.pth file\n",
            "Installing sphinx-apidoc script to /usr/local/bin\n",
            "Installing sphinx-autogen script to /usr/local/bin\n",
            "Installing sphinx-build script to /usr/local/bin\n",
            "Installing sphinx-quickstart script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/Sphinx-2.2.0-py3.6.egg\n",
            "Searching for sh==1.12.14\n",
            "Reading https://pypi.org/simple/sh/\n",
            "Downloading https://files.pythonhosted.org/packages/4a/22/17b22ef5b049f12080f5815c41bf94de3c229217609e469001a8f80c1b3d/sh-1.12.14-py2.py3-none-any.whl#sha256=ae3258c5249493cebe73cb4e18253a41ed69262484bad36fdb3efcb8ad8870bb\n",
            "Best match: sh 1.12.14\n",
            "Processing sh-1.12.14-py2.py3-none-any.whl\n",
            "Installing sh-1.12.14-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding sh 1.12.14 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/sh-1.12.14-py3.6.egg\n",
            "Searching for requests-mock==1.7.0\n",
            "Reading https://pypi.org/simple/requests-mock/\n",
            "Downloading https://files.pythonhosted.org/packages/8c/f1/66c54a412543b29454102ae74b1454fce2d307b1c36e6bd2e9818394df88/requests_mock-1.7.0-py2.py3-none-any.whl#sha256=510df890afe08d36eca5bb16b4aa6308a6f85e3159ad3013bac8b9de7bd5a010\n",
            "Best match: requests-mock 1.7.0\n",
            "Processing requests_mock-1.7.0-py2.py3-none-any.whl\n",
            "Installing requests_mock-1.7.0-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding requests-mock 1.7.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/requests_mock-1.7.0-py3.6.egg\n",
            "Searching for requests==2.22.0\n",
            "Reading https://pypi.org/simple/requests/\n",
            "Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl#sha256=9cf5292fcd0f598c671cfc1e0d7d1a7f13bb8085e9a590f48c010551dc6c4b31\n",
            "Best match: requests 2.22.0\n",
            "Processing requests-2.22.0-py2.py3-none-any.whl\n",
            "Installing requests-2.22.0-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding requests 2.22.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/requests-2.22.0-py3.6.egg\n",
            "Searching for recommonmark==0.6.0\n",
            "Reading https://pypi.org/simple/recommonmark/\n",
            "Downloading https://files.pythonhosted.org/packages/94/de/334aaf73df8c0e77fb07f883d1e274344526196c137ef3479cb5e5aef086/recommonmark-0.6.0-py2.py3-none-any.whl#sha256=2ec4207a574289355d5b6ae4ae4abb29043346ca12cdd5f07d374dc5987d2852\n",
            "Best match: recommonmark 0.6.0\n",
            "Processing recommonmark-0.6.0-py2.py3-none-any.whl\n",
            "Installing recommonmark-0.6.0-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding recommonmark 0.6.0 to easy-install.pth file\n",
            "Installing cm2html script to /usr/local/bin\n",
            "Installing cm2latex script to /usr/local/bin\n",
            "Installing cm2man script to /usr/local/bin\n",
            "Installing cm2pseudoxml script to /usr/local/bin\n",
            "Installing cm2xetex script to /usr/local/bin\n",
            "Installing cm2xml script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/recommonmark-0.6.0-py3.6.egg\n",
            "Searching for regex==2019.8.19\n",
            "Reading https://pypi.org/simple/regex/\n",
            "Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz#sha256=587b62d48ca359d2d4f02d486f1f0aa9a20fbaf23a9d4198c4bed72ab2f6c849\n",
            "Best match: regex 2019.8.19\n",
            "Processing regex-2019.08.19.tar.gz\n",
            "Writing /tmp/easy_install-autwekma/regex-2019.08.19/setup.cfg\n",
            "Running regex-2019.08.19/setup.py -q bdist_egg --dist-dir /tmp/easy_install-autwekma/regex-2019.08.19/egg-dist-tmp-uh6g6y4q\n",
            "BASE_DIR is /tmp/easy_install-autwekma/regex-2019.08.19\n",
            "/usr/local/lib/python3.6/dist-packages/setuptools/dist.py:454: UserWarning: Normalizing '2019.08.19' to '2019.8.19'\n",
            "  warnings.warn(tmpl.format(**locals()))\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfolded_char_at\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:10625:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kfolded_len\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kfolded_len\u001b[m\u001b[K;\n",
            "         \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfuzzy_match_group_fld\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11503:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfuzzy_match_string_fld\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11270:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbasic_match\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11608:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11522:18:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ was declared here\n",
            "     RE_FuzzyData \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K;\n",
            "                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11369:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11288:18:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ was declared here\n",
            "     RE_FuzzyData \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K;\n",
            "                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "regex.__pycache__._regex.cpython-36: module references __file__\n",
            "creating /usr/local/lib/python3.6/dist-packages/regex-2019.8.19-py3.6-linux-x86_64.egg\n",
            "Extracting regex-2019.8.19-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding regex 2019.8.19 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/regex-2019.8.19-py3.6-linux-x86_64.egg\n",
            "Searching for pyzmq==18.1.0\n",
            "Reading https://pypi.org/simple/pyzmq/\n",
            "Downloading https://files.pythonhosted.org/packages/75/89/6f0ea51ffa9c2c00c0ab0460f137b16a5ab5b47e3b060c5b1fc9ca425836/pyzmq-18.1.0-cp36-cp36m-manylinux1_x86_64.whl#sha256=b645a49376547b3816433a7e2d2a99135c8e651e50497e7ecac3bd126e4bea16\n",
            "Best match: pyzmq 18.1.0\n",
            "Processing pyzmq-18.1.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Installing pyzmq-18.1.0-cp36-cp36m-manylinux1_x86_64.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding pyzmq 18.1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/pyzmq-18.1.0-py3.6-linux-x86_64.egg\n",
            "Searching for pyyaml==5.1\n",
            "Reading https://pypi.org/simple/pyyaml/\n",
            "Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz#sha256=436bc774ecf7c103814098159fbb84c2715d25980175292c648f2da143909f95\n",
            "Best match: PyYAML 5.1\n",
            "Processing PyYAML-5.1.tar.gz\n",
            "Writing /tmp/easy_install-il40tn_2/PyYAML-5.1/setup.cfg\n",
            "Running PyYAML-5.1/setup.py -q bdist_egg --dist-dir /tmp/easy_install-il40tn_2/PyYAML-5.1/egg-dist-tmp-u1mby0j4\n",
            "In file included from \u001b[01m\u001b[Kext/_yaml.c:591:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kext/_yaml.h:2:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kyaml.h: No such file or directory\n",
            " #include \u001b[01;31m\u001b[K<yaml.h>\u001b[m\u001b[K\n",
            "          \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n",
            "Error compiling module, falling back to pure Python\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "Moving PyYAML-5.1-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding PyYAML 5.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/PyYAML-5.1-py3.6-linux-x86_64.egg\n",
            "Searching for py-rouge==1.1\n",
            "Reading https://pypi.org/simple/py-rouge/\n",
            "Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl#sha256=9ae2a859a9edc6d25f3908e48706f7d82d6e78ea18954560c4cb21897dc1d270\n",
            "Best match: py-rouge 1.1\n",
            "Processing py_rouge-1.1-py3-none-any.whl\n",
            "Installing py_rouge-1.1-py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding py-rouge 1.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/py_rouge-1.1-py3.6.egg\n",
            "Searching for py-gfm==0.1.4\n",
            "Reading https://pypi.org/simple/py-gfm/\n",
            "Downloading https://files.pythonhosted.org/packages/21/e0/be291e07b5e72e83285e4c0caf8060db0ab8d26f60bda254651e41493652/py_gfm-0.1.4-py2.py3-none-any.whl#sha256=d873541bcae194cc3b5053858719668834d3f1829342f0566ab3a48d0d744d58\n",
            "Best match: py-gfm 0.1.4\n",
            "Processing py_gfm-0.1.4-py2.py3-none-any.whl\n",
            "Installing py_gfm-0.1.4-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding py-gfm 0.1.4 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/py_gfm-0.1.4-py3.6.egg\n",
            "Searching for pexpect==4.7.0\n",
            "Reading https://pypi.org/simple/pexpect/\n",
            "Downloading https://files.pythonhosted.org/packages/0e/3e/377007e3f36ec42f1b84ec322ee12141a9e10d808312e5738f52f80a232c/pexpect-4.7.0-py2.py3-none-any.whl#sha256=2094eefdfcf37a1fdbfb9aa090862c1a4878e5c7e0e7e7088bdb511c558e5cd1\n",
            "Best match: pexpect 4.7.0\n",
            "Processing pexpect-4.7.0-py2.py3-none-any.whl\n",
            "Installing pexpect-4.7.0-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding pexpect 4.7.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/pexpect-4.7.0-py3.6.egg\n",
            "Searching for pytest==5.3.2\n",
            "Reading https://pypi.org/simple/pytest/\n",
            "Downloading https://files.pythonhosted.org/packages/19/cf/711f1d887cb92c5155c9a1eb338f1b5d2411b50e4492a3b20e4a188a22b2/pytest-5.3.2-py3-none-any.whl#sha256=e41d489ff43948babd0fad7ad5e49b8735d5d55e26628a58673c39ff61d95de4\n",
            "Best match: pytest 5.3.2\n",
            "Processing pytest-5.3.2-py3-none-any.whl\n",
            "Installing pytest-5.3.2-py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding pytest 5.3.2 to easy-install.pth file\n",
            "Installing py.test script to /usr/local/bin\n",
            "Installing pytest script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/pytest-5.3.2-py3.6.egg\n",
            "Searching for nltk==3.4.5\n",
            "Reading https://pypi.org/simple/nltk/\n",
            "Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip#sha256=bed45551259aa2101381bbdd5df37d44ca2669c5c3dad72439fa459b29137d94\n",
            "Best match: nltk 3.4.5\n",
            "Processing nltk-3.4.5.zip\n",
            "Writing /tmp/easy_install-vg0c7muo/nltk-3.4.5/setup.cfg\n",
            "Running nltk-3.4.5/setup.py -q bdist_egg --dist-dir /tmp/easy_install-vg0c7muo/nltk-3.4.5/egg-dist-tmp-2my1k3lv\n",
            "warning: no files found matching 'README.txt'\n",
            "warning: no files found matching 'Makefile' under directory '*.txt'\n",
            "warning: no previously-included files matching '*~' found anywhere in distribution\n",
            "creating /usr/local/lib/python3.6/dist-packages/nltk-3.4.5-py3.6.egg\n",
            "Extracting nltk-3.4.5-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding nltk 3.4.5 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/nltk-3.4.5-py3.6.egg\n",
            "Searching for GitPython==3.0.3\n",
            "Reading https://pypi.org/simple/GitPython/\n",
            "Downloading https://files.pythonhosted.org/packages/ff/f8/05f58bd7852dad7edcf70a8de953b4fa39f61cdc13812ae62118be6ffa23/GitPython-3.0.3-py3-none-any.whl#sha256=6e97b9f0954807f30c2dd8e3165731ed6c477a1b365f194b69d81d7940a08332\n",
            "Best match: GitPython 3.0.3\n",
            "Processing GitPython-3.0.3-py3-none-any.whl\n",
            "Installing GitPython-3.0.3-py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding GitPython 3.0.3 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/GitPython-3.0.3-py3.6.egg\n",
            "Searching for gitdb2==2.0.5\n",
            "Reading https://pypi.org/simple/gitdb2/\n",
            "Downloading https://files.pythonhosted.org/packages/da/30/a407568aa8d8f25db817cf50121a958722f3fc5f87e3a6fba1f40c0633e3/gitdb2-2.0.5-py2.py3-none-any.whl#sha256=e3a0141c5f2a3f635c7209d56c496ebe1ad35da82fe4d3ec4aaa36278d70648a\n",
            "Best match: gitdb2 2.0.5\n",
            "Processing gitdb2-2.0.5-py2.py3-none-any.whl\n",
            "Installing gitdb2-2.0.5-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding gitdb2 2.0.5 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/gitdb2-2.0.5-py3.6.egg\n",
            "Searching for flake8-bugbear==19.8.0\n",
            "Reading https://pypi.org/simple/flake8-bugbear/\n",
            "Downloading https://files.pythonhosted.org/packages/9f/f8/170861859fb8ae97923b4fc28501dd25209925e25face836562d3e3f5ea2/flake8_bugbear-19.8.0-py35.py36.py37-none-any.whl#sha256=ded4d282778969b5ab5530ceba7aa1a9f1b86fa7618fc96a19a1d512331640f8\n",
            "Best match: flake8-bugbear 19.8.0\n",
            "Processing flake8_bugbear-19.8.0-py35.py36.py37-none-any.whl\n",
            "Installing flake8_bugbear-19.8.0-py35.py36.py37-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding flake8-bugbear 19.8.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/flake8_bugbear-19.8.0-py3.6.egg\n",
            "Searching for flake8==3.7.8\n",
            "Reading https://pypi.org/simple/flake8/\n",
            "Downloading https://files.pythonhosted.org/packages/26/de/3f815a99d86eb10464ea7bd6059c0172c7ca97d4bdcfca41051b388a653b/flake8-3.7.8-py2.py3-none-any.whl#sha256=8e9dfa3cecb2400b3738a42c54c3043e821682b9c840b0448c0503f781130696\n",
            "Best match: flake8 3.7.8\n",
            "Processing flake8-3.7.8-py2.py3-none-any.whl\n",
            "Installing flake8-3.7.8-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding flake8 3.7.8 to easy-install.pth file\n",
            "Installing flake8 script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/flake8-3.7.8-py3.6.egg\n",
            "Searching for docformatter==1.3.0\n",
            "Reading https://pypi.org/simple/docformatter/\n",
            "Downloading https://files.pythonhosted.org/packages/35/52/41c0152a44873c8f11f0f9fe21b680588ae0e2b20e4a0a9b4812f8decbdc/docformatter-1.3.tar.gz#sha256=c68c8db0952d7ec6423581915fde3f23d4be5eccc11eb28a3415b5cd0a1e4f73\n",
            "Best match: docformatter 1.3\n",
            "Processing docformatter-1.3.tar.gz\n",
            "Writing /tmp/easy_install-2yu41uyb/docformatter-1.3/setup.cfg\n",
            "Running docformatter-1.3/setup.py -q bdist_egg --dist-dir /tmp/easy_install-2yu41uyb/docformatter-1.3/egg-dist-tmp-8r4pmo_7\n",
            "warning: no previously-included files found matching '.pre-commit-hooks.yaml'\n",
            "warning: no previously-included files found matching '.travis.yml'\n",
            "warning: no previously-included files found matching 'Makefile'\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "Moving docformatter-1.3-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding docformatter 1.3 to easy-install.pth file\n",
            "Installing docformatter script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/docformatter-1.3-py3.6.egg\n",
            "Searching for emoji==0.5.4\n",
            "Reading https://pypi.org/simple/emoji/\n",
            "Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz#sha256=60652d3a2dcee5b8af8acb097c31776fb6d808027aeb7221830f72cdafefc174\n",
            "Best match: emoji 0.5.4\n",
            "Processing emoji-0.5.4.tar.gz\n",
            "Writing /tmp/easy_install-n41x6h30/emoji-0.5.4/setup.cfg\n",
            "Running emoji-0.5.4/setup.py -q bdist_egg --dist-dir /tmp/easy_install-n41x6h30/emoji-0.5.4/egg-dist-tmp-8y9p2ds5\n",
            "Moving emoji-0.5.4-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding emoji 0.5.4 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/emoji-0.5.4-py3.6.egg\n",
            "Searching for docutils==0.14\n",
            "Reading https://pypi.org/simple/docutils/\n",
            "Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl#sha256=02aec4bd92ab067f6ff27a38a38a41173bf01bed8f89157768c1573f53e474a6\n",
            "Best match: docutils 0.14\n",
            "Processing docutils-0.14-py3-none-any.whl\n",
            "Installing docutils-0.14-py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding docutils 0.14 to easy-install.pth file\n",
            "Installing rst2odt.py script to /usr/local/bin\n",
            "Installing rst2pseudoxml.py script to /usr/local/bin\n",
            "Installing rst2html4.py script to /usr/local/bin\n",
            "Installing rst2man.py script to /usr/local/bin\n",
            "Installing rstpep2html.py script to /usr/local/bin\n",
            "Installing rst2latex.py script to /usr/local/bin\n",
            "Installing rst2odt_prepstyles.py script to /usr/local/bin\n",
            "Installing rst2xml.py script to /usr/local/bin\n",
            "Installing rst2html5.py script to /usr/local/bin\n",
            "Installing rst2html.py script to /usr/local/bin\n",
            "Installing rst2xetex.py script to /usr/local/bin\n",
            "Installing rst2s5.py script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/docutils-0.14-py3.6.egg\n",
            "Searching for botocore==1.12.246\n",
            "Reading https://pypi.org/simple/botocore/\n",
            "Downloading https://files.pythonhosted.org/packages/6d/22/398af6ea8d5c6bac57154442068613df74e7adbf255417c6894ef49fda42/botocore-1.12.246-py2.py3-none-any.whl#sha256=6dcc121be4917cc731577a2ddff67b89cee6a4b0ec30241ce207a80a0d41990a\n",
            "Best match: botocore 1.12.246\n",
            "Processing botocore-1.12.246-py2.py3-none-any.whl\n",
            "Installing botocore-1.12.246-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding botocore 1.12.246 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/botocore-1.12.246-py3.6.egg\n",
            "Searching for boto3==1.9.246\n",
            "Reading https://pypi.org/simple/boto3/\n",
            "Downloading https://files.pythonhosted.org/packages/5b/cd/a888759c39670cac6b74027d3e3a93073ecb99135b1292d15bc2f3b6f90a/boto3-1.9.246-py2.py3-none-any.whl#sha256=9a84be232ff6432312c16b8e52cc5a01e6ff461cb9b50b3cafee24c6876a5012\n",
            "Best match: boto3 1.9.246\n",
            "Processing boto3-1.9.246-py2.py3-none-any.whl\n",
            "Installing boto3-1.9.246-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding boto3 1.9.246 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/boto3-1.9.246-py3.6.egg\n",
            "Searching for sphinxcontrib-serializinghtml\n",
            "Reading https://pypi.org/simple/sphinxcontrib-serializinghtml/\n",
            "Downloading https://files.pythonhosted.org/packages/9a/ca/bfad79b79b3821d0c6361c431f0ef4aec16ee248338b2c2013008b34d345/sphinxcontrib_serializinghtml-1.1.4-py2.py3-none-any.whl#sha256=f242a81d423f59617a8e5cf16f5d4d74e28ee9a66f9e5b637a18082991db5a9a\n",
            "Best match: sphinxcontrib-serializinghtml 1.1.4\n",
            "Processing sphinxcontrib_serializinghtml-1.1.4-py2.py3-none-any.whl\n",
            "Installing sphinxcontrib_serializinghtml-1.1.4-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding sphinxcontrib-serializinghtml 1.1.4 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/sphinxcontrib_serializinghtml-1.1.4-py3.6.egg\n",
            "Searching for sphinxcontrib-qthelp\n",
            "Reading https://pypi.org/simple/sphinxcontrib-qthelp/\n",
            "Downloading https://files.pythonhosted.org/packages/2b/14/05f9206cf4e9cfca1afb5fd224c7cd434dcc3a433d6d9e4e0264d29c6cdb/sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl#sha256=bd9fc24bcb748a8d51fd4ecaade681350aa63009a347a8c14e637895444dfab6\n",
            "Best match: sphinxcontrib-qthelp 1.0.3\n",
            "Processing sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl\n",
            "Installing sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding sphinxcontrib-qthelp 1.0.3 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/sphinxcontrib_qthelp-1.0.3-py3.6.egg\n",
            "Searching for sphinxcontrib-jsmath\n",
            "Reading https://pypi.org/simple/sphinxcontrib-jsmath/\n",
            "Downloading https://files.pythonhosted.org/packages/c2/42/4c8646762ee83602e3fb3fbe774c2fac12f317deb0b5dbeeedd2d3ba4b77/sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl#sha256=2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178\n",
            "Best match: sphinxcontrib-jsmath 1.0.1\n",
            "Processing sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl\n",
            "Installing sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding sphinxcontrib-jsmath 1.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/sphinxcontrib_jsmath-1.0.1-py3.6.egg\n",
            "Searching for sphinxcontrib-htmlhelp\n",
            "Reading https://pypi.org/simple/sphinxcontrib-htmlhelp/\n",
            "Downloading https://files.pythonhosted.org/packages/36/62/8222554b29b3acde8420128d6d3999c5904d40922ef4b6ccb370e2be7421/sphinxcontrib_htmlhelp-1.0.3-py2.py3-none-any.whl#sha256=3c0bc24a2c41e340ac37c85ced6dafc879ab485c095b1d65d2461ac2f7cca86f\n",
            "Best match: sphinxcontrib-htmlhelp 1.0.3\n",
            "Processing sphinxcontrib_htmlhelp-1.0.3-py2.py3-none-any.whl\n",
            "Installing sphinxcontrib_htmlhelp-1.0.3-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding sphinxcontrib-htmlhelp 1.0.3 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/sphinxcontrib_htmlhelp-1.0.3-py3.6.egg\n",
            "Searching for sphinxcontrib-devhelp\n",
            "Reading https://pypi.org/simple/sphinxcontrib-devhelp/\n",
            "Downloading https://files.pythonhosted.org/packages/c5/09/5de5ed43a521387f18bdf5f5af31d099605c992fd25372b2b9b825ce48ee/sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl#sha256=8165223f9a335cc1af7ffe1ed31d2871f325254c0423bc0c4c7cd1c1e4734a2e\n",
            "Best match: sphinxcontrib-devhelp 1.0.2\n",
            "Processing sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl\n",
            "Installing sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding sphinxcontrib-devhelp 1.0.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/sphinxcontrib_devhelp-1.0.2-py3.6.egg\n",
            "Searching for sphinxcontrib-applehelp\n",
            "Reading https://pypi.org/simple/sphinxcontrib-applehelp/\n",
            "Downloading https://files.pythonhosted.org/packages/dc/47/86022665a9433d89a66f5911b558ddff69861766807ba685de2e324bd6ed/sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl#sha256=806111e5e962be97c29ec4c1e7fe277bfd19e9652fb1a4392105b43e01af885a\n",
            "Best match: sphinxcontrib-applehelp 1.0.2\n",
            "Processing sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl\n",
            "Installing sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding sphinxcontrib-applehelp 1.0.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/sphinxcontrib_applehelp-1.0.2-py3.6.egg\n",
            "Searching for idna<2.9,>=2.5\n",
            "Reading https://pypi.org/simple/idna/\n",
            "Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl#sha256=ea8b7f6188e6fa117537c3df7da9fc686d485087abf6ac197f9c46432f7e4a3c\n",
            "Best match: idna 2.8\n",
            "Processing idna-2.8-py2.py3-none-any.whl\n",
            "Installing idna-2.8-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding idna 2.8 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/idna-2.8-py3.6.egg\n",
            "Searching for commonmark>=0.8.1\n",
            "Reading https://pypi.org/simple/commonmark/\n",
            "Downloading https://files.pythonhosted.org/packages/b1/92/dfd892312d822f36c55366118b95d914e5f16de11044a27cf10a7d71bbbf/commonmark-0.9.1-py2.py3-none-any.whl#sha256=da2f38c92590f83de410ba1a3cbceafbc74fee9def35f9251ba9a971d6d66fd9\n",
            "Best match: commonmark 0.9.1\n",
            "Processing commonmark-0.9.1-py2.py3-none-any.whl\n",
            "Installing commonmark-0.9.1-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding commonmark 0.9.1 to easy-install.pth file\n",
            "Installing cmark script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/commonmark-0.9.1-py3.6.egg\n",
            "Searching for markdown<3.0\n",
            "Reading https://pypi.org/simple/markdown/\n",
            "Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl#sha256=9ba587db9daee7ec761cfc656272be6aabe2ed300fece21208e4aab2e457bc8f\n",
            "Best match: Markdown 2.6.11\n",
            "Processing Markdown-2.6.11-py2.py3-none-any.whl\n",
            "Installing Markdown-2.6.11-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding Markdown 2.6.11 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/Markdown-2.6.11-py3.6.egg\n",
            "Searching for pluggy<1.0,>=0.12\n",
            "Reading https://pypi.org/simple/pluggy/\n",
            "Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl#sha256=966c145cd83c96502c3c3868f50408687b38434af77734af1e9ca461a4081d2d\n",
            "Best match: pluggy 0.13.1\n",
            "Processing pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Installing pluggy-0.13.1-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding pluggy 0.13.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/pluggy-0.13.1-py3.6.egg\n",
            "Searching for smmap2>=2.0.0\n",
            "Reading https://pypi.org/simple/smmap2/\n",
            "Downloading https://files.pythonhosted.org/packages/3e/11/2dae3df2f19c43e156cce8e02c0080b46821faf816b839a2023ef7b6b84f/smmap2-3.0.1-py3-none-any.whl#sha256=0cb6ea470b1ad9a65a02ca7f4c7ae601861f7dd24a43812ca51cfca2892bb524\n",
            "Best match: smmap2 3.0.1\n",
            "Processing smmap2-3.0.1-py3-none-any.whl\n",
            "Installing smmap2-3.0.1-py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding smmap2 3.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/smmap2-3.0.1-py3.6.egg\n",
            "Searching for pyflakes<2.2.0,>=2.1.0\n",
            "Reading https://pypi.org/simple/pyflakes/\n",
            "Downloading https://files.pythonhosted.org/packages/84/f2/ed0ffb887f8138a8fe5a621b8c0bb9598bfb3989e029f6c6a85ee66628ee/pyflakes-2.1.1-py2.py3-none-any.whl#sha256=17dbeb2e3f4d772725c777fabc446d5634d1038f234e77343108ce445ea69ce0\n",
            "Best match: pyflakes 2.1.1\n",
            "Processing pyflakes-2.1.1-py2.py3-none-any.whl\n",
            "Installing pyflakes-2.1.1-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding pyflakes 2.1.1 to easy-install.pth file\n",
            "Installing pyflakes script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/pyflakes-2.1.1-py3.6.egg\n",
            "Searching for pycodestyle<2.6.0,>=2.5.0\n",
            "Reading https://pypi.org/simple/pycodestyle/\n",
            "Downloading https://files.pythonhosted.org/packages/0e/0c/04a353e104d2f324f8ee5f4b32012618c1c86dd79e52a433b64fceed511b/pycodestyle-2.5.0-py2.py3-none-any.whl#sha256=95a2219d12372f05704562a14ec30bc76b05a5b297b21a5dfe3f6fac3491ae56\n",
            "Best match: pycodestyle 2.5.0\n",
            "Processing pycodestyle-2.5.0-py2.py3-none-any.whl\n",
            "Installing pycodestyle-2.5.0-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding pycodestyle 2.5.0 to easy-install.pth file\n",
            "Installing pycodestyle script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/pycodestyle-2.5.0-py3.6.egg\n",
            "Searching for mccabe<0.7.0,>=0.6.0\n",
            "Reading https://pypi.org/simple/mccabe/\n",
            "Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl#sha256=ab8a6258860da4b6677da4bd2fe5dc2c659cff31b3ee4f7f5d64e79735b80d42\n",
            "Best match: mccabe 0.6.1\n",
            "Processing mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Installing mccabe-0.6.1-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding mccabe 0.6.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/mccabe-0.6.1-py3.6.egg\n",
            "Searching for untokenize\n",
            "Reading https://pypi.org/simple/untokenize/\n",
            "Downloading https://files.pythonhosted.org/packages/f7/46/e7cea8159199096e1df52da20a57a6665da80c37fb8aeb848a3e47442c32/untokenize-0.1.1.tar.gz#sha256=3865dbbbb8efb4bb5eaa72f1be7f3e0be00ea8b7f125c69cbd1f5fda926f37a2\n",
            "Best match: untokenize 0.1.1\n",
            "Processing untokenize-0.1.1.tar.gz\n",
            "Writing /tmp/easy_install-rz024kep/untokenize-0.1.1/setup.cfg\n",
            "Running untokenize-0.1.1/setup.py -q bdist_egg --dist-dir /tmp/easy_install-rz024kep/untokenize-0.1.1/egg-dist-tmp-ny54qq17\n",
            "warning: no previously-included files found matching '.travis.yml'\n",
            "warning: no previously-included files found matching 'Makefile'\n",
            "warning: no previously-included files found matching 'test_acid.py'\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "Moving untokenize-0.1.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding untokenize 0.1.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/untokenize-0.1.1-py3.6.egg\n",
            "Searching for s3transfer<0.3.0,>=0.2.0\n",
            "Reading https://pypi.org/simple/s3transfer/\n",
            "Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl#sha256=b780f2411b824cb541dbcd2c713d0cb61c7d1bcadae204cdddda2b35cef493ba\n",
            "Best match: s3transfer 0.2.1\n",
            "Processing s3transfer-0.2.1-py2.py3-none-any.whl\n",
            "Installing s3transfer-0.2.1-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding s3transfer 0.2.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/s3transfer-0.2.1-py3.6.egg\n",
            "Searching for smmap>=3.0.1\n",
            "Reading https://pypi.org/simple/smmap/\n",
            "Downloading https://files.pythonhosted.org/packages/27/b1/e379cfb7c07bbf8faee29c4a1a2469dbea525f047c2b454c4afdefa20a30/smmap-3.0.2-py2.py3-none-any.whl#sha256=52ea78b3e708d2c2b0cfe93b6fc3fbeec53db913345c26be6ed84c11ed8bebc1\n",
            "Best match: smmap 3.0.2\n",
            "Processing smmap-3.0.2-py2.py3-none-any.whl\n",
            "Installing smmap-3.0.2-py2.py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding smmap 3.0.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/smmap-3.0.2-py3.6.egg\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scikit-learn==0.22.2.post1\n",
            "Best match: scikit-learn 0.22.2.post1\n",
            "Adding scikit-learn 0.22.2.post1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Pillow==7.0.0\n",
            "Best match: Pillow 7.0.0\n",
            "Adding Pillow 7.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.18.3\n",
            "Best match: numpy 1.18.3\n",
            "Adding numpy 1.18.3 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for joblib==0.14.1\n",
            "Best match: joblib 0.14.1\n",
            "Adding joblib 0.14.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for h5py==2.10.0\n",
            "Best match: h5py 2.10.0\n",
            "Adding h5py 2.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for snowballstemmer==2.0.0\n",
            "Best match: snowballstemmer 2.0.0\n",
            "Adding snowballstemmer 2.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for setuptools==46.1.3\n",
            "Best match: setuptools 46.1.3\n",
            "Adding setuptools 46.1.3 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "Installing easy_install-3.8 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for packaging==20.3\n",
            "Best match: packaging 20.3\n",
            "Adding packaging 20.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for imagesize==1.2.0\n",
            "Best match: imagesize 1.2.0\n",
            "Adding imagesize 1.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Babel==2.8.0\n",
            "Best match: Babel 2.8.0\n",
            "Adding Babel 2.8.0 to easy-install.pth file\n",
            "Installing pybabel script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for alabaster==0.7.12\n",
            "Best match: alabaster 0.7.12\n",
            "Adding alabaster 0.7.12 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Pygments==2.1.3\n",
            "Best match: Pygments 2.1.3\n",
            "Adding Pygments 2.1.3 to easy-install.pth file\n",
            "Installing pygmentize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Jinja2==2.11.2\n",
            "Best match: Jinja2 2.11.2\n",
            "Adding Jinja2 2.11.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for certifi==2020.4.5.1\n",
            "Best match: certifi 2020.4.5.1\n",
            "Adding certifi 2020.4.5.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for ptyprocess==0.6.0\n",
            "Best match: ptyprocess 0.6.0\n",
            "Adding ptyprocess 0.6.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for wcwidth==0.1.9\n",
            "Best match: wcwidth 0.1.9\n",
            "Adding wcwidth 0.1.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for py==1.8.1\n",
            "Best match: py 1.8.1\n",
            "Adding py 1.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for more-itertools==8.2.0\n",
            "Best match: more-itertools 8.2.0\n",
            "Adding more-itertools 8.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for importlib-metadata==1.6.0\n",
            "Best match: importlib-metadata 1.6.0\n",
            "Adding importlib-metadata 1.6.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for attrs==19.3.0\n",
            "Best match: attrs 19.3.0\n",
            "Adding attrs 19.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for entrypoints==0.3\n",
            "Best match: entrypoints 0.3\n",
            "Adding entrypoints 0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.8.1\n",
            "Best match: python-dateutil 2.8.1\n",
            "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for jmespath==0.9.5\n",
            "Best match: jmespath 0.9.5\n",
            "Adding jmespath 0.9.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyparsing==2.4.7\n",
            "Best match: pyparsing 2.4.7\n",
            "Adding pyparsing 2.4.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for MarkupSafe==1.1.1\n",
            "Best match: MarkupSafe 1.1.1\n",
            "Adding MarkupSafe 1.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for zipp==3.1.0\n",
            "Best match: zipp 3.1.0\n",
            "Adding zipp 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for parlai==0.1.20200504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF7j-oNe0en4",
        "colab_type": "code",
        "outputId": "16c9d498-ac62-46e9-d4fa-de89de9445a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!python ParlAI/setup.py develop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"ParlAI/setup.py\", line 19, in <module>\n",
            "    with open('README.md', encoding=\"utf8\") as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'README.md'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46DHehQ30k3r",
        "colab_type": "code",
        "outputId": "38b02c5e-6258-48b9-9543-b0875cc7a2f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!ls ParlAI"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "autoformat.sh\t    example_parlai_internal  parlai\t       setup.py\n",
            "codecov.yml\t    examples\t\t     projects\t       tests\n",
            "CODE_OF_CONDUCT.md  LICENSE\t\t     pyproject.toml    website\n",
            "conftest.py\t    MANIFEST.in\t\t     pytest.ini\n",
            "CONTRIBUTING.md     mypy.ini\t\t     README.md\n",
            "docs\t\t    NEWS.md\t\t     requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3iFXRZ-0r0H",
        "colab_type": "code",
        "outputId": "339b0c5e-0ece-4d4f-c3ef-2d4487f4e717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwzTmg4G1x-_",
        "colab_type": "code",
        "outputId": "41d1b00f-be92-435a-ec97-cd040eed33e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "autoformat.sh\t    example_parlai_internal  parlai\t      requirements.txt\n",
            "codecov.yml\t    examples\t\t     parlai.egg-info  setup.py\n",
            "CODE_OF_CONDUCT.md  LICENSE\t\t     projects\t      tests\n",
            "conftest.py\t    MANIFEST.in\t\t     pyproject.toml   website\n",
            "CONTRIBUTING.md     mypy.ini\t\t     pytest.ini\n",
            "docs\t\t    NEWS.md\t\t     README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46mvBxnq1zZo",
        "colab_type": "code",
        "outputId": "e4ab9eb9-70e8-4514-d1d6-b3aa30f0ed8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd ParlAI "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ParlAI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3gNk0ym183f",
        "colab_type": "code",
        "outputId": "25868434-70c5-48fa-a1f1-c16cfe7d33ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%cd data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'data'\n",
            "/content/ParlAI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coQreJRB1-k-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrEkwgAz36JJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.randn(10000,889,77871)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Gn_Ddu39Dq",
        "colab_type": "code",
        "outputId": "8b160259-353c-4ef7-a656-ab089509cac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install 'git+https://github.com/rsennrich/subword-nmt.git#egg=subword-nmt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting subword-nmt\n",
            "  Cloning https://github.com/rsennrich/subword-nmt.git to /tmp/pip-install-4gdjox2y/subword-nmt\n",
            "  Running command git clone -q https://github.com/rsennrich/subword-nmt.git /tmp/pip-install-4gdjox2y/subword-nmt\n",
            "Building wheels for collected packages: subword-nmt\n",
            "  Building wheel for subword-nmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subword-nmt: filename=subword_nmt-0.3.7-cp36-none-any.whl size=128280 sha256=13a649fce45e0243b7462daaa88c5490f62705f335dc287b510b5fb2bf9823ca\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cy9khcqs/wheels/05/2b/3a/73cd213b79c18512c76a9ea40329a6819fa9276e232679d834\n",
            "Successfully built subword-nmt\n",
            "Installing collected packages: subword-nmt\n",
            "Successfully installed subword-nmt-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzD_iHyr5dfB",
        "colab_type": "code",
        "outputId": "e829401a-8b4e-402b-d2e6-8af497ef221d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget http://parl.ai/downloads/_models/blender/BST3B.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-04 18:54:28--  http://parl.ai/downloads/_models/blender/BST3B.tgz\n",
            "Resolving parl.ai (parl.ai)... 99.86.57.102, 99.86.57.82, 99.86.57.113, ...\n",
            "Connecting to parl.ai (parl.ai)|99.86.57.102|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://parl.ai/downloads/_models/blender/BST3B.tgz [following]\n",
            "--2020-05-04 18:54:28--  https://parl.ai/downloads/_models/blender/BST3B.tgz\n",
            "Connecting to parl.ai (parl.ai)|99.86.57.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://dl.fbaipublicfiles.com/parlai/_models/blender/BST3B.tgz [following]\n",
            "--2020-05-04 18:54:28--  https://dl.fbaipublicfiles.com/parlai/_models/blender/BST3B.tgz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4b8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4953178122 (4.6G) [application/gzip]\n",
            "Saving to: ‘BST3B.tgz’\n",
            "\n",
            "BST3B.tgz           100%[===================>]   4.61G  20.3MB/s    in 3m 57s  \n",
            "\n",
            "2020-05-04 18:58:25 (20.0 MB/s) - ‘BST3B.tgz’ saved [4953178122/4953178122]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-joPfdwCdtU",
        "colab_type": "code",
        "outputId": "1ffa6797-e3a2-4233-e2a6-0ac6a70f42bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "autoformat.sh\t    data\t\t     mypy.ini\t      pytest.ini\n",
            "BST3B.tgz\t    docs\t\t     NEWS.md\t      README.md\n",
            "codecov.yml\t    example_parlai_internal  parlai\t      requirements.txt\n",
            "CODE_OF_CONDUCT.md  examples\t\t     parlai.egg-info  setup.py\n",
            "conftest.py\t    LICENSE\t\t     projects\t      tests\n",
            "CONTRIBUTING.md     MANIFEST.in\t\t     pyproject.toml   website\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19kknO0kD5O8",
        "colab_type": "code",
        "outputId": "af358420-1c4b-4f38-8b61-94c9d816f3d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!tar zxvf BST3B.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model\n",
            "model.dict\n",
            "model.dict-merges.txt\n",
            "model.dict.opt\n",
            "model.dict-vocab.json\n",
            "model.opt\n",
            "model.trainstats\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpQ4L1TcEHCp",
        "colab_type": "code",
        "outputId": "f8fc03ce-145c-491e-efbf-252801f6a12c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "autoformat.sh\t\t MANIFEST.in\t\tparlai.egg-info\n",
            "BST3B.tgz\t\t model\t\t\tprojects\n",
            "codecov.yml\t\t model.dict\t\tpyproject.toml\n",
            "CODE_OF_CONDUCT.md\t model.dict-merges.txt\tpytest.ini\n",
            "conftest.py\t\t model.dict.opt\t\tREADME.md\n",
            "CONTRIBUTING.md\t\t model.dict-vocab.json\trequirements.txt\n",
            "data\t\t\t model.opt\t\tsetup.py\n",
            "docs\t\t\t model.trainstats\ttests\n",
            "example_parlai_internal  mypy.ini\t\twebsite\n",
            "examples\t\t NEWS.md\n",
            "LICENSE\t\t\t parlai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vqmH4miEuJ8",
        "colab_type": "code",
        "outputId": "bb79bd80-4141-4670-d7ae-a0a03371767b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%cd model.dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 20] Not a directory: 'model.dict'\n",
            "/content/ParlAI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y68fl23UE37G",
        "colab_type": "code",
        "outputId": "f85b27c9-6792-44f8-a9bf-9824e05bb2e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls -lh model.dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-rw-r-- 1 1185200181 1185200181 71K Mar 31 20:43 model.dict\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tB0O2sMFD3f",
        "colab_type": "code",
        "outputId": "1b7f1fee-40ee-42d2-bf04-1dcc40cdaf6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!cat model.opt"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"init_opt\": null, \"show_advanced_args\": false, \"task\": \"internal:blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues\", \"download_path\": \"/private/home/edinan/ParlAI/downloads\", \"datatype\": \"train\", \"image_mode\": \"raw\", \"numthreads\": 1, \"hide_labels\": false, \"multitask_weights\": [1.0, 3.0, 3.0, 3.0], \"batchsize\": 128, \"dynamic_batching\": null, \"datapath\": \"/private/home/edinan/ParlAI/data\", \"model\": \"transformer/generator\", \"model_file\": \"/checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/de6/model\", \"init_model\": \"/checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model\", \"dict_class\": \"parlai.core.dict:DictionaryAgent\", \"evaltask\": null, \"eval_batchsize\": null, \"display_examples\": false, \"num_epochs\": -1, \"max_train_time\": 27647.999999999996, \"validation_every_n_secs\": -1, \"save_every_n_secs\": -1, \"save_after_valid\": true, \"validation_every_n_epochs\": 0.25, \"validation_max_exs\": -1, \"short_final_eval\": false, \"validation_patience\": 10, \"validation_metric\": \"ppl\", \"validation_metric_mode\": \"min\", \"validation_cutoff\": 1.0, \"load_from_checkpoint\": false, \"validation_share_agent\": false, \"metrics\": \"default\", \"aggregate_micro\": false, \"tensorboard_log\": false, \"dict_maxexs\": -1, \"dict_include_valid\": false, \"dict_include_test\": false, \"log_every_n_secs\": 10.0, \"image_size\": 256, \"image_cropsize\": 224, \"label_type\": \"response\", \"include_knowledge\": true, \"include_checked_sentence\": true, \"include_knowledge_separator\": false, \"num_topics\": 5, \"train_experiencer_only\": false, \"remove_political_convos\": false, \"embedding_size\": 2560, \"n_layers\": 2, \"ffn_size\": 10240, \"dropout\": 0.1, \"attention_dropout\": 0.0, \"relu_dropout\": 0.0, \"n_heads\": 32, \"learn_positional_embeddings\": false, \"embeddings_scale\": true, \"n_positions\": 128, \"n_segments\": 0, \"variant\": \"prelayernorm\", \"activation\": \"gelu\", \"output_scaling\": 1.0, \"share_word_embeddings\": true, \"n_encoder_layers\": 2, \"n_decoder_layers\": 24, \"model_parallel\": true, \"beam_size\": 10, \"beam_min_length\": 20, \"beam_context_block_ngram\": 3, \"beam_block_ngram\": 3, \"beam_length_penalty\": 0.65, \"skip_generation\": false, \"inference\": \"beam\", \"topk\": 10, \"topp\": 0.9, \"beam_delay\": 30, \"temperature\": 1.0, \"compute_tokenized_bleu\": false, \"embedding_type\": \"random\", \"embedding_projection\": \"random\", \"fp16\": true, \"fp16_impl\": \"mem_efficient\", \"force_fp16_tokens\": false, \"optimizer\": \"mem_eff_adam\", \"learningrate\": 7e-06, \"gradient_clip\": 0.1, \"adam_eps\": 1e-08, \"adafactor_eps\": [1e-30, 0.001], \"momentum\": 0, \"nesterov\": true, \"nus\": [0.7], \"betas\": [0.9, 0.999], \"weight_decay\": null, \"rank_candidates\": false, \"truncate\": 128, \"text_truncate\": 128, \"label_truncate\": 128, \"history_size\": -1, \"person_tokens\": false, \"split_lines\": false, \"use_reply\": \"label\", \"add_p1_after_newln\": false, \"delimiter\": \"  \", \"history_add_global_end_token\": \"end\", \"gpu\": -1, \"no_cuda\": false, \"dict_file\": \"/checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict\", \"dict_initpath\": null, \"dict_language\": \"english\", \"dict_max_ngram_size\": -1, \"dict_minfreq\": 0, \"dict_maxtokens\": -1, \"dict_nulltoken\": \"__null__\", \"dict_starttoken\": \"__start__\", \"dict_endtoken\": \"__end__\", \"dict_unktoken\": \"__unk__\", \"dict_tokenizer\": \"bytelevelbpe\", \"dict_lower\": false, \"bpe_debug\": false, \"dict_textfields\": \"text,labels\", \"bpe_vocab\": \"/checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-vocab.json\", \"bpe_merge\": \"/checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict-merges.txt\", \"bpe_add_prefix_space\": true, \"lr_scheduler\": \"reduceonplateau\", \"lr_scheduler_patience\": 3, \"lr_scheduler_decay\": 0.5, \"max_lr_steps\": -1, \"invsqrt_lr_decay_gamma\": -1, \"warmup_updates\": 100, \"warmup_rate\": 0.0001, \"update_freq\": 2, \"parlai_home\": \"/checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/ParlAI\", \"override\": {\"task\": \"internal:blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues\", \"multitask_weights\": [1.0, 3.0, 3.0, 3.0], \"validation_every_n_epochs\": 0.25, \"attention_dropout\": 0.0, \"batchsize\": 128, \"model\": \"transformer/generator\", \"embedding_size\": 2560, \"ffn_size\": 10240, \"variant\": \"prelayernorm\", \"n_heads\": 32, \"n_positions\": 128, \"n_encoder_layers\": 2, \"n_decoder_layers\": 24, \"history_add_global_end_token\": \"end\", \"delimiter\": \"  \", \"dict_tokenizer\": \"bytelevelbpe\", \"dict_file\": \"/checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model.dict\", \"dropout\": 0.1, \"fp16\": true, \"init_model\": \"/checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model\", \"label_truncate\": 128, \"learningrate\": 7e-06, \"lr_scheduler\": \"reduceonplateau\", \"lr_scheduler_patience\": 3, \"optimizer\": \"adam\", \"relu_dropout\": 0.0, \"activation\": \"gelu\", \"model_parallel\": true, \"save_after_valid\": true, \"text_truncate\": 128, \"truncate\": 128, \"fp16_impl\": \"mem_efficient\", \"update_freq\": 2, \"gradient_clip\": 0.1, \"skip_generation\": false, \"validation_patience\": 10, \"max_train_time\": 27647.999999999996, \"validation_metric\": \"ppl\", \"validation_metric_mode\": \"min\", \"model_file\": \"/checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/de6/model\", \"inference\": \"beam\", \"beam_size\": 10, \"beam_block_ngram\": 3, \"beam_context_block_ngram\": 3, \"beam_min_length\": 20}, \"starttime\": \"Mar31_06-04\", \"batchindex\": 127}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJOBieAuFk7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMB9uNFdHA6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvtKRVclGILr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load(os.path.join(os.getcwd(),'model'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNuSgMs1GXLS",
        "colab_type": "code",
        "outputId": "50422f2e-8a09-40b4-e3c8-b3c7a60999e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lr_scheduler': {'_last_lr': [7e-06],\n",
              "  'best': 2.1927281020610314,\n",
              "  'cooldown': 0,\n",
              "  'cooldown_counter': 0,\n",
              "  'eps': 1e-08,\n",
              "  'factor': 0.5,\n",
              "  'last_epoch': 17,\n",
              "  'min_lrs': [0],\n",
              "  'mode': 'min',\n",
              "  'mode_worse': inf,\n",
              "  'num_bad_epochs': 0,\n",
              "  'patience': 3,\n",
              "  'threshold': 0.0001,\n",
              "  'threshold_mode': 'rel',\n",
              "  'verbose': True},\n",
              " 'lr_scheduler_type': 'reduceonplateau',\n",
              " 'model': OrderedDict([('START', tensor([1])),\n",
              "              ('embeddings.weight',\n",
              "               tensor([[-8.7204e-03,  4.5815e-03,  6.3210e-03,  ..., -7.6294e-03,\n",
              "                        -1.5793e-02,  1.1459e-02],\n",
              "                       [-2.3941e-02, -2.6283e-03, -2.2949e-02,  ..., -1.8585e-02,\n",
              "                        -9.9106e-03, -2.1820e-02],\n",
              "                       [-2.3941e-02, -1.1616e-03, -2.2949e-02,  ..., -1.8585e-02,\n",
              "                        -1.0139e-02, -2.1820e-02],\n",
              "                       ...,\n",
              "                       [ 0.0000e+00,  9.4891e-04,  2.9206e-06,  ...,  1.1325e-05,\n",
              "                        -2.3117e-03, -9.9301e-05],\n",
              "                       [ 0.0000e+00,  9.4891e-04,  2.9206e-06,  ...,  1.1325e-05,\n",
              "                        -2.3117e-03, -9.9301e-05],\n",
              "                       [ 0.0000e+00,  9.4891e-04,  2.9206e-06,  ...,  1.1325e-05,\n",
              "                        -2.3117e-03, -9.9301e-05]], dtype=torch.float16)),\n",
              "              ('encoder.embeddings.weight',\n",
              "               tensor([[-8.7204e-03,  4.5815e-03,  6.3210e-03,  ..., -7.6294e-03,\n",
              "                        -1.5793e-02,  1.1459e-02],\n",
              "                       [-2.3941e-02, -2.6283e-03, -2.2949e-02,  ..., -1.8585e-02,\n",
              "                        -9.9106e-03, -2.1820e-02],\n",
              "                       [-2.3941e-02, -1.1616e-03, -2.2949e-02,  ..., -1.8585e-02,\n",
              "                        -1.0139e-02, -2.1820e-02],\n",
              "                       ...,\n",
              "                       [ 0.0000e+00,  9.4891e-04,  2.9206e-06,  ...,  1.1325e-05,\n",
              "                        -2.3117e-03, -9.9301e-05],\n",
              "                       [ 0.0000e+00,  9.4891e-04,  2.9206e-06,  ...,  1.1325e-05,\n",
              "                        -2.3117e-03, -9.9301e-05],\n",
              "                       [ 0.0000e+00,  9.4891e-04,  2.9206e-06,  ...,  1.1325e-05,\n",
              "                        -2.3117e-03, -9.9301e-05]], dtype=torch.float16)),\n",
              "              ('encoder.position_embeddings.weight',\n",
              "               tensor([[ 0.9092,  0.9150,  0.9209,  ...,  1.0000,  1.0000,  1.0000],\n",
              "                       [ 0.1411,  0.1624,  0.1835,  ...,  1.0000,  1.0000,  1.0000],\n",
              "                       [-0.7568, -0.7378, -0.7183,  ...,  1.0000,  1.0000,  1.0000],\n",
              "                       ...,\n",
              "                       [ 0.9727,  0.4124, -0.4614,  ...,  1.0000,  1.0000,  1.0000],\n",
              "                       [ 0.7212,  0.9883,  0.4846,  ...,  1.0000,  1.0000,  1.0000],\n",
              "                       [-0.1935,  0.6675,  0.9971,  ...,  1.0000,  1.0000,  1.0000]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.norm_embeddings.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('encoder.norm_embeddings.bias',\n",
              "               tensor([ 0.0048, -0.0063, -0.0021,  ..., -0.0151, -0.0097, -0.0041],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.0.attention.q_lin.weight',\n",
              "               tensor([[-0.0024, -0.0092, -0.0152,  ..., -0.0145, -0.0026,  0.0093],\n",
              "                       [-0.0198, -0.0192,  0.0261,  ...,  0.0455, -0.0070,  0.0037],\n",
              "                       [-0.0138, -0.0102, -0.0124,  ...,  0.0056,  0.0052, -0.0187],\n",
              "                       ...,\n",
              "                       [ 0.0222,  0.0185,  0.0212,  ...,  0.0061,  0.0017,  0.0027],\n",
              "                       [-0.0184, -0.0319,  0.0061,  ...,  0.0033,  0.0007,  0.0013],\n",
              "                       [-0.0261, -0.0068, -0.0106,  ..., -0.0079,  0.0131,  0.0388]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.0.attention.q_lin.bias',\n",
              "               tensor([-0.0085,  0.0009, -0.0078,  ...,  0.0281,  0.0071,  0.0076],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.0.attention.k_lin.weight',\n",
              "               tensor([[ 1.2909e-02,  4.9067e-04,  1.0948e-02,  ..., -6.1264e-03,\n",
              "                         4.4067e-02,  1.2512e-03],\n",
              "                       [-8.2779e-03, -1.7578e-02, -1.1009e-02,  ...,  1.6489e-03,\n",
              "                        -2.8336e-02, -2.3987e-02],\n",
              "                       [ 2.0691e-02, -7.8964e-03, -7.5519e-05,  ...,  1.3374e-02,\n",
              "                        -1.9897e-02, -1.3123e-02],\n",
              "                       ...,\n",
              "                       [-2.9007e-02, -3.4210e-02, -1.4366e-02,  ..., -4.2915e-03,\n",
              "                        -3.4657e-03,  5.5695e-03],\n",
              "                       [-1.5442e-02, -2.6566e-02, -2.3994e-03,  ...,  5.6572e-03,\n",
              "                        -8.0490e-03, -1.4130e-02],\n",
              "                       [-5.1498e-03,  4.6387e-03,  1.6418e-02,  ..., -3.4885e-03,\n",
              "                         8.8806e-03,  1.8951e-02]], dtype=torch.float16)),\n",
              "              ('encoder.layers.0.attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('encoder.layers.0.attention.v_lin.weight',\n",
              "               tensor([[ 0.0371, -0.0215, -0.0004,  ...,  0.0060, -0.0157, -0.0316],\n",
              "                       [-0.0246, -0.0033,  0.0064,  ...,  0.0359,  0.0135,  0.0124],\n",
              "                       [ 0.0034,  0.0031,  0.0070,  ..., -0.0007, -0.0016,  0.0099],\n",
              "                       ...,\n",
              "                       [ 0.0022,  0.0026, -0.0091,  ..., -0.0253, -0.0212, -0.0051],\n",
              "                       [ 0.0046, -0.0014,  0.0045,  ...,  0.0107,  0.0185,  0.0111],\n",
              "                       [ 0.0080,  0.0058,  0.0066,  ..., -0.0081,  0.0113, -0.0106]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.0.attention.v_lin.bias',\n",
              "               tensor([-0.0064,  0.0047, -0.0043,  ..., -0.0005,  0.0158,  0.0035],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.0.attention.out_lin.weight',\n",
              "               tensor([[-3.5065e-02,  1.9333e-02, -6.6872e-03,  ...,  4.8828e-03,\n",
              "                         7.0267e-03, -1.2474e-03],\n",
              "                       [ 1.5411e-02,  3.6602e-03,  2.4200e-02,  ..., -4.5959e-02,\n",
              "                         1.8060e-05, -8.0585e-05],\n",
              "                       [ 9.6130e-03,  1.5282e-02,  2.7103e-03,  ...,  4.1809e-03,\n",
              "                         2.7447e-03, -1.1810e-02],\n",
              "                       ...,\n",
              "                       [-1.9409e-02,  1.1108e-02, -2.8286e-03,  ...,  3.0899e-03,\n",
              "                         4.6692e-03, -1.0849e-02],\n",
              "                       [ 1.7405e-03,  2.6459e-02, -1.3786e-02,  ...,  3.5453e-04,\n",
              "                        -8.0032e-03,  1.6052e-02],\n",
              "                       [-1.0719e-02,  1.4603e-02,  7.1144e-04,  ..., -2.0111e-02,\n",
              "                        -2.2461e-02, -3.2234e-03]], dtype=torch.float16)),\n",
              "              ('encoder.layers.0.attention.out_lin.bias',\n",
              "               tensor([ 0.0348,  0.0309, -0.0055,  ...,  0.0252,  0.0297,  0.0170],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.0.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('encoder.layers.0.norm1.bias',\n",
              "               tensor([ 0.0622,  0.0709,  0.0873,  ..., -0.0866, -0.0956, -0.0657],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.0.ffn.lin1.weight',\n",
              "               tensor([[-0.0060,  0.0011,  0.0082,  ..., -0.0278, -0.0224,  0.0024],\n",
              "                       [-0.0218,  0.0137,  0.0026,  ...,  0.0265, -0.0482, -0.0305],\n",
              "                       [-0.0182,  0.0116, -0.0031,  ...,  0.0233, -0.0361,  0.0196],\n",
              "                       ...,\n",
              "                       [ 0.0053,  0.0077,  0.0035,  ...,  0.0088,  0.0180, -0.0198],\n",
              "                       [-0.0573, -0.0277,  0.0011,  ..., -0.0038, -0.0180,  0.0014],\n",
              "                       [ 0.0210, -0.0048,  0.0149,  ...,  0.0181,  0.0018, -0.0014]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.0.ffn.lin1.bias',\n",
              "               tensor([-0.0246, -0.0173, -0.0057,  ..., -0.0311, -0.0147, -0.0249],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.0.ffn.lin2.weight',\n",
              "               tensor([[-0.0259,  0.0461, -0.0173,  ...,  0.0023,  0.0003, -0.0145],\n",
              "                       [ 0.0023, -0.0050,  0.0235,  ...,  0.0335,  0.0230, -0.0130],\n",
              "                       [ 0.0025, -0.0030, -0.0182,  ..., -0.0033,  0.0007, -0.0143],\n",
              "                       ...,\n",
              "                       [ 0.0338,  0.0304,  0.0035,  ..., -0.0463,  0.0114,  0.0102],\n",
              "                       [-0.0136,  0.0208,  0.0026,  ..., -0.0101, -0.0467,  0.0055],\n",
              "                       [ 0.0055, -0.0480, -0.0140,  ..., -0.0119,  0.0016, -0.0082]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.0.ffn.lin2.bias',\n",
              "               tensor([-0.0025, -0.0219, -0.0065,  ...,  0.0125, -0.0003,  0.0083],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.0.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('encoder.layers.0.norm2.bias',\n",
              "               tensor([0.1043, 0.1058, 0.0371,  ..., 0.0406, 0.0704, 0.0474],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.attention.q_lin.weight',\n",
              "               tensor([[ 0.0115,  0.0224, -0.0161,  ..., -0.0127, -0.0171, -0.0222],\n",
              "                       [-0.0139,  0.0374,  0.0001,  ...,  0.0015,  0.0006, -0.0107],\n",
              "                       [-0.0132, -0.0139,  0.0071,  ..., -0.0097,  0.0270,  0.0070],\n",
              "                       ...,\n",
              "                       [ 0.0104, -0.0386,  0.0089,  ...,  0.0156,  0.0160, -0.0096],\n",
              "                       [ 0.0267, -0.0348,  0.0071,  ..., -0.0084,  0.0131, -0.0040],\n",
              "                       [ 0.0094,  0.0098, -0.0101,  ..., -0.0111,  0.0077,  0.0117]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.attention.q_lin.bias',\n",
              "               tensor([ 0.0334, -0.0071, -0.0047,  ..., -0.0144,  0.0028,  0.0063],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.attention.k_lin.weight',\n",
              "               tensor([[-0.0216,  0.0039,  0.0155,  ...,  0.0130, -0.0179, -0.0265],\n",
              "                       [-0.0648,  0.0012, -0.0008,  ..., -0.0079,  0.0118,  0.0052],\n",
              "                       [-0.0413,  0.0065,  0.0143,  ...,  0.0119,  0.0052,  0.0034],\n",
              "                       ...,\n",
              "                       [ 0.0053,  0.0099, -0.0121,  ..., -0.0175,  0.0011,  0.0042],\n",
              "                       [-0.0185, -0.0107, -0.0083,  ...,  0.0090, -0.0016, -0.0040],\n",
              "                       [-0.0232, -0.0218, -0.0014,  ...,  0.0393, -0.0074, -0.0253]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('encoder.layers.1.attention.v_lin.weight',\n",
              "               tensor([[-0.0115,  0.0284,  0.0283,  ..., -0.0036, -0.0234,  0.0179],\n",
              "                       [-0.0066, -0.0258,  0.0145,  ...,  0.0142,  0.0613, -0.0371],\n",
              "                       [-0.0260, -0.0079, -0.0179,  ..., -0.0484, -0.0155,  0.0221],\n",
              "                       ...,\n",
              "                       [-0.0219, -0.0201,  0.0284,  ..., -0.0435, -0.0284, -0.0052],\n",
              "                       [ 0.0249, -0.0259,  0.0321,  ...,  0.0410, -0.0108, -0.0351],\n",
              "                       [-0.0192,  0.0354,  0.0291,  ..., -0.0190, -0.0276,  0.0192]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.attention.v_lin.bias',\n",
              "               tensor([-0.0006, -0.0008,  0.0003,  ...,  0.0005, -0.0040,  0.0033],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.attention.out_lin.weight',\n",
              "               tensor([[-0.0448,  0.0153,  0.0376,  ...,  0.0043,  0.0160,  0.0359],\n",
              "                       [ 0.0129,  0.0248,  0.0627,  ...,  0.0326,  0.0337,  0.0195],\n",
              "                       [-0.0291,  0.0224, -0.0094,  ..., -0.0150, -0.0142, -0.0035],\n",
              "                       ...,\n",
              "                       [-0.0302,  0.0080, -0.0157,  ...,  0.0095, -0.0124,  0.0377],\n",
              "                       [ 0.0015,  0.0191,  0.0124,  ..., -0.0081,  0.0191,  0.0150],\n",
              "                       [-0.0375,  0.0007,  0.0011,  ...,  0.0173, -0.0091, -0.0047]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.attention.out_lin.bias',\n",
              "               tensor([ 0.0052, -0.0098,  0.0013,  ...,  0.0153,  0.0061,  0.0190],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('encoder.layers.1.norm1.bias',\n",
              "               tensor([-0.0371, -0.0140,  0.0208,  ..., -0.0625, -0.0631, -0.0289],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.ffn.lin1.weight',\n",
              "               tensor([[-0.0181, -0.0205, -0.0172,  ...,  0.0120, -0.0275, -0.0143],\n",
              "                       [-0.0108,  0.0110, -0.0012,  ...,  0.0360, -0.0078, -0.0324],\n",
              "                       [-0.0326,  0.0263,  0.0062,  ...,  0.0260, -0.0261, -0.0186],\n",
              "                       ...,\n",
              "                       [ 0.0152,  0.0063, -0.0002,  ..., -0.0107,  0.0571, -0.0073],\n",
              "                       [ 0.0071, -0.0174, -0.0172,  ..., -0.0286, -0.0153, -0.0160],\n",
              "                       [-0.0612,  0.0088,  0.0549,  ..., -0.0175, -0.0211, -0.0101]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.ffn.lin1.bias',\n",
              "               tensor([-0.0320,  0.0031, -0.0093,  ..., -0.0070,  0.0056, -0.0181],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.ffn.lin2.weight',\n",
              "               tensor([[-0.0078, -0.0191, -0.0042,  ...,  0.0258, -0.0081, -0.0476],\n",
              "                       [-0.0331, -0.0436, -0.0291,  ..., -0.0258, -0.0118, -0.0382],\n",
              "                       [ 0.0174,  0.0338, -0.0613,  ...,  0.0107, -0.0111,  0.0018],\n",
              "                       ...,\n",
              "                       [ 0.0030,  0.0047,  0.0110,  ..., -0.0120, -0.0069,  0.0148],\n",
              "                       [-0.0150,  0.0240,  0.0129,  ...,  0.0040, -0.0230,  0.0179],\n",
              "                       [-0.0011,  0.0174,  0.0143,  ...,  0.0095,  0.0148, -0.0308]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.ffn.lin2.bias',\n",
              "               tensor([-0.0078,  0.0043, -0.0038,  ..., -0.0006, -0.0076,  0.0095],\n",
              "                      dtype=torch.float16)),\n",
              "              ('encoder.layers.1.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('encoder.layers.1.norm2.bias',\n",
              "               tensor([ 0.0338, -0.0004, -0.0028,  ...,  0.0315,  0.0039, -0.0174],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.embeddings.weight',\n",
              "               tensor([[-8.7204e-03,  4.5815e-03,  6.3210e-03,  ..., -7.6294e-03,\n",
              "                        -1.5793e-02,  1.1459e-02],\n",
              "                       [-2.3941e-02, -2.6283e-03, -2.2949e-02,  ..., -1.8585e-02,\n",
              "                        -9.9106e-03, -2.1820e-02],\n",
              "                       [-2.3941e-02, -1.1616e-03, -2.2949e-02,  ..., -1.8585e-02,\n",
              "                        -1.0139e-02, -2.1820e-02],\n",
              "                       ...,\n",
              "                       [ 0.0000e+00,  9.4891e-04,  2.9206e-06,  ...,  1.1325e-05,\n",
              "                        -2.3117e-03, -9.9301e-05],\n",
              "                       [ 0.0000e+00,  9.4891e-04,  2.9206e-06,  ...,  1.1325e-05,\n",
              "                        -2.3117e-03, -9.9301e-05],\n",
              "                       [ 0.0000e+00,  9.4891e-04,  2.9206e-06,  ...,  1.1325e-05,\n",
              "                        -2.3117e-03, -9.9301e-05]], dtype=torch.float16)),\n",
              "              ('decoder.norm_embeddings.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.norm_embeddings.bias',\n",
              "               tensor([ 0.0627, -0.0319, -0.0623,  ...,  0.0209,  0.0728, -0.0747],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.position_embeddings.weight',\n",
              "               tensor([[ 0.9092,  0.9150,  0.9209,  ...,  1.0000,  1.0000,  1.0000],\n",
              "                       [ 0.1411,  0.1624,  0.1835,  ...,  1.0000,  1.0000,  1.0000],\n",
              "                       [-0.7568, -0.7378, -0.7183,  ...,  1.0000,  1.0000,  1.0000],\n",
              "                       ...,\n",
              "                       [ 0.9727,  0.4124, -0.4614,  ...,  1.0000,  1.0000,  1.0000],\n",
              "                       [ 0.7212,  0.9883,  0.4846,  ...,  1.0000,  1.0000,  1.0000],\n",
              "                       [-0.1935,  0.6675,  0.9971,  ...,  1.0000,  1.0000,  1.0000]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.self_attention.q_lin.weight',\n",
              "               tensor([[ 0.0009,  0.0100, -0.0244,  ...,  0.0342,  0.0244,  0.0078],\n",
              "                       [ 0.0157,  0.0134,  0.0003,  ..., -0.0144,  0.0026,  0.0143],\n",
              "                       [ 0.0054, -0.0337,  0.0273,  ...,  0.0012, -0.0075, -0.0251],\n",
              "                       ...,\n",
              "                       [ 0.0124, -0.0235, -0.0125,  ..., -0.0070, -0.0013, -0.0179],\n",
              "                       [ 0.0281,  0.0108,  0.0244,  ...,  0.0071, -0.0051,  0.0059],\n",
              "                       [-0.0003, -0.0270,  0.0268,  ..., -0.0064, -0.0035,  0.0341]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.self_attention.q_lin.bias',\n",
              "               tensor([-0.0091, -0.0075, -0.0034,  ...,  0.0017, -0.0107, -0.0280],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0030, -0.0031,  0.0210,  ...,  0.0004,  0.0013, -0.0388],\n",
              "                       [ 0.0024,  0.0074,  0.0292,  ..., -0.0413, -0.0098,  0.0044],\n",
              "                       [-0.0042,  0.0012, -0.0166,  ...,  0.0245,  0.0146, -0.0301],\n",
              "                       ...,\n",
              "                       [-0.0331,  0.0272,  0.0347,  ..., -0.0172,  0.0107, -0.0042],\n",
              "                       [-0.0278,  0.0278, -0.0122,  ..., -0.0093, -0.0116,  0.0072],\n",
              "                       [-0.0027,  0.0253,  0.0186,  ...,  0.0348, -0.0185,  0.0072]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.0.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0008, -0.0044,  0.0214,  ..., -0.0021,  0.0152,  0.0126],\n",
              "                       [-0.0095, -0.0143, -0.0050,  ...,  0.0055, -0.0148,  0.0149],\n",
              "                       [-0.0046,  0.0119, -0.0043,  ..., -0.0032, -0.0068, -0.0293],\n",
              "                       ...,\n",
              "                       [ 0.0121,  0.0020,  0.0281,  ...,  0.0138, -0.0101, -0.0201],\n",
              "                       [-0.0087, -0.0024,  0.0012,  ..., -0.0094,  0.0256, -0.0148],\n",
              "                       [-0.0079, -0.0200, -0.0123,  ..., -0.0070, -0.0165, -0.0098]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.self_attention.v_lin.bias',\n",
              "               tensor([-0.0061,  0.0036, -0.0076,  ...,  0.0153, -0.0210, -0.0159],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.self_attention.out_lin.weight',\n",
              "               tensor([[ 0.0333, -0.0247, -0.0021,  ...,  0.0039,  0.0219, -0.0003],\n",
              "                       [ 0.0057, -0.0027, -0.0011,  ..., -0.0067,  0.0078,  0.0276],\n",
              "                       [ 0.0251,  0.0089, -0.0349,  ..., -0.0285, -0.0102,  0.0348],\n",
              "                       ...,\n",
              "                       [ 0.0038,  0.0011,  0.0003,  ...,  0.0129, -0.0157, -0.0136],\n",
              "                       [-0.0007,  0.0128, -0.0188,  ...,  0.0033, -0.0341,  0.0219],\n",
              "                       [ 0.0101,  0.0189,  0.0033,  ...,  0.0217, -0.0096,  0.0131]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0334,  0.0297, -0.0034,  ...,  0.0225,  0.0333,  0.0108],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.0.norm1.bias',\n",
              "               tensor([ 0.0652,  0.0124,  0.0895,  ..., -0.1008, -0.1206, -0.0523],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.encoder_attention.q_lin.weight',\n",
              "               tensor([[ 0.0281, -0.0047,  0.0173,  ..., -0.0154,  0.0025,  0.0109],\n",
              "                       [ 0.0017, -0.0068, -0.0046,  ..., -0.0002, -0.0100,  0.0094],\n",
              "                       [-0.0069,  0.0136, -0.0109,  ..., -0.0162,  0.0277, -0.0081],\n",
              "                       ...,\n",
              "                       [-0.0321,  0.0145,  0.0118,  ...,  0.0010,  0.0219,  0.0076],\n",
              "                       [ 0.0251, -0.0123, -0.0326,  ..., -0.0033, -0.0005, -0.0080],\n",
              "                       [-0.0430,  0.0323,  0.0156,  ..., -0.0154, -0.0225,  0.0009]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0098, -0.0003, -0.0200,  ..., -0.0092, -0.0214, -0.0215],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0429,  0.0240, -0.0233,  ...,  0.0249,  0.0047, -0.0016],\n",
              "                       [ 0.0267, -0.0023,  0.0281,  ..., -0.0287, -0.0084, -0.0131],\n",
              "                       [-0.0219, -0.0057, -0.0161,  ...,  0.0052,  0.0004, -0.0242],\n",
              "                       ...,\n",
              "                       [-0.0014,  0.0111,  0.0049,  ..., -0.0318,  0.0088, -0.0332],\n",
              "                       [ 0.0264, -0.0036,  0.0063,  ..., -0.0320, -0.0014, -0.0151],\n",
              "                       [ 0.0168, -0.0177, -0.0168,  ..., -0.0218,  0.0119,  0.0183]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.0.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0182,  0.0068, -0.0146,  ..., -0.0292, -0.0128,  0.0351],\n",
              "                       [ 0.0115,  0.0113,  0.0045,  ..., -0.0122, -0.0237,  0.0253],\n",
              "                       [ 0.0011, -0.0129, -0.0208,  ...,  0.0043, -0.0023,  0.0086],\n",
              "                       ...,\n",
              "                       [ 0.0349, -0.0171, -0.0161,  ..., -0.0354,  0.0127, -0.0121],\n",
              "                       [ 0.0206, -0.0033, -0.0129,  ...,  0.0011, -0.0014, -0.0068],\n",
              "                       [-0.0066, -0.0352, -0.0244,  ...,  0.0359, -0.0110,  0.0034]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.encoder_attention.v_lin.bias',\n",
              "               tensor([ 0.0347,  0.0202, -0.0022,  ...,  0.0055, -0.0003, -0.0010],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0274, -0.0345,  0.0176,  ...,  0.0262, -0.0186,  0.0260],\n",
              "                       [-0.0469,  0.0240, -0.0219,  ...,  0.0242, -0.0043, -0.0275],\n",
              "                       [ 0.0090,  0.0064, -0.0092,  ..., -0.0042, -0.0311, -0.0061],\n",
              "                       ...,\n",
              "                       [ 0.0158, -0.0282,  0.0087,  ...,  0.0021,  0.0264,  0.0441],\n",
              "                       [-0.0154,  0.0146,  0.0075,  ...,  0.0328, -0.0173,  0.0003],\n",
              "                       [-0.0071,  0.0014, -0.0197,  ...,  0.0065, -0.0018,  0.0335]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0318, -0.0049,  0.0058,  ...,  0.0057,  0.0247, -0.0079],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.0.norm2.bias',\n",
              "               tensor([0.0625, 0.1165, 0.0262,  ..., 0.0336, 0.0511, 0.0622],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.ffn.lin1.weight',\n",
              "               tensor([[-4.2572e-02,  8.2910e-05,  5.3619e-02,  ..., -2.3621e-02,\n",
              "                        -3.9062e-03,  3.0304e-02],\n",
              "                       [-1.6983e-02,  1.1377e-03, -4.1351e-02,  ...,  3.3607e-03,\n",
              "                        -6.3210e-03, -1.0345e-02],\n",
              "                       [-6.3171e-03,  3.3752e-02,  3.6621e-02,  ...,  2.3460e-03,\n",
              "                         3.4199e-03, -1.3596e-02],\n",
              "                       ...,\n",
              "                       [ 3.1311e-02,  3.8055e-02, -1.2070e-02,  ..., -1.4870e-02,\n",
              "                        -4.3121e-02,  4.9866e-02],\n",
              "                       [ 1.8036e-02,  4.1885e-03, -6.9962e-03,  ..., -9.2316e-03,\n",
              "                         1.2512e-02, -3.6278e-03],\n",
              "                       [ 8.2703e-03,  2.4582e-02, -1.7853e-02,  ..., -1.3458e-02,\n",
              "                         5.1403e-04,  1.5701e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.0.ffn.lin1.bias',\n",
              "               tensor([-0.0222, -0.0031, -0.0073,  ...,  0.0045, -0.0116, -0.0127],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.ffn.lin2.weight',\n",
              "               tensor([[ 0.0101,  0.0236, -0.0212,  ..., -0.0251,  0.0068, -0.0309],\n",
              "                       [ 0.0117,  0.0004,  0.0094,  ..., -0.0079, -0.0165, -0.0129],\n",
              "                       [ 0.0100,  0.0217, -0.0199,  ...,  0.0367,  0.0090, -0.0250],\n",
              "                       ...,\n",
              "                       [-0.0198,  0.0048, -0.0126,  ..., -0.0013, -0.0328,  0.0033],\n",
              "                       [ 0.0046,  0.0006, -0.0095,  ..., -0.0215,  0.0237, -0.0323],\n",
              "                       [ 0.0138, -0.0085, -0.0243,  ..., -0.0327, -0.0335, -0.0479]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.ffn.lin2.bias',\n",
              "               tensor([ 0.0021, -0.0344,  0.0014,  ...,  0.0110,  0.0055,  0.0039],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.0.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.0.norm3.bias',\n",
              "               tensor([0.1173, 0.0887, 0.0611,  ..., 0.0625, 0.0717, 0.0105],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.self_attention.q_lin.weight',\n",
              "               tensor([[-0.0160,  0.0225, -0.0045,  ..., -0.0007, -0.0104, -0.0069],\n",
              "                       [ 0.0054, -0.0212, -0.0085,  ...,  0.0186,  0.0116,  0.0038],\n",
              "                       [-0.0148, -0.0061, -0.0203,  ...,  0.0228,  0.0066,  0.0198],\n",
              "                       ...,\n",
              "                       [-0.0644, -0.0042,  0.0060,  ...,  0.0350,  0.0264,  0.0299],\n",
              "                       [ 0.0190, -0.0065, -0.0070,  ..., -0.0398, -0.0210, -0.0035],\n",
              "                       [ 0.0081,  0.0052,  0.0061,  ...,  0.0042, -0.0137, -0.0328]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0229,  0.0035,  0.0144,  ..., -0.0220, -0.0360, -0.0187],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.self_attention.k_lin.weight',\n",
              "               tensor([[-0.0146, -0.0065, -0.0452,  ...,  0.0387,  0.0264, -0.0157],\n",
              "                       [-0.0053, -0.0139, -0.0278,  ...,  0.0256,  0.0215, -0.0165],\n",
              "                       [ 0.0560,  0.0039, -0.0043,  ...,  0.0148, -0.0173,  0.0129],\n",
              "                       ...,\n",
              "                       [ 0.0547,  0.0179, -0.0131,  ..., -0.0279,  0.0321,  0.0049],\n",
              "                       [ 0.0032, -0.0419, -0.0032,  ...,  0.0053, -0.0121, -0.0229],\n",
              "                       [ 0.0207, -0.0080,  0.0005,  ..., -0.0030,  0.0038,  0.0197]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.1.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0062,  0.0136, -0.0293,  ...,  0.0202, -0.0070, -0.0017],\n",
              "                       [-0.0113,  0.0151, -0.0091,  ..., -0.0034, -0.0048,  0.0330],\n",
              "                       [ 0.0222, -0.0316, -0.0283,  ..., -0.0324,  0.0037, -0.0046],\n",
              "                       ...,\n",
              "                       [-0.0210, -0.0131,  0.0131,  ...,  0.0073, -0.0009, -0.0413],\n",
              "                       [-0.0221, -0.0050, -0.0259,  ...,  0.0108, -0.0123, -0.0072],\n",
              "                       [-0.0060,  0.0314, -0.0139,  ..., -0.0079,  0.0262,  0.0018]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0026, -0.0034,  0.0134,  ...,  0.0104, -0.0028,  0.0055],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.self_attention.out_lin.weight',\n",
              "               tensor([[ 0.0048,  0.0154,  0.0034,  ..., -0.0220,  0.0186,  0.0207],\n",
              "                       [-0.0357,  0.0167,  0.0085,  ...,  0.0352, -0.0157,  0.0145],\n",
              "                       [ 0.0161,  0.0231,  0.0410,  ...,  0.0050,  0.0320, -0.0017],\n",
              "                       ...,\n",
              "                       [ 0.0157, -0.0249,  0.0080,  ...,  0.0020, -0.0294, -0.0189],\n",
              "                       [ 0.0452,  0.0163, -0.0051,  ..., -0.0094,  0.0032, -0.0092],\n",
              "                       [-0.0314,  0.0004,  0.0014,  ..., -0.0244,  0.0384, -0.0200]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.self_attention.out_lin.bias',\n",
              "               tensor([-0.0012, -0.0032, -0.0052,  ..., -0.0034,  0.0015, -0.0130],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.1.norm1.bias',\n",
              "               tensor([ 0.0053, -0.0227,  0.0349,  ..., -0.0487, -0.0759, -0.0278],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0155,  0.0062,  0.0349,  ..., -0.0054, -0.0450, -0.0170],\n",
              "                       [-0.0527,  0.0293, -0.0036,  ...,  0.0294, -0.0295,  0.0062],\n",
              "                       [-0.0168,  0.0171,  0.0043,  ..., -0.0042,  0.0133, -0.0204],\n",
              "                       ...,\n",
              "                       [ 0.0183, -0.0172, -0.0187,  ...,  0.0069, -0.0242,  0.0361],\n",
              "                       [ 0.0246, -0.0005, -0.0273,  ..., -0.0039,  0.0018,  0.0064],\n",
              "                       [ 0.0054, -0.0054, -0.0576,  ...,  0.0033, -0.0274,  0.0262]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.encoder_attention.q_lin.bias',\n",
              "               tensor([-0.0262,  0.0253, -0.0099,  ...,  0.0060,  0.0130,  0.0104],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0161, -0.0042,  0.0056,  ...,  0.0257,  0.0140, -0.0315],\n",
              "                       [-0.0033, -0.0536, -0.0226,  ..., -0.0024,  0.0004,  0.0109],\n",
              "                       [ 0.0167,  0.0061, -0.0187,  ...,  0.0042,  0.0335, -0.0151],\n",
              "                       ...,\n",
              "                       [-0.0257,  0.0125,  0.0133,  ...,  0.0022,  0.0092, -0.0034],\n",
              "                       [ 0.0063,  0.0214,  0.0561,  ...,  0.0457, -0.0064, -0.0266],\n",
              "                       [ 0.0152, -0.0308,  0.0302,  ..., -0.0051, -0.0035, -0.0147]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.1.encoder_attention.v_lin.weight',\n",
              "               tensor([[ 0.0124,  0.0235,  0.0119,  ..., -0.0197, -0.0014,  0.0269],\n",
              "                       [-0.0153,  0.0054,  0.0075,  ..., -0.0007,  0.0018,  0.0009],\n",
              "                       [-0.0216,  0.0017,  0.0069,  ...,  0.0387, -0.0074,  0.0042],\n",
              "                       ...,\n",
              "                       [ 0.0323, -0.0101, -0.0043,  ...,  0.0188,  0.0053,  0.0095],\n",
              "                       [-0.0159,  0.0076, -0.0200,  ...,  0.0228, -0.0160,  0.0112],\n",
              "                       [-0.0251, -0.0091,  0.0241,  ..., -0.0172,  0.0103,  0.0352]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.encoder_attention.v_lin.bias',\n",
              "               tensor([-0.0090,  0.0105, -0.0014,  ..., -0.0070, -0.0046, -0.0003],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0105, -0.0181, -0.0079,  ...,  0.0008,  0.0057, -0.0065],\n",
              "                       [ 0.0169, -0.0257, -0.0079,  ..., -0.0005,  0.0181, -0.0267],\n",
              "                       [-0.0074, -0.0068, -0.0044,  ...,  0.0224, -0.0034, -0.0224],\n",
              "                       ...,\n",
              "                       [ 0.0241, -0.0099, -0.0033,  ..., -0.0006,  0.0081,  0.0293],\n",
              "                       [-0.0092,  0.0044,  0.0106,  ..., -0.0216,  0.0185,  0.0218],\n",
              "                       [-0.0143, -0.0224, -0.0328,  ..., -0.0152, -0.0235, -0.0135]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0019, -0.0047,  0.0022,  ..., -0.0082, -0.0031, -0.0133],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.1.norm2.bias',\n",
              "               tensor([ 0.0097,  0.0948, -0.0522,  ...,  0.0412,  0.0336,  0.0418],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.ffn.lin1.weight',\n",
              "               tensor([[-0.0483,  0.0498,  0.0275,  ..., -0.0313, -0.0144, -0.0117],\n",
              "                       [ 0.0514, -0.0420,  0.0266,  ..., -0.0054, -0.0168,  0.0251],\n",
              "                       [ 0.0157,  0.0211, -0.0299,  ...,  0.0096, -0.0200, -0.0333],\n",
              "                       ...,\n",
              "                       [-0.0277,  0.0269,  0.0156,  ...,  0.0220, -0.0440,  0.0160],\n",
              "                       [ 0.0317,  0.0518, -0.0209,  ..., -0.0138, -0.0083, -0.0416],\n",
              "                       [-0.0237,  0.0404,  0.0293,  ..., -0.0373, -0.0204,  0.0148]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.ffn.lin1.bias',\n",
              "               tensor([-0.0103, -0.0113, -0.0108,  ..., -0.0262, -0.0168, -0.0034],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.ffn.lin2.weight',\n",
              "               tensor([[ 0.0279,  0.0101,  0.0003,  ..., -0.0080, -0.0385,  0.0025],\n",
              "                       [-0.0533,  0.0177, -0.0079,  ..., -0.0072, -0.0092, -0.0098],\n",
              "                       [-0.0130,  0.0258,  0.0428,  ..., -0.0491,  0.0156,  0.0154],\n",
              "                       ...,\n",
              "                       [ 0.0042, -0.0071,  0.0338,  ...,  0.0147, -0.0183,  0.0163],\n",
              "                       [ 0.0133,  0.0118, -0.0005,  ..., -0.0451, -0.0260,  0.0045],\n",
              "                       [ 0.0012, -0.0047,  0.0282,  ...,  0.0226,  0.0190, -0.0108]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.ffn.lin2.bias',\n",
              "               tensor([-0.0004, -0.0211,  0.0106,  ...,  0.0050,  0.0050, -0.0114],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.1.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.1.norm3.bias',\n",
              "               tensor([ 0.0078,  0.0763, -0.0450,  ...,  0.0490,  0.0808,  0.0015],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.self_attention.q_lin.weight',\n",
              "               tensor([[ 0.0067, -0.0056, -0.0315,  ..., -0.0061, -0.0098, -0.0197],\n",
              "                       [-0.0411,  0.0200,  0.0277,  ..., -0.0276,  0.0296, -0.0015],\n",
              "                       [ 0.0240,  0.0055,  0.0002,  ...,  0.0161,  0.0121, -0.0123],\n",
              "                       ...,\n",
              "                       [ 0.0259,  0.0357,  0.0230,  ..., -0.0333, -0.0346,  0.0098],\n",
              "                       [ 0.0341,  0.0397,  0.0136,  ...,  0.0017, -0.0105,  0.0330],\n",
              "                       [-0.0265, -0.0347, -0.0504,  ..., -0.0077,  0.0002, -0.0137]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0308, -0.0228, -0.0047,  ...,  0.0079, -0.0100, -0.0179],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.self_attention.k_lin.weight',\n",
              "               tensor([[-0.0051, -0.0059,  0.0254,  ...,  0.0436,  0.0145,  0.0047],\n",
              "                       [ 0.0064, -0.0447, -0.0080,  ..., -0.0304,  0.0093,  0.0025],\n",
              "                       [-0.0368,  0.0266, -0.0394,  ..., -0.0196,  0.0246,  0.0309],\n",
              "                       ...,\n",
              "                       [-0.0341, -0.0238,  0.0367,  ..., -0.0376, -0.0270,  0.0321],\n",
              "                       [ 0.0086, -0.0223, -0.0092,  ...,  0.0470, -0.0133, -0.0182],\n",
              "                       [-0.0079, -0.0113, -0.0308,  ..., -0.0104,  0.0280, -0.0043]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.2.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0127, -0.0108, -0.0131,  ...,  0.0192, -0.0054,  0.0193],\n",
              "                       [ 0.0118, -0.0313,  0.0131,  ...,  0.0091,  0.0111, -0.0004],\n",
              "                       [-0.0191, -0.0146,  0.0190,  ..., -0.0188,  0.0023, -0.0213],\n",
              "                       ...,\n",
              "                       [-0.0005, -0.0156, -0.0035,  ...,  0.0186, -0.0054, -0.0170],\n",
              "                       [ 0.0447,  0.0379, -0.0270,  ..., -0.0171,  0.0396, -0.0074],\n",
              "                       [ 0.0252,  0.0128, -0.0081,  ...,  0.0087,  0.0323,  0.0327]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.self_attention.v_lin.bias',\n",
              "               tensor([-8.2672e-05, -8.7814e-03,  9.0218e-04,  ...,  5.0697e-03,\n",
              "                        3.3665e-03, -5.9586e-03], dtype=torch.float16)),\n",
              "              ('decoder.layers.2.self_attention.out_lin.weight',\n",
              "               tensor([[-0.0113, -0.0096, -0.0010,  ..., -0.0043, -0.0381,  0.0156],\n",
              "                       [-0.0217, -0.0010, -0.0222,  ..., -0.0088, -0.0374,  0.0157],\n",
              "                       [-0.0038,  0.0157,  0.0448,  ..., -0.0090, -0.0059,  0.0142],\n",
              "                       ...,\n",
              "                       [ 0.0186, -0.0087, -0.0357,  ..., -0.0238,  0.0173, -0.0231],\n",
              "                       [-0.0184,  0.0016,  0.0272,  ...,  0.0031, -0.0064,  0.0311],\n",
              "                       [ 0.0331, -0.0450,  0.0104,  ..., -0.0047,  0.0222,  0.0154]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0047,  0.0039,  0.0015,  ..., -0.0066, -0.0019, -0.0151],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.2.norm1.bias',\n",
              "               tensor([-0.0051, -0.0132,  0.0269,  ..., -0.0323, -0.0626, -0.0079],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0188,  0.0211, -0.0632,  ..., -0.0054,  0.0193, -0.0020],\n",
              "                       [ 0.0249,  0.0091,  0.0137,  ..., -0.0457, -0.0378,  0.0079],\n",
              "                       [ 0.0067,  0.0207, -0.0525,  ..., -0.0053,  0.0417,  0.0350],\n",
              "                       ...,\n",
              "                       [-0.0209, -0.0153,  0.0251,  ...,  0.0150, -0.0002, -0.0113],\n",
              "                       [-0.0141,  0.0002,  0.0176,  ..., -0.0195, -0.0103, -0.0233],\n",
              "                       [ 0.0347, -0.0103, -0.0199,  ...,  0.0106, -0.0149, -0.0126]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0334, -0.0300,  0.0601,  ..., -0.0406,  0.0317,  0.0299],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0068, -0.0259,  0.0066,  ..., -0.0259, -0.0595,  0.0108],\n",
              "                       [ 0.0025, -0.0326,  0.0108,  ...,  0.0081, -0.0392,  0.0387],\n",
              "                       [-0.0183,  0.0193, -0.0141,  ..., -0.0079, -0.0266, -0.0230],\n",
              "                       ...,\n",
              "                       [-0.0003,  0.0100, -0.0120,  ..., -0.0009,  0.0118,  0.0081],\n",
              "                       [-0.0243, -0.0056,  0.0199,  ...,  0.0069,  0.0311, -0.0368],\n",
              "                       [ 0.0348, -0.0238, -0.0068,  ...,  0.0241, -0.0295,  0.0069]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.2.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0228, -0.0104, -0.0107,  ..., -0.0177,  0.0277, -0.0314],\n",
              "                       [-0.0189, -0.0024,  0.0059,  ...,  0.0243,  0.0183,  0.0122],\n",
              "                       [ 0.0222, -0.0378, -0.0130,  ...,  0.0132, -0.0083, -0.0246],\n",
              "                       ...,\n",
              "                       [-0.0157,  0.0244,  0.0257,  ...,  0.0353,  0.0128,  0.0235],\n",
              "                       [ 0.0088, -0.0049,  0.0189,  ..., -0.0103,  0.0032,  0.0351],\n",
              "                       [-0.0219,  0.0317,  0.0176,  ..., -0.0105,  0.0069,  0.0105]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.encoder_attention.v_lin.bias',\n",
              "               tensor([-0.0067,  0.0012,  0.0093,  ..., -0.0353, -0.0311, -0.0311],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.encoder_attention.out_lin.weight',\n",
              "               tensor([[-0.0428, -0.0146, -0.0123,  ..., -0.0176,  0.0109, -0.0073],\n",
              "                       [-0.0129,  0.0171, -0.0278,  ...,  0.0024,  0.0266,  0.0269],\n",
              "                       [-0.0223,  0.0077,  0.0032,  ...,  0.0127,  0.0209,  0.0117],\n",
              "                       ...,\n",
              "                       [-0.0335, -0.0270,  0.0212,  ..., -0.0312,  0.0008,  0.0220],\n",
              "                       [-0.0026, -0.0133, -0.0004,  ...,  0.0131,  0.0182,  0.0052],\n",
              "                       [-0.0176, -0.0022, -0.0146,  ..., -0.0447, -0.0378,  0.0129]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.encoder_attention.out_lin.bias',\n",
              "               tensor([-0.0012,  0.0041,  0.0071,  ..., -0.0132, -0.0082, -0.0115],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.2.norm2.bias',\n",
              "               tensor([ 0.0572,  0.0290, -0.0618,  ...,  0.0626,  0.0671,  0.0625],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.ffn.lin1.weight',\n",
              "               tensor([[ 0.0283, -0.0205,  0.0225,  ...,  0.0180,  0.0245,  0.0624],\n",
              "                       [-0.0242, -0.0453,  0.0079,  ...,  0.0153,  0.0125,  0.0128],\n",
              "                       [ 0.0088, -0.0045, -0.0466,  ..., -0.0248, -0.0069, -0.0120],\n",
              "                       ...,\n",
              "                       [ 0.0237, -0.0014,  0.0256,  ...,  0.0274,  0.0011,  0.0054],\n",
              "                       [ 0.0097, -0.0324, -0.0526,  ..., -0.0287, -0.0434,  0.0192],\n",
              "                       [-0.0207,  0.0300, -0.0062,  ..., -0.0313,  0.0195,  0.0189]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.ffn.lin1.bias',\n",
              "               tensor([ 0.0023, -0.0274, -0.0125,  ..., -0.0130, -0.0233, -0.0182],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.ffn.lin2.weight',\n",
              "               tensor([[-0.0050,  0.0229, -0.0094,  ..., -0.0180, -0.0129,  0.0443],\n",
              "                       [ 0.0130,  0.0269,  0.0068,  ...,  0.0288, -0.0031, -0.0131],\n",
              "                       [ 0.0106, -0.0132,  0.0049,  ..., -0.0133,  0.0131,  0.0221],\n",
              "                       ...,\n",
              "                       [ 0.0147, -0.0533, -0.0176,  ..., -0.0173, -0.0313,  0.0390],\n",
              "                       [ 0.0278, -0.0527, -0.0167,  ...,  0.0138,  0.0126,  0.0137],\n",
              "                       [ 0.0601, -0.0172,  0.0378,  ..., -0.0188,  0.0236, -0.0211]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.ffn.lin2.bias',\n",
              "               tensor([-0.0010, -0.0211, -0.0025,  ..., -0.0017,  0.0066, -0.0078],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.2.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.2.norm3.bias',\n",
              "               tensor([ 0.0019,  0.0097, -0.0618,  ...,  0.0408,  0.0977, -0.0287],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.self_attention.q_lin.weight',\n",
              "               tensor([[-0.0052, -0.0234, -0.0008,  ..., -0.0151, -0.0002,  0.0173],\n",
              "                       [ 0.0005, -0.0017,  0.0321,  ..., -0.0018,  0.0234,  0.0188],\n",
              "                       [ 0.0090,  0.0036,  0.0047,  ..., -0.0003, -0.0487, -0.0154],\n",
              "                       ...,\n",
              "                       [-0.0364, -0.0452,  0.0221,  ..., -0.0497, -0.0034,  0.0301],\n",
              "                       [ 0.0155, -0.0015, -0.0052,  ..., -0.0118,  0.0160,  0.0107],\n",
              "                       [-0.0227,  0.0095,  0.0164,  ..., -0.0525, -0.0162, -0.0155]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0005,  0.0079,  0.0011,  ..., -0.0090,  0.0164,  0.0028],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0241, -0.0298,  0.0041,  ...,  0.0297, -0.0524, -0.0036],\n",
              "                       [ 0.0139,  0.0207,  0.0026,  ..., -0.0182,  0.0069, -0.0152],\n",
              "                       [-0.0008,  0.0027, -0.0570,  ..., -0.0018, -0.0103,  0.0361],\n",
              "                       ...,\n",
              "                       [-0.0151, -0.0301,  0.0098,  ..., -0.0063, -0.0258, -0.0329],\n",
              "                       [ 0.0111, -0.0249, -0.0320,  ..., -0.0027,  0.0255,  0.0101],\n",
              "                       [-0.0522,  0.0246,  0.0118,  ...,  0.0245, -0.0222, -0.0177]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.3.self_attention.v_lin.weight',\n",
              "               tensor([[ 0.0087, -0.0397,  0.0249,  ...,  0.0076,  0.0200, -0.0110],\n",
              "                       [ 0.0153,  0.0134, -0.0147,  ..., -0.0034,  0.0117, -0.0015],\n",
              "                       [ 0.0202,  0.0123, -0.0328,  ...,  0.0387, -0.0279, -0.0191],\n",
              "                       ...,\n",
              "                       [ 0.0204, -0.0201,  0.0438,  ..., -0.0179, -0.0211,  0.0097],\n",
              "                       [ 0.0167,  0.0026,  0.0029,  ...,  0.0212, -0.0222, -0.0164],\n",
              "                       [-0.0185,  0.0402, -0.0151,  ..., -0.0174, -0.0374, -0.0196]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0151,  0.0049,  0.0053,  ...,  0.0076, -0.0006, -0.0055],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.self_attention.out_lin.weight',\n",
              "               tensor([[-0.0283,  0.0102, -0.0248,  ..., -0.0105,  0.0053, -0.0012],\n",
              "                       [-0.0070, -0.0103, -0.0219,  ...,  0.0378, -0.0133,  0.0056],\n",
              "                       [ 0.0020, -0.0058, -0.0353,  ..., -0.0240,  0.0026,  0.0241],\n",
              "                       ...,\n",
              "                       [-0.0060,  0.0194, -0.0242,  ..., -0.0386, -0.0244,  0.0141],\n",
              "                       [-0.0196, -0.0028,  0.0297,  ..., -0.0059, -0.0117, -0.0035],\n",
              "                       [-0.0110,  0.0224,  0.0300,  ..., -0.0217,  0.0054,  0.0095]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.self_attention.out_lin.bias',\n",
              "               tensor([-0.0107,  0.0072,  0.0046,  ..., -0.0005,  0.0040, -0.0122],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.3.norm1.bias',\n",
              "               tensor([-0.0123, -0.0261,  0.0228,  ..., -0.0173, -0.0638,  0.0075],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0027, -0.0138,  0.0324,  ...,  0.0012,  0.0003,  0.0336],\n",
              "                       [-0.0077, -0.0163,  0.0207,  ...,  0.0334,  0.0040, -0.0111],\n",
              "                       [ 0.0147,  0.0163, -0.0371,  ...,  0.0330,  0.0151, -0.0100],\n",
              "                       ...,\n",
              "                       [-0.0285,  0.0150,  0.0065,  ...,  0.0052, -0.0146,  0.0402],\n",
              "                       [-0.0097,  0.0171,  0.0045,  ..., -0.0036, -0.0199,  0.0150],\n",
              "                       [-0.0001, -0.0155, -0.0181,  ...,  0.0258, -0.0350,  0.0318]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0500,  0.0091, -0.0140,  ..., -0.0002,  0.0200, -0.0148],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0111,  0.0283, -0.0105,  ..., -0.0219, -0.0215, -0.0154],\n",
              "                       [-0.0085, -0.0184, -0.0065,  ...,  0.0371, -0.0025, -0.0358],\n",
              "                       [ 0.0106, -0.0083,  0.0021,  ..., -0.0120,  0.0013,  0.0325],\n",
              "                       ...,\n",
              "                       [ 0.0233, -0.0418, -0.0133,  ...,  0.0095, -0.0314,  0.0085],\n",
              "                       [ 0.0158,  0.0067,  0.0221,  ..., -0.0428, -0.0046, -0.0069],\n",
              "                       [-0.0085, -0.0211, -0.0240,  ..., -0.0282, -0.0061,  0.0116]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.3.encoder_attention.v_lin.weight',\n",
              "               tensor([[ 0.0419,  0.0094, -0.0148,  ..., -0.0266,  0.0245, -0.0336],\n",
              "                       [-0.0137,  0.0583,  0.0031,  ...,  0.0253,  0.0033,  0.0142],\n",
              "                       [-0.0032,  0.0058, -0.0113,  ..., -0.0089, -0.0107, -0.0227],\n",
              "                       ...,\n",
              "                       [ 0.0098, -0.0204,  0.0306,  ..., -0.0005,  0.0184, -0.0108],\n",
              "                       [-0.0078,  0.0017, -0.0085,  ..., -0.0017,  0.0359, -0.0026],\n",
              "                       [ 0.0096, -0.0463,  0.0077,  ..., -0.0220, -0.0295,  0.0230]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.encoder_attention.v_lin.bias',\n",
              "               tensor([-0.0011,  0.0018,  0.0059,  ..., -0.0044,  0.0024, -0.0132],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0140, -0.0049,  0.0272,  ...,  0.0204,  0.0059, -0.0468],\n",
              "                       [ 0.0401,  0.0104,  0.0115,  ...,  0.0327, -0.0217, -0.0150],\n",
              "                       [-0.0112, -0.0013, -0.0370,  ..., -0.0016,  0.0119, -0.0138],\n",
              "                       ...,\n",
              "                       [-0.0506, -0.0080, -0.0037,  ..., -0.0114,  0.0366,  0.0205],\n",
              "                       [ 0.0014, -0.0441,  0.0049,  ...,  0.0125, -0.0235,  0.0475],\n",
              "                       [ 0.0142, -0.0309, -0.0021,  ..., -0.0346, -0.0100,  0.0130]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.encoder_attention.out_lin.bias',\n",
              "               tensor([-6.0616e-03,  5.6505e-05,  7.3166e-03,  ..., -7.0915e-03,\n",
              "                        9.5224e-04, -1.9043e-02], dtype=torch.float16)),\n",
              "              ('decoder.layers.3.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.3.norm2.bias',\n",
              "               tensor([-0.0328,  0.0725, -0.0179,  ...,  0.0345,  0.0620,  0.0218],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.ffn.lin1.weight',\n",
              "               tensor([[ 0.0161,  0.0365, -0.0238,  ..., -0.0384, -0.0599,  0.0352],\n",
              "                       [-0.0271, -0.0248,  0.0435,  ..., -0.0216, -0.0324, -0.0082],\n",
              "                       [ 0.0423, -0.0259,  0.0371,  ..., -0.0172,  0.0083, -0.0217],\n",
              "                       ...,\n",
              "                       [ 0.0281,  0.0118,  0.0261,  ...,  0.0074, -0.0044, -0.0248],\n",
              "                       [ 0.0107, -0.0089, -0.0158,  ...,  0.0048,  0.0197, -0.0225],\n",
              "                       [ 0.0270,  0.0025, -0.0155,  ...,  0.0141, -0.0163, -0.0008]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.ffn.lin1.bias',\n",
              "               tensor([-0.0373, -0.0140, -0.0142,  ..., -0.0021, -0.0161, -0.0105],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.ffn.lin2.weight',\n",
              "               tensor([[-2.1317e-02,  1.4977e-02,  5.3558e-03,  ..., -2.9739e-02,\n",
              "                        -7.7009e-04, -3.5126e-02],\n",
              "                       [-1.5244e-02,  3.2067e-05, -1.2459e-02,  ..., -8.3542e-03,\n",
              "                        -9.9640e-03, -1.8570e-02],\n",
              "                       [-3.3447e-02, -7.5455e-03, -1.5152e-02,  ..., -2.3666e-02,\n",
              "                        -2.4994e-02,  1.9058e-02],\n",
              "                       ...,\n",
              "                       [-3.2902e-03,  4.8370e-03,  2.0187e-02,  ..., -2.9831e-03,\n",
              "                        -1.3687e-02,  2.0767e-02],\n",
              "                       [ 1.7471e-02,  1.6022e-02, -7.8201e-03,  ...,  1.4137e-02,\n",
              "                         5.8212e-03,  1.6510e-02],\n",
              "                       [ 3.4698e-02,  1.7975e-02,  1.6357e-02,  ...,  1.7593e-02,\n",
              "                        -2.2171e-02,  1.9836e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.3.ffn.lin2.bias',\n",
              "               tensor([-0.0011, -0.0153, -0.0035,  ..., -0.0018,  0.0091, -0.0145],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.3.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.3.norm3.bias',\n",
              "               tensor([ 0.0049, -0.0068, -0.0122,  ...,  0.0212,  0.0767, -0.0523],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.self_attention.q_lin.weight',\n",
              "               tensor([[-1.8311e-02, -3.6201e-03,  1.9409e-02,  ...,  1.0843e-03,\n",
              "                         3.1662e-03, -1.7509e-03],\n",
              "                       [-2.4597e-02,  1.2230e-02, -2.1393e-02,  ..., -5.7793e-04,\n",
              "                        -3.1464e-02, -1.5625e-02],\n",
              "                       [ 9.2850e-03,  3.7551e-04, -2.5085e-02,  ...,  2.4033e-02,\n",
              "                         2.0493e-02, -7.9956e-03],\n",
              "                       ...,\n",
              "                       [ 6.2218e-03,  6.5184e-04, -8.7023e-06,  ..., -1.7967e-03,\n",
              "                         1.9806e-02, -3.1769e-02],\n",
              "                       [-9.1553e-03, -3.3051e-02, -4.1695e-03,  ..., -1.1726e-02,\n",
              "                         1.3000e-02,  1.0796e-02],\n",
              "                       [ 4.7607e-03, -2.7969e-02,  7.4530e-04,  ..., -6.5384e-03,\n",
              "                        -4.6112e-02, -9.7275e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.4.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0153, -0.0116,  0.0135,  ...,  0.0006, -0.0031, -0.0039],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.self_attention.k_lin.weight',\n",
              "               tensor([[-0.0003,  0.0253,  0.0363,  ..., -0.0294, -0.0139, -0.0064],\n",
              "                       [ 0.0274,  0.0037,  0.0349,  ...,  0.0120,  0.0273,  0.0204],\n",
              "                       [ 0.0136,  0.0101,  0.0016,  ..., -0.0431,  0.0102,  0.0349],\n",
              "                       ...,\n",
              "                       [-0.0085,  0.0085,  0.0371,  ..., -0.0235, -0.0185, -0.0351],\n",
              "                       [ 0.0055, -0.0304, -0.0249,  ..., -0.0002,  0.0010, -0.0478],\n",
              "                       [-0.0094,  0.0262,  0.0399,  ...,  0.0155, -0.0323, -0.0518]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.4.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0046, -0.0193, -0.0098,  ...,  0.0219, -0.0282,  0.0019],\n",
              "                       [-0.0096, -0.0302, -0.0103,  ..., -0.0190, -0.0183, -0.0103],\n",
              "                       [ 0.0172,  0.0264,  0.0320,  ...,  0.0025, -0.0048,  0.0151],\n",
              "                       ...,\n",
              "                       [ 0.0141,  0.0051,  0.0092,  ...,  0.0035, -0.0346, -0.0372],\n",
              "                       [ 0.0127, -0.0011, -0.0088,  ...,  0.0106,  0.0461,  0.0068],\n",
              "                       [-0.0251, -0.0301,  0.0062,  ...,  0.0037, -0.0079, -0.0457]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0038, -0.0039,  0.0234,  ...,  0.0020, -0.0052, -0.0092],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.self_attention.out_lin.weight',\n",
              "               tensor([[-5.0783e-04,  3.1528e-03, -1.9379e-02,  ...,  2.2354e-02,\n",
              "                        -3.7498e-03,  2.1835e-02],\n",
              "                       [-5.2567e-03, -7.9880e-03,  9.0637e-03,  ..., -6.9761e-04,\n",
              "                        -8.1482e-03, -6.9809e-03],\n",
              "                       [-2.8381e-02,  4.5853e-03,  4.6272e-03,  ..., -6.4964e-03,\n",
              "                         3.6659e-03, -1.0977e-03],\n",
              "                       ...,\n",
              "                       [-1.4809e-02, -1.3145e-02,  2.4170e-02,  ..., -4.0344e-02,\n",
              "                        -2.0645e-02, -1.3939e-02],\n",
              "                       [-3.5583e-02, -8.2321e-03,  2.7908e-02,  ...,  3.9597e-03,\n",
              "                        -2.7634e-02, -1.6153e-05],\n",
              "                       [ 1.3939e-02, -1.3428e-02,  6.6490e-03,  ...,  4.4312e-02,\n",
              "                        -2.5742e-02,  7.6218e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.4.self_attention.out_lin.bias',\n",
              "               tensor([-0.0059,  0.0010, -0.0029,  ...,  0.0039,  0.0122, -0.0154],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.4.norm1.bias',\n",
              "               tensor([-0.0095, -0.0303,  0.0192,  ..., -0.0224, -0.0630,  0.0210],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0200, -0.0138, -0.0334,  ...,  0.0211, -0.0141, -0.0360],\n",
              "                       [-0.0209, -0.0032, -0.0001,  ...,  0.0398,  0.0114, -0.0396],\n",
              "                       [-0.0027, -0.0085,  0.0081,  ..., -0.0362, -0.0045, -0.0090],\n",
              "                       ...,\n",
              "                       [-0.0320,  0.0024,  0.0346,  ...,  0.0053, -0.0079, -0.0011],\n",
              "                       [ 0.0178,  0.0019,  0.0319,  ...,  0.0247, -0.0038, -0.0200],\n",
              "                       [-0.0083, -0.0045,  0.0234,  ...,  0.0339,  0.0239,  0.0010]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.encoder_attention.q_lin.bias',\n",
              "               tensor([ 3.7909e-05,  1.6312e-02, -1.4896e-03,  ...,  1.0471e-03,\n",
              "                       -1.3763e-02,  1.3802e-02], dtype=torch.float16)),\n",
              "              ('decoder.layers.4.encoder_attention.k_lin.weight',\n",
              "               tensor([[-0.0344, -0.0097,  0.0435,  ..., -0.0306, -0.0326,  0.0044],\n",
              "                       [ 0.0281,  0.0118, -0.0063,  ...,  0.0408, -0.0118, -0.0110],\n",
              "                       [-0.0149, -0.0183,  0.0237,  ...,  0.0103, -0.0193,  0.0173],\n",
              "                       ...,\n",
              "                       [-0.0081,  0.0174, -0.0313,  ..., -0.0375, -0.0029, -0.0218],\n",
              "                       [ 0.0127,  0.0252, -0.0040,  ...,  0.0018, -0.0151,  0.0137],\n",
              "                       [ 0.0153,  0.0028,  0.0048,  ...,  0.0005,  0.0153, -0.0199]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.4.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0035,  0.0101, -0.0168,  ...,  0.0108, -0.0025,  0.0331],\n",
              "                       [-0.0044, -0.0001, -0.0091,  ...,  0.0032,  0.0136,  0.0038],\n",
              "                       [ 0.0261,  0.0304, -0.0199,  ...,  0.0316,  0.0153,  0.0260],\n",
              "                       ...,\n",
              "                       [-0.0109,  0.0103, -0.0240,  ..., -0.0259, -0.0003,  0.0076],\n",
              "                       [-0.0258, -0.0202, -0.0023,  ..., -0.0087,  0.0162,  0.0118],\n",
              "                       [-0.0368, -0.0167,  0.0260,  ...,  0.0094,  0.0019, -0.0227]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.encoder_attention.v_lin.bias',\n",
              "               tensor([ 0.0050, -0.0095,  0.0100,  ..., -0.0313, -0.0004,  0.0092],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.encoder_attention.out_lin.weight',\n",
              "               tensor([[-0.0036,  0.0261, -0.0040,  ...,  0.0474,  0.0209, -0.0209],\n",
              "                       [ 0.0079,  0.0033,  0.0363,  ...,  0.0022,  0.0007,  0.0002],\n",
              "                       [ 0.0131, -0.0179,  0.0242,  ..., -0.0114, -0.0050, -0.0014],\n",
              "                       ...,\n",
              "                       [-0.0395,  0.0121,  0.0240,  ..., -0.0169,  0.0342, -0.0272],\n",
              "                       [ 0.0140, -0.0148,  0.0228,  ..., -0.0109, -0.0131, -0.0169],\n",
              "                       [ 0.0312, -0.0142, -0.0104,  ...,  0.0232,  0.0199,  0.0132]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0001, -0.0002,  0.0007,  ..., -0.0007,  0.0029, -0.0136],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.4.norm2.bias',\n",
              "               tensor([ 0.0302,  0.0928, -0.0076,  ...,  0.0282,  0.0671, -0.0035],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.ffn.lin1.weight',\n",
              "               tensor([[ 0.0330, -0.0062,  0.0233,  ...,  0.0122, -0.0065, -0.0098],\n",
              "                       [ 0.0217, -0.0306,  0.0014,  ...,  0.0189, -0.0033, -0.0148],\n",
              "                       [ 0.0183,  0.0042,  0.0020,  ..., -0.0334,  0.0118,  0.0489],\n",
              "                       ...,\n",
              "                       [-0.0163,  0.0169,  0.0446,  ...,  0.0020, -0.0308,  0.0355],\n",
              "                       [ 0.0276,  0.0323,  0.0044,  ...,  0.0079, -0.0207, -0.0226],\n",
              "                       [-0.0354,  0.0117,  0.0303,  ..., -0.0067, -0.0178, -0.0170]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.ffn.lin1.bias',\n",
              "               tensor([-0.0069, -0.0309, -0.0059,  ..., -0.0134, -0.0195, -0.0173],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.ffn.lin2.weight',\n",
              "               tensor([[-0.0612,  0.0011, -0.0294,  ..., -0.0323, -0.0051, -0.0083],\n",
              "                       [ 0.0005, -0.0144, -0.0065,  ..., -0.0027, -0.0103, -0.0241],\n",
              "                       [-0.0280,  0.0092, -0.0307,  ..., -0.0116,  0.0181,  0.0046],\n",
              "                       ...,\n",
              "                       [-0.0186, -0.0271,  0.0131,  ..., -0.0013, -0.0122,  0.0246],\n",
              "                       [-0.0123,  0.0080,  0.0003,  ..., -0.0227,  0.0108,  0.0172],\n",
              "                       [ 0.0048, -0.0132, -0.0104,  ..., -0.0020,  0.0376, -0.0007]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.ffn.lin2.bias',\n",
              "               tensor([-0.0079, -0.0124,  0.0012,  ...,  0.0061,  0.0088, -0.0150],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.4.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.4.norm3.bias',\n",
              "               tensor([ 0.0063, -0.0085, -0.0124,  ...,  0.0235,  0.0716, -0.0560],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.self_attention.q_lin.weight',\n",
              "               tensor([[ 0.0317, -0.0027, -0.0016,  ..., -0.0333,  0.0388,  0.0166],\n",
              "                       [-0.0180, -0.0054,  0.0110,  ...,  0.0231, -0.0121, -0.0110],\n",
              "                       [ 0.0088,  0.0152, -0.0415,  ...,  0.0360, -0.0064,  0.0186],\n",
              "                       ...,\n",
              "                       [ 0.0084, -0.0056,  0.0282,  ..., -0.0092,  0.0200, -0.0080],\n",
              "                       [ 0.0022, -0.0248, -0.0002,  ...,  0.0145,  0.0036, -0.0257],\n",
              "                       [ 0.0294, -0.0539, -0.0142,  ...,  0.0253, -0.0331,  0.0095]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0146, -0.0242, -0.0062,  ...,  0.0077,  0.0493, -0.0040],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.self_attention.k_lin.weight',\n",
              "               tensor([[-0.0081, -0.0187, -0.0165,  ..., -0.0145,  0.0227, -0.0495],\n",
              "                       [-0.0358,  0.0025, -0.0580,  ...,  0.0016,  0.0153, -0.0114],\n",
              "                       [ 0.0246, -0.0110, -0.0057,  ..., -0.0239, -0.0450,  0.0191],\n",
              "                       ...,\n",
              "                       [-0.0087, -0.0043,  0.0054,  ...,  0.0567,  0.0012,  0.0192],\n",
              "                       [ 0.0269, -0.0070,  0.0059,  ...,  0.0456,  0.0038,  0.0043],\n",
              "                       [ 0.0238,  0.0031,  0.0028,  ..., -0.0030,  0.0080, -0.0017]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.5.self_attention.v_lin.weight',\n",
              "               tensor([[ 0.0031,  0.0449, -0.0316,  ...,  0.0098,  0.0032,  0.0101],\n",
              "                       [ 0.0334, -0.0150, -0.0395,  ..., -0.0329, -0.0068,  0.0384],\n",
              "                       [-0.0158, -0.0312,  0.0042,  ..., -0.0319,  0.0078, -0.0218],\n",
              "                       ...,\n",
              "                       [ 0.0061,  0.0099,  0.0392,  ...,  0.0249,  0.0152, -0.0171],\n",
              "                       [ 0.0170, -0.0242, -0.0246,  ...,  0.0417, -0.0174, -0.0037],\n",
              "                       [ 0.0313, -0.0059, -0.0341,  ...,  0.0191, -0.0036, -0.0230]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0007, -0.0032,  0.0137,  ...,  0.0089, -0.0094, -0.0166],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.self_attention.out_lin.weight',\n",
              "               tensor([[-0.0333, -0.0141,  0.0141,  ..., -0.0114,  0.0183,  0.0194],\n",
              "                       [-0.0115, -0.0123, -0.0394,  ...,  0.0103,  0.0208,  0.0071],\n",
              "                       [-0.0115, -0.0186,  0.0044,  ..., -0.0019, -0.0086, -0.0002],\n",
              "                       ...,\n",
              "                       [-0.0380,  0.0354, -0.0126,  ..., -0.0594,  0.0061, -0.0509],\n",
              "                       [-0.0087,  0.0152,  0.0197,  ...,  0.0032,  0.0026,  0.0043],\n",
              "                       [ 0.0240, -0.0596,  0.0015,  ..., -0.0088, -0.0289, -0.0117]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0030,  0.0046, -0.0102,  ...,  0.0071,  0.0079, -0.0124],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.5.norm1.bias',\n",
              "               tensor([-0.0058, -0.0394,  0.0091,  ..., -0.0154, -0.0414,  0.0356],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.encoder_attention.q_lin.weight',\n",
              "               tensor([[ 0.0150,  0.0170,  0.0328,  ..., -0.0080,  0.0264,  0.0063],\n",
              "                       [ 0.0452,  0.0034, -0.0318,  ..., -0.0341, -0.0046, -0.0109],\n",
              "                       [ 0.0382,  0.0275, -0.0006,  ..., -0.0129, -0.0136,  0.0202],\n",
              "                       ...,\n",
              "                       [-0.0031, -0.0146,  0.0347,  ...,  0.0363, -0.0349, -0.0075],\n",
              "                       [ 0.0151, -0.0115,  0.0215,  ..., -0.0117,  0.0451,  0.0202],\n",
              "                       [ 0.0101, -0.0309, -0.0003,  ...,  0.0015,  0.0314, -0.0273]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0176,  0.0325,  0.0037,  ..., -0.0199,  0.0195, -0.0135],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0320, -0.0094, -0.0370,  ..., -0.0316,  0.0231,  0.0191],\n",
              "                       [-0.0196, -0.0116,  0.0111,  ...,  0.0041,  0.0170,  0.0188],\n",
              "                       [-0.0107, -0.0066,  0.0118,  ..., -0.0147, -0.0049, -0.0049],\n",
              "                       ...,\n",
              "                       [-0.0161, -0.0127,  0.0104,  ...,  0.0038,  0.0071,  0.0268],\n",
              "                       [ 0.0309,  0.0116, -0.0400,  ...,  0.0203,  0.0296,  0.0048],\n",
              "                       [ 0.0086, -0.0047, -0.0095,  ..., -0.0153,  0.0326,  0.0210]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.5.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0363,  0.0300, -0.0262,  ..., -0.0104, -0.0324,  0.0169],\n",
              "                       [-0.0296,  0.0279, -0.0102,  ...,  0.0237,  0.0334,  0.0020],\n",
              "                       [-0.0212, -0.0330,  0.0036,  ..., -0.0308,  0.0023,  0.0220],\n",
              "                       ...,\n",
              "                       [ 0.0116, -0.0026, -0.0182,  ...,  0.0182,  0.0145,  0.0173],\n",
              "                       [-0.0211, -0.0149, -0.0120,  ..., -0.0444, -0.0017, -0.0069],\n",
              "                       [ 0.0688, -0.0323,  0.0223,  ...,  0.0199, -0.0022, -0.0366]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.encoder_attention.v_lin.bias',\n",
              "               tensor([ 0.0099,  0.0053, -0.0148,  ...,  0.0107, -0.0020, -0.0121],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.encoder_attention.out_lin.weight',\n",
              "               tensor([[-0.0147, -0.0219, -0.0081,  ..., -0.0202,  0.0350,  0.0132],\n",
              "                       [ 0.0060,  0.0108, -0.0094,  ..., -0.0215,  0.0323,  0.0276],\n",
              "                       [-0.0395, -0.0033,  0.0041,  ...,  0.0354, -0.0027, -0.0436],\n",
              "                       ...,\n",
              "                       [-0.0396, -0.0279,  0.0487,  ..., -0.0099,  0.0107,  0.0057],\n",
              "                       [-0.0021,  0.0044,  0.0312,  ...,  0.0298,  0.0208,  0.0400],\n",
              "                       [-0.0627,  0.0240,  0.0363,  ..., -0.0148,  0.0179,  0.0011]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.encoder_attention.out_lin.bias',\n",
              "               tensor([-0.0050,  0.0029, -0.0073,  ...,  0.0041, -0.0003, -0.0191],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.5.norm2.bias',\n",
              "               tensor([ 0.0091,  0.0668, -0.0073,  ...,  0.0542,  0.0613, -0.0158],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.ffn.lin1.weight',\n",
              "               tensor([[ 0.0137,  0.0176,  0.0010,  ...,  0.0288,  0.0043, -0.0353],\n",
              "                       [-0.0070,  0.0085, -0.0073,  ..., -0.0095, -0.0338,  0.0468],\n",
              "                       [ 0.0083,  0.0080, -0.0222,  ...,  0.0222, -0.0022,  0.0243],\n",
              "                       ...,\n",
              "                       [ 0.0106,  0.0012,  0.0419,  ..., -0.0584,  0.0114,  0.0220],\n",
              "                       [-0.0315,  0.0267,  0.0309,  ..., -0.0357, -0.0040,  0.0195],\n",
              "                       [-0.0092, -0.0024, -0.0004,  ...,  0.0045, -0.0126,  0.0171]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.ffn.lin1.bias',\n",
              "               tensor([-0.0010, -0.0143, -0.0123,  ..., -0.0007, -0.0105, -0.0285],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.ffn.lin2.weight',\n",
              "               tensor([[-0.0325,  0.0342,  0.0297,  ...,  0.0061,  0.0518,  0.0249],\n",
              "                       [-0.0078,  0.0117, -0.0226,  ..., -0.0253, -0.0300, -0.0419],\n",
              "                       [ 0.0300, -0.0288,  0.0167,  ..., -0.0379, -0.0058, -0.0056],\n",
              "                       ...,\n",
              "                       [ 0.0025,  0.0217, -0.0264,  ...,  0.0241, -0.0087, -0.0142],\n",
              "                       [-0.0166,  0.0163,  0.0199,  ..., -0.0307,  0.0220, -0.0259],\n",
              "                       [ 0.0397, -0.0210, -0.0144,  ...,  0.0086,  0.0012, -0.0034]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.ffn.lin2.bias',\n",
              "               tensor([-0.0168, -0.0160, -0.0108,  ...,  0.0217,  0.0127, -0.0231],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.5.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.5.norm3.bias',\n",
              "               tensor([ 0.0265, -0.0201, -0.0106,  ...,  0.0162,  0.0769, -0.0646],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.self_attention.q_lin.weight',\n",
              "               tensor([[-1.1971e-02, -6.6948e-03,  5.2490e-03,  ...,  2.2232e-02,\n",
              "                         4.2801e-03,  1.3977e-02],\n",
              "                       [ 4.6326e-02,  1.0895e-02, -6.9695e-03,  ...,  4.3182e-03,\n",
              "                        -2.2583e-02,  2.3479e-03],\n",
              "                       [-3.8849e-02, -1.7609e-02,  1.1635e-02,  ...,  3.0914e-02,\n",
              "                        -1.1854e-03,  3.5400e-02],\n",
              "                       ...,\n",
              "                       [ 2.0859e-02,  1.8892e-03, -9.6588e-03,  ..., -3.0487e-02,\n",
              "                         1.3947e-02,  2.9030e-03],\n",
              "                       [-5.0240e-03,  1.2344e-02, -7.7784e-05,  ...,  1.9592e-02,\n",
              "                         3.0533e-02, -4.7028e-02],\n",
              "                       [-2.4261e-02,  6.2370e-03,  2.5848e-02,  ...,  6.5193e-03,\n",
              "                         6.0730e-02,  9.8419e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.6.self_attention.q_lin.bias',\n",
              "               tensor([-0.0113,  0.0130, -0.0028,  ...,  0.0262,  0.0099,  0.0109],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0261,  0.0150, -0.0107,  ...,  0.0222,  0.0066,  0.0124],\n",
              "                       [-0.0092, -0.0150,  0.0217,  ..., -0.0007, -0.0105,  0.0034],\n",
              "                       [ 0.0013,  0.0189,  0.0036,  ..., -0.0050,  0.0170, -0.0099],\n",
              "                       ...,\n",
              "                       [ 0.0125,  0.0157, -0.0110,  ..., -0.0328, -0.0146,  0.0352],\n",
              "                       [-0.0050,  0.0010, -0.0096,  ..., -0.0420,  0.0222, -0.0145],\n",
              "                       [-0.0069, -0.0024,  0.0507,  ...,  0.0025,  0.0262, -0.0194]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.6.self_attention.v_lin.weight',\n",
              "               tensor([[ 1.6159e-02, -9.5978e-03, -1.7700e-02,  ...,  8.7280e-03,\n",
              "                         2.7725e-02,  3.2959e-02],\n",
              "                       [ 1.6373e-02,  5.3825e-03,  1.7334e-02,  ..., -7.2708e-03,\n",
              "                        -2.2034e-02,  1.4734e-03],\n",
              "                       [ 2.2049e-02, -2.4063e-02, -2.9144e-02,  ...,  3.9764e-02,\n",
              "                         3.7567e-02, -2.2354e-02],\n",
              "                       ...,\n",
              "                       [ 1.2589e-02,  2.5864e-02,  3.8147e-03,  ...,  2.1469e-02,\n",
              "                        -1.0857e-02,  2.4933e-02],\n",
              "                       [ 3.8666e-02, -2.8000e-02, -9.8648e-03,  ...,  1.3283e-02,\n",
              "                        -1.3313e-02,  3.2166e-02],\n",
              "                       [ 4.2236e-02, -1.4938e-02, -1.8206e-03,  ..., -1.0216e-02,\n",
              "                        -3.2410e-02, -8.2552e-05]], dtype=torch.float16)),\n",
              "              ('decoder.layers.6.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0055, -0.0006,  0.0033,  ...,  0.0029,  0.0010, -0.0141],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.self_attention.out_lin.weight',\n",
              "               tensor([[-0.0018, -0.0657,  0.0381,  ..., -0.0159, -0.0046, -0.0081],\n",
              "                       [ 0.0040, -0.0029,  0.0128,  ...,  0.0154,  0.0193,  0.0295],\n",
              "                       [ 0.0120,  0.0088, -0.0052,  ..., -0.0097, -0.0090, -0.0356],\n",
              "                       ...,\n",
              "                       [ 0.0136, -0.0063,  0.0166,  ..., -0.0151, -0.0427, -0.0139],\n",
              "                       [ 0.0087, -0.0085, -0.0284,  ...,  0.0131, -0.0249,  0.0262],\n",
              "                       [-0.0295,  0.0029, -0.0013,  ...,  0.0214, -0.0287,  0.0254]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.self_attention.out_lin.bias',\n",
              "               tensor([-0.0092,  0.0094, -0.0064,  ...,  0.0163,  0.0151, -0.0137],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.6.norm1.bias',\n",
              "               tensor([-0.0095, -0.0437, -0.0019,  ..., -0.0096, -0.0325,  0.0184],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0135,  0.0072,  0.0149,  ...,  0.0081, -0.0271,  0.0160],\n",
              "                       [ 0.0417, -0.0184,  0.0198,  ...,  0.0195,  0.0053, -0.0043],\n",
              "                       [ 0.0005, -0.0052, -0.0034,  ..., -0.0020,  0.0345, -0.0159],\n",
              "                       ...,\n",
              "                       [ 0.0177, -0.0101,  0.0193,  ..., -0.0035,  0.0150, -0.0137],\n",
              "                       [ 0.0128, -0.0136, -0.0033,  ..., -0.0538, -0.0353,  0.0307],\n",
              "                       [ 0.0491,  0.0073,  0.0121,  ..., -0.0137,  0.0338,  0.0225]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0178,  0.0061, -0.0043,  ..., -0.0246, -0.0049, -0.0287],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0354,  0.0100,  0.0010,  ..., -0.0107, -0.0461,  0.0135],\n",
              "                       [ 0.0079, -0.0103,  0.0051,  ...,  0.0272, -0.0457, -0.0110],\n",
              "                       [-0.0047,  0.0229,  0.0102,  ..., -0.0031,  0.0236,  0.0159],\n",
              "                       ...,\n",
              "                       [-0.0461,  0.0113,  0.0299,  ..., -0.0056,  0.0322,  0.0181],\n",
              "                       [ 0.0120,  0.0403, -0.0007,  ..., -0.0096, -0.0440,  0.0133],\n",
              "                       [ 0.0240,  0.0086,  0.0073,  ...,  0.0172,  0.0331,  0.0039]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.6.encoder_attention.v_lin.weight',\n",
              "               tensor([[-2.0477e-02,  1.1360e-02, -8.0032e-03,  ..., -4.9706e-03,\n",
              "                         2.5223e-02,  1.5266e-02],\n",
              "                       [ 1.4923e-02, -2.8183e-02, -3.8891e-03,  ...,  1.6205e-02,\n",
              "                        -1.6418e-02,  6.1393e-06],\n",
              "                       [-9.6130e-03,  3.3539e-02, -1.1658e-02,  ..., -4.5105e-02,\n",
              "                         5.7945e-03,  9.8114e-03],\n",
              "                       ...,\n",
              "                       [-2.3209e-02, -1.1314e-02, -3.4485e-02,  ...,  3.5229e-03,\n",
              "                         2.5208e-02,  3.6865e-02],\n",
              "                       [ 7.5607e-03, -8.8959e-03, -1.3481e-02,  ...,  3.1219e-02,\n",
              "                        -3.2291e-03, -8.6288e-03],\n",
              "                       [ 7.6218e-03,  1.3313e-02,  2.6646e-03,  ...,  8.2016e-03,\n",
              "                         9.9335e-03, -1.6815e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.6.encoder_attention.v_lin.bias',\n",
              "               tensor([ 0.0048,  0.0010, -0.0052,  ..., -0.0185,  0.0079, -0.0040],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0003, -0.0314,  0.0312,  ...,  0.0313, -0.0027, -0.0125],\n",
              "                       [ 0.0194, -0.0011, -0.0045,  ..., -0.0156,  0.0068, -0.0132],\n",
              "                       [-0.0275, -0.0131,  0.0252,  ..., -0.0535, -0.0557, -0.0250],\n",
              "                       ...,\n",
              "                       [-0.0212,  0.0363,  0.0419,  ..., -0.0017, -0.0232,  0.0431],\n",
              "                       [ 0.0316, -0.0139, -0.0029,  ...,  0.0037, -0.0001,  0.0051],\n",
              "                       [ 0.0138, -0.0032,  0.0395,  ..., -0.0032,  0.0409, -0.0343]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.encoder_attention.out_lin.bias',\n",
              "               tensor([-1.7960e-02, -5.2719e-03,  2.2087e-03,  ...,  5.2631e-05,\n",
              "                        3.9902e-03, -8.5373e-03], dtype=torch.float16)),\n",
              "              ('decoder.layers.6.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.6.norm2.bias',\n",
              "               tensor([ 0.0569,  0.0630, -0.0108,  ...,  0.0409,  0.0626, -0.0623],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.ffn.lin1.weight',\n",
              "               tensor([[-0.0070,  0.0267,  0.0167,  ..., -0.0332,  0.0444, -0.0133],\n",
              "                       [-0.0165,  0.0041, -0.0279,  ...,  0.0004,  0.0232, -0.0301],\n",
              "                       [ 0.0542,  0.0305,  0.0324,  ..., -0.0057, -0.0096, -0.0193],\n",
              "                       ...,\n",
              "                       [ 0.0261, -0.0130,  0.0412,  ..., -0.0086,  0.0472, -0.0180],\n",
              "                       [-0.0098,  0.0273, -0.0029,  ..., -0.0300, -0.0154, -0.0024],\n",
              "                       [ 0.0117,  0.0184, -0.0334,  ...,  0.0241,  0.0005, -0.0190]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.ffn.lin1.bias',\n",
              "               tensor([-0.0126, -0.0014, -0.0030,  ..., -0.0109, -0.0077, -0.0079],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.ffn.lin2.weight',\n",
              "               tensor([[ 1.1940e-02,  1.9669e-02, -2.6978e-02,  ..., -2.8442e-02,\n",
              "                         1.2749e-02,  2.9968e-02],\n",
              "                       [-2.0081e-02,  3.7174e-03, -1.8219e-02,  ...,  1.5411e-02,\n",
              "                         8.7128e-03,  1.7042e-03],\n",
              "                       [-2.1698e-02,  5.9843e-05, -3.3997e-02,  ..., -9.4452e-03,\n",
              "                         1.3590e-03, -1.5244e-02],\n",
              "                       ...,\n",
              "                       [ 1.0300e-02, -3.7231e-03, -4.1260e-02,  ...,  1.1238e-02,\n",
              "                         1.8997e-02,  1.8143e-02],\n",
              "                       [-1.6642e-03, -2.4536e-02, -3.0594e-03,  ..., -2.6154e-02,\n",
              "                         1.6332e-04,  2.2339e-02],\n",
              "                       [-6.2485e-03,  2.0020e-02, -1.4679e-02,  ..., -7.3509e-03,\n",
              "                         4.5967e-03,  2.3675e-04]], dtype=torch.float16)),\n",
              "              ('decoder.layers.6.ffn.lin2.bias',\n",
              "               tensor([-0.0125, -0.0073, -0.0209,  ...,  0.0088, -0.0050, -0.0187],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.6.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.6.norm3.bias',\n",
              "               tensor([ 0.0206, -0.0104, -0.0176,  ...,  0.0126,  0.0614, -0.0757],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.self_attention.q_lin.weight',\n",
              "               tensor([[-1.7975e-02, -1.1429e-02,  3.5217e-02,  ...,  5.1231e-03,\n",
              "                        -1.5030e-02, -1.5358e-02],\n",
              "                       [-3.5828e-02,  1.1490e-02,  1.6922e-02,  ..., -6.1569e-03,\n",
              "                         5.1212e-04, -3.3508e-02],\n",
              "                       [ 1.3687e-02,  2.3472e-04,  8.4763e-03,  ...,  2.3544e-02,\n",
              "                        -3.5217e-02, -8.1024e-03],\n",
              "                       ...,\n",
              "                       [-2.3453e-02,  1.4946e-02, -1.2665e-02,  ...,  1.6060e-03,\n",
              "                         2.0161e-03, -1.8051e-02],\n",
              "                       [ 3.0479e-03,  1.5167e-02,  2.1027e-02,  ...,  5.6305e-03,\n",
              "                         5.0476e-02, -8.1406e-03],\n",
              "                       [ 1.9455e-02, -1.4229e-02,  1.8555e-02,  ..., -1.8890e-02,\n",
              "                         3.0696e-05, -6.3553e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.7.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0098, -0.0038, -0.0037,  ..., -0.0482, -0.0010, -0.0081],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0251,  0.0228, -0.0103,  ...,  0.0031,  0.0246,  0.0051],\n",
              "                       [ 0.0597, -0.0038,  0.0172,  ...,  0.0320, -0.0005, -0.0079],\n",
              "                       [ 0.0180,  0.0084, -0.0332,  ...,  0.0274,  0.0060,  0.0276],\n",
              "                       ...,\n",
              "                       [-0.0169, -0.0023, -0.0088,  ...,  0.0235, -0.0297, -0.0189],\n",
              "                       [ 0.0030, -0.0159, -0.0182,  ...,  0.0153,  0.0071, -0.0320],\n",
              "                       [ 0.0416,  0.0211, -0.0047,  ...,  0.0050, -0.0123,  0.0623]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.7.self_attention.v_lin.weight',\n",
              "               tensor([[ 0.0034, -0.0006, -0.0033,  ...,  0.0217,  0.0369, -0.0268],\n",
              "                       [-0.0007, -0.0163,  0.0180,  ..., -0.0388, -0.0302,  0.0353],\n",
              "                       [-0.0307, -0.0409,  0.0151,  ..., -0.0050, -0.0377,  0.0197],\n",
              "                       ...,\n",
              "                       [ 0.0448, -0.0068,  0.0172,  ...,  0.0044,  0.0257, -0.0140],\n",
              "                       [-0.0273,  0.0158,  0.0264,  ..., -0.0119, -0.0189,  0.0228],\n",
              "                       [ 0.0117, -0.0121,  0.0254,  ..., -0.0190, -0.0069,  0.0333]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.self_attention.v_lin.bias',\n",
              "               tensor([-0.0105,  0.0131,  0.0009,  ..., -0.0039, -0.0044,  0.0190],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.self_attention.out_lin.weight',\n",
              "               tensor([[ 0.0008, -0.0273, -0.0246,  ..., -0.0251,  0.0162, -0.0327],\n",
              "                       [ 0.0165, -0.0005, -0.0206,  ..., -0.0079,  0.0234,  0.0209],\n",
              "                       [-0.0042,  0.0026, -0.0145,  ..., -0.0109, -0.0266, -0.0222],\n",
              "                       ...,\n",
              "                       [-0.0256, -0.0124,  0.0235,  ..., -0.0046,  0.0249,  0.0060],\n",
              "                       [ 0.0361, -0.0013, -0.0394,  ..., -0.0387,  0.0125,  0.0213],\n",
              "                       [-0.0194,  0.0477, -0.0002,  ...,  0.0078, -0.0155, -0.0281]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.self_attention.out_lin.bias',\n",
              "               tensor([-0.0038, -0.0101, -0.0028,  ...,  0.0021,  0.0081, -0.0067],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.7.norm1.bias',\n",
              "               tensor([-0.0047, -0.0404,  0.0085,  ..., -0.0071, -0.0303,  0.0316],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.encoder_attention.q_lin.weight',\n",
              "               tensor([[-3.5820e-03,  3.4241e-02,  5.3223e-02,  ..., -2.5959e-03,\n",
              "                         1.9608e-02,  3.5004e-02],\n",
              "                       [ 1.8616e-02,  4.2145e-02,  1.5503e-02,  ..., -9.7275e-03,\n",
              "                        -3.4213e-05,  5.1422e-03],\n",
              "                       [ 2.0401e-02,  3.5461e-02, -2.0844e-02,  ...,  2.9297e-02,\n",
              "                         8.5831e-03, -3.6377e-02],\n",
              "                       ...,\n",
              "                       [ 1.0933e-02, -1.8539e-02, -2.7802e-02,  ..., -2.2491e-02,\n",
              "                        -4.2000e-03, -1.7181e-02],\n",
              "                       [-1.6891e-02,  7.8125e-03, -1.1787e-02,  ..., -5.5771e-03,\n",
              "                        -2.0950e-02, -2.3880e-02],\n",
              "                       [-1.3313e-02, -1.8570e-02, -3.3875e-02,  ..., -6.7940e-03,\n",
              "                         3.2898e-02, -2.8351e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.7.encoder_attention.q_lin.bias',\n",
              "               tensor([-0.0025,  0.0060,  0.0150,  ...,  0.0044, -0.0210,  0.0271],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0135, -0.0692, -0.0349,  ..., -0.0183,  0.0125, -0.0023],\n",
              "                       [-0.0109, -0.0144,  0.0108,  ..., -0.0196, -0.0254, -0.0018],\n",
              "                       [ 0.0245, -0.0271,  0.0183,  ..., -0.0041, -0.0176,  0.0084],\n",
              "                       ...,\n",
              "                       [-0.0382,  0.0025, -0.0172,  ...,  0.0015,  0.0212,  0.0400],\n",
              "                       [ 0.0621, -0.0175, -0.0313,  ..., -0.0057,  0.0055, -0.0227],\n",
              "                       [ 0.0104,  0.0272, -0.0295,  ..., -0.0115, -0.0153,  0.0309]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.7.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0068, -0.0370, -0.0081,  ..., -0.0252, -0.0367, -0.0069],\n",
              "                       [-0.0012, -0.0090,  0.0010,  ..., -0.0084,  0.0135,  0.0179],\n",
              "                       [-0.0171,  0.0275, -0.0057,  ...,  0.0045,  0.0175, -0.0546],\n",
              "                       ...,\n",
              "                       [ 0.0350, -0.0158, -0.0052,  ..., -0.0359, -0.0609, -0.0100],\n",
              "                       [ 0.0168,  0.0176, -0.0121,  ...,  0.0264, -0.0035, -0.0124],\n",
              "                       [-0.0194,  0.0252, -0.0001,  ...,  0.0137,  0.0118,  0.0197]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.encoder_attention.v_lin.bias',\n",
              "               tensor([ 0.0027, -0.0193,  0.0029,  ..., -0.0034,  0.0070,  0.0149],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0080, -0.0241,  0.0329,  ...,  0.0386, -0.0039, -0.0135],\n",
              "                       [ 0.0261,  0.0117, -0.0014,  ...,  0.0185,  0.0587, -0.0287],\n",
              "                       [-0.0297, -0.0002,  0.0222,  ..., -0.0338, -0.0169, -0.0331],\n",
              "                       ...,\n",
              "                       [ 0.0124, -0.0180,  0.0163,  ..., -0.0016, -0.0207,  0.0031],\n",
              "                       [-0.0041, -0.0255,  0.0139,  ...,  0.0084, -0.0179, -0.0334],\n",
              "                       [-0.0027, -0.0145,  0.0109,  ...,  0.0409, -0.0330,  0.0098]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.encoder_attention.out_lin.bias',\n",
              "               tensor([-0.0103, -0.0081,  0.0043,  ...,  0.0031,  0.0011, -0.0172],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.7.norm2.bias',\n",
              "               tensor([ 0.0614,  0.0873, -0.0114,  ...,  0.0505,  0.0462, -0.0258],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.ffn.lin1.weight',\n",
              "               tensor([[ 0.0099,  0.0054, -0.0108,  ..., -0.0052, -0.0070,  0.0323],\n",
              "                       [ 0.0392,  0.0259,  0.0116,  ..., -0.0179, -0.0214,  0.0232],\n",
              "                       [ 0.0050, -0.0099, -0.0402,  ...,  0.0065, -0.0126,  0.0250],\n",
              "                       ...,\n",
              "                       [-0.0082,  0.0155, -0.0263,  ...,  0.0078, -0.0112,  0.0208],\n",
              "                       [ 0.0387,  0.0126, -0.0088,  ...,  0.0209,  0.0356,  0.0215],\n",
              "                       [-0.0016, -0.0184,  0.0086,  ...,  0.0063,  0.0084,  0.0045]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.ffn.lin1.bias',\n",
              "               tensor([ 0.0020,  0.0042, -0.0181,  ..., -0.0061, -0.0023, -0.0148],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.ffn.lin2.weight',\n",
              "               tensor([[ 0.0055, -0.0512,  0.0095,  ...,  0.0010, -0.0035, -0.0624],\n",
              "                       [-0.0157, -0.0283,  0.0081,  ...,  0.0087, -0.0027, -0.0013],\n",
              "                       [ 0.0158, -0.0121,  0.0333,  ...,  0.0053,  0.0185, -0.0064],\n",
              "                       ...,\n",
              "                       [ 0.0209, -0.0204, -0.0390,  ..., -0.0172, -0.0103, -0.0170],\n",
              "                       [ 0.0186,  0.0067, -0.0124,  ...,  0.0325, -0.0129, -0.0047],\n",
              "                       [-0.0192,  0.0015,  0.0137,  ..., -0.0400,  0.0310, -0.0049]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.ffn.lin2.bias',\n",
              "               tensor([-0.0115, -0.0095, -0.0187,  ...,  0.0054,  0.0037, -0.0208],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.7.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.7.norm3.bias',\n",
              "               tensor([ 0.0289, -0.0112, -0.0356,  ...,  0.0149,  0.0410, -0.0592],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.self_attention.q_lin.weight',\n",
              "               tensor([[ 2.1759e-02, -2.1759e-02, -2.4826e-02,  ...,  6.5231e-04,\n",
              "                         3.6407e-02,  3.3752e-02],\n",
              "                       [-2.4109e-02,  2.5986e-02, -2.8290e-02,  ..., -3.5217e-02,\n",
              "                         4.0474e-03, -3.7956e-03],\n",
              "                       [ 2.5925e-02,  2.7420e-02,  4.3182e-02,  ..., -2.4170e-02,\n",
              "                        -2.6093e-03,  1.3756e-02],\n",
              "                       ...,\n",
              "                       [-4.0405e-02, -2.2156e-02,  7.4005e-04,  ...,  4.3449e-03,\n",
              "                         9.7096e-05, -2.8381e-02],\n",
              "                       [-5.4108e-02, -3.2867e-02, -7.8278e-03,  ...,  6.7596e-03,\n",
              "                        -1.2558e-02, -2.1408e-02],\n",
              "                       [-1.9409e-02,  9.6207e-03, -1.4015e-02,  ..., -9.3765e-03,\n",
              "                        -2.4292e-02, -2.3163e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.8.self_attention.q_lin.bias',\n",
              "               tensor([-0.0091,  0.0130, -0.0158,  ..., -0.0004,  0.0129, -0.0137],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.self_attention.k_lin.weight',\n",
              "               tensor([[-0.0237, -0.0133,  0.0622,  ..., -0.0323, -0.0050, -0.0238],\n",
              "                       [-0.0067, -0.0424,  0.0017,  ..., -0.0022,  0.0631,  0.0238],\n",
              "                       [-0.0160,  0.0066, -0.0479,  ..., -0.0211, -0.0110,  0.0210],\n",
              "                       ...,\n",
              "                       [ 0.0145,  0.0160,  0.0049,  ..., -0.0095, -0.0240,  0.0010],\n",
              "                       [ 0.0068, -0.0388, -0.0120,  ...,  0.0295, -0.0017, -0.0177],\n",
              "                       [ 0.0083,  0.0033,  0.0089,  ..., -0.0317,  0.0204,  0.0275]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.8.self_attention.v_lin.weight',\n",
              "               tensor([[ 0.0362,  0.0298, -0.0145,  ...,  0.0234, -0.0141, -0.0023],\n",
              "                       [-0.0243, -0.0602,  0.0078,  ...,  0.0187, -0.0034,  0.0204],\n",
              "                       [ 0.0256, -0.0232,  0.0011,  ...,  0.0261, -0.0341,  0.0118],\n",
              "                       ...,\n",
              "                       [-0.0219, -0.0112, -0.0041,  ..., -0.0056,  0.0334,  0.0190],\n",
              "                       [-0.0035,  0.0161, -0.0084,  ...,  0.0008, -0.0182, -0.0125],\n",
              "                       [ 0.0013,  0.0023, -0.0426,  ...,  0.0209,  0.0165, -0.0120]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0068, -0.0004,  0.0120,  ...,  0.0219, -0.0022,  0.0003],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.self_attention.out_lin.weight',\n",
              "               tensor([[ 0.0132,  0.0363,  0.0063,  ...,  0.0329,  0.0285,  0.0057],\n",
              "                       [-0.0063, -0.0294,  0.0221,  ..., -0.0193, -0.0020, -0.0414],\n",
              "                       [-0.0039,  0.0189, -0.0135,  ..., -0.0109,  0.0261,  0.0105],\n",
              "                       ...,\n",
              "                       [ 0.0032,  0.0352,  0.0315,  ..., -0.0051,  0.0147, -0.0486],\n",
              "                       [-0.0155,  0.0412, -0.0369,  ..., -0.0315,  0.0426,  0.0406],\n",
              "                       [-0.0194, -0.0264, -0.0125,  ..., -0.0248,  0.0028,  0.0406]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.self_attention.out_lin.bias',\n",
              "               tensor([-0.0180, -0.0083,  0.0065,  ...,  0.0023,  0.0131, -0.0256],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.8.norm1.bias',\n",
              "               tensor([-0.0034, -0.0399,  0.0079,  ..., -0.0095, -0.0302,  0.0330],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0054,  0.0011, -0.0358,  ..., -0.0359,  0.0148, -0.0142],\n",
              "                       [-0.0124, -0.0195,  0.0101,  ...,  0.0049,  0.0080, -0.0509],\n",
              "                       [ 0.0109, -0.0309,  0.0151,  ...,  0.0127,  0.0116,  0.0298],\n",
              "                       ...,\n",
              "                       [ 0.0367, -0.0301,  0.0110,  ..., -0.0063,  0.0254, -0.0178],\n",
              "                       [-0.0129,  0.0002, -0.0172,  ..., -0.0169, -0.0232, -0.0515],\n",
              "                       [-0.0417,  0.0350, -0.0482,  ...,  0.0042, -0.0349, -0.0072]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0078, -0.0017,  0.0057,  ..., -0.0084,  0.0038, -0.0134],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0215,  0.0521,  0.0155,  ..., -0.0082,  0.0398, -0.0045],\n",
              "                       [-0.0250, -0.0205,  0.0138,  ...,  0.0012, -0.0176, -0.0516],\n",
              "                       [-0.0003, -0.0377, -0.0098,  ...,  0.0013,  0.0054, -0.0017],\n",
              "                       ...,\n",
              "                       [-0.0082,  0.0529, -0.0078,  ..., -0.0322, -0.0160,  0.0210],\n",
              "                       [ 0.0262,  0.0319, -0.0484,  ..., -0.0371, -0.0313,  0.0125],\n",
              "                       [ 0.0345,  0.0100, -0.0373,  ...,  0.0225, -0.0199, -0.0223]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.8.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0385, -0.0019,  0.0030,  ...,  0.0161,  0.0341,  0.0208],\n",
              "                       [ 0.0171,  0.0493, -0.0446,  ...,  0.0180, -0.0022,  0.0131],\n",
              "                       [-0.0139,  0.0078,  0.0008,  ...,  0.0607,  0.0014,  0.0024],\n",
              "                       ...,\n",
              "                       [ 0.0198,  0.0089, -0.0335,  ..., -0.0315, -0.0058, -0.0313],\n",
              "                       [-0.0224, -0.0207, -0.0017,  ..., -0.0069,  0.0513, -0.0034],\n",
              "                       [ 0.0488, -0.0187,  0.0122,  ..., -0.0251,  0.0580, -0.0005]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.encoder_attention.v_lin.bias',\n",
              "               tensor([-0.0057,  0.0071, -0.0017,  ...,  0.0073,  0.0073,  0.0061],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.encoder_attention.out_lin.weight',\n",
              "               tensor([[-0.0067, -0.0217, -0.0029,  ...,  0.0304, -0.0009, -0.0017],\n",
              "                       [-0.0291, -0.0007,  0.0047,  ...,  0.0305, -0.0145,  0.0072],\n",
              "                       [ 0.0014,  0.0058,  0.0461,  ..., -0.0340,  0.0065,  0.0010],\n",
              "                       ...,\n",
              "                       [-0.0254,  0.0065,  0.0547,  ...,  0.0184,  0.0488,  0.0450],\n",
              "                       [-0.0509, -0.0020, -0.0024,  ...,  0.0325, -0.0011, -0.0148],\n",
              "                       [-0.0206, -0.0061, -0.0056,  ..., -0.0038,  0.0121, -0.0103]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.encoder_attention.out_lin.bias',\n",
              "               tensor([-0.0178, -0.0171,  0.0014,  ...,  0.0090,  0.0024, -0.0249],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.8.norm2.bias',\n",
              "               tensor([ 0.0406,  0.0706,  0.0304,  ...,  0.0474,  0.0622, -0.0623],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.ffn.lin1.weight',\n",
              "               tensor([[-0.0087,  0.0289,  0.0498,  ...,  0.0231,  0.0023,  0.0115],\n",
              "                       [ 0.0085, -0.0124, -0.0059,  ..., -0.0150,  0.0430, -0.0088],\n",
              "                       [-0.0269, -0.0012,  0.0327,  ..., -0.0139,  0.0100, -0.0363],\n",
              "                       ...,\n",
              "                       [-0.0056, -0.0012,  0.0043,  ..., -0.0221,  0.0197,  0.0504],\n",
              "                       [-0.0145,  0.0371, -0.0085,  ..., -0.0610,  0.0252,  0.0246],\n",
              "                       [-0.0281,  0.0134,  0.0324,  ..., -0.0183,  0.0207,  0.0166]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.ffn.lin1.bias',\n",
              "               tensor([-0.0111, -0.0073, -0.0090,  ..., -0.0150, -0.0106, -0.0151],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.ffn.lin2.weight',\n",
              "               tensor([[ 1.2215e-02,  7.5989e-03, -2.7420e-02,  ..., -1.3481e-02,\n",
              "                         1.6098e-02,  6.6578e-05],\n",
              "                       [ 2.6474e-03,  1.9348e-02,  2.3899e-03,  ...,  1.2856e-02,\n",
              "                        -2.3300e-02,  1.8489e-04],\n",
              "                       [-1.6985e-03,  3.3020e-02, -8.9874e-03,  ...,  3.3722e-02,\n",
              "                         1.3466e-02, -1.6129e-02],\n",
              "                       ...,\n",
              "                       [-2.0035e-02, -2.1606e-02, -2.3956e-02,  ..., -2.8534e-02,\n",
              "                         4.4525e-02,  2.9068e-02],\n",
              "                       [ 2.2705e-02, -3.1311e-02,  5.5618e-03,  ..., -1.5305e-02,\n",
              "                        -1.5335e-02, -4.6158e-03],\n",
              "                       [ 3.2013e-02,  3.2776e-02,  2.0859e-02,  ..., -2.8076e-03,\n",
              "                        -3.6774e-02,  6.7253e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.8.ffn.lin2.bias',\n",
              "               tensor([-0.0175,  0.0042, -0.0318,  ...,  0.0118, -0.0036, -0.0221],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.8.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.8.norm3.bias',\n",
              "               tensor([ 0.0138, -0.0063, -0.0274,  ...,  0.0407,  0.0540, -0.0721],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.self_attention.q_lin.weight',\n",
              "               tensor([[-9.3231e-03, -6.3232e-02,  9.8038e-03,  ..., -2.6443e-02,\n",
              "                         2.4277e-02, -2.3987e-02],\n",
              "                       [ 2.5085e-02,  5.6496e-03, -1.9760e-02,  ...,  4.2175e-02,\n",
              "                        -1.0201e-02,  9.1324e-03],\n",
              "                       [ 5.6791e-04,  3.2562e-02,  2.0447e-02,  ..., -2.9175e-02,\n",
              "                         2.5711e-02,  7.7903e-05],\n",
              "                       ...,\n",
              "                       [ 4.9103e-02, -2.7130e-02,  2.1057e-02,  ..., -2.7714e-03,\n",
              "                        -4.1733e-03, -4.9858e-03],\n",
              "                       [ 2.7161e-02,  7.5417e-03,  2.4399e-02,  ...,  3.1616e-02,\n",
              "                        -3.3051e-02,  1.5099e-02],\n",
              "                       [ 2.6749e-02, -2.5681e-02, -3.9948e-02,  ...,  1.0506e-02,\n",
              "                         3.7498e-03,  1.0139e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.9.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0017,  0.0109,  0.0142,  ...,  0.0059, -0.0107, -0.0061],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.self_attention.k_lin.weight',\n",
              "               tensor([[-0.0028, -0.0120, -0.0040,  ...,  0.0221, -0.0242, -0.0032],\n",
              "                       [-0.0098,  0.0306, -0.0152,  ...,  0.0196, -0.0143,  0.0079],\n",
              "                       [ 0.0220,  0.0002, -0.0181,  ..., -0.0139,  0.0455, -0.0162],\n",
              "                       ...,\n",
              "                       [-0.0529,  0.0179,  0.0387,  ...,  0.0129, -0.0213, -0.0366],\n",
              "                       [ 0.0263, -0.0362,  0.0396,  ..., -0.0359, -0.0298,  0.0144],\n",
              "                       [-0.0555,  0.0215, -0.0118,  ..., -0.0159, -0.0092, -0.0254]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.9.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0086,  0.0081, -0.0218,  ..., -0.0107, -0.0376,  0.0125],\n",
              "                       [ 0.0231, -0.0078, -0.0040,  ..., -0.0138, -0.0278,  0.0156],\n",
              "                       [-0.0083, -0.0129, -0.0179,  ..., -0.0123, -0.0010,  0.0294],\n",
              "                       ...,\n",
              "                       [ 0.0592, -0.0110, -0.0482,  ..., -0.0201, -0.0115, -0.0262],\n",
              "                       [-0.0347,  0.0220, -0.0054,  ...,  0.0166,  0.0281,  0.0081],\n",
              "                       [-0.0366, -0.0003, -0.0289,  ...,  0.0329, -0.0112,  0.0171]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.self_attention.v_lin.bias',\n",
              "               tensor([-0.0180, -0.0122, -0.0049,  ...,  0.0112, -0.0027,  0.0062],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.self_attention.out_lin.weight',\n",
              "               tensor([[ 0.0017, -0.0003, -0.0053,  ...,  0.0070, -0.0280,  0.0335],\n",
              "                       [ 0.0072, -0.0489, -0.0377,  ...,  0.0059, -0.0406,  0.0149],\n",
              "                       [-0.0260,  0.0340,  0.0247,  ..., -0.0218, -0.0154, -0.0193],\n",
              "                       ...,\n",
              "                       [ 0.0181, -0.0235, -0.0204,  ..., -0.0531,  0.0174, -0.0241],\n",
              "                       [-0.0192, -0.0105, -0.0541,  ..., -0.0113, -0.0322,  0.0103],\n",
              "                       [-0.0062,  0.0075, -0.0015,  ..., -0.0045, -0.0144, -0.0028]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.self_attention.out_lin.bias',\n",
              "               tensor([-0.0020, -0.0074,  0.0028,  ...,  0.0015,  0.0185, -0.0296],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.9.norm1.bias',\n",
              "               tensor([-0.0121, -0.0300, -0.0074,  ..., -0.0161, -0.0210,  0.0419],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0227, -0.0490, -0.0051,  ..., -0.0048,  0.0235, -0.0059],\n",
              "                       [-0.0073,  0.0228, -0.0430,  ..., -0.0183, -0.0020, -0.0006],\n",
              "                       [-0.0012,  0.0265,  0.0117,  ...,  0.0341,  0.0079, -0.0087],\n",
              "                       ...,\n",
              "                       [-0.0052, -0.0050, -0.0084,  ...,  0.0244,  0.0187, -0.0204],\n",
              "                       [-0.0022, -0.0620,  0.0320,  ..., -0.0112, -0.0285,  0.0092],\n",
              "                       [ 0.0339, -0.0152,  0.0444,  ..., -0.0171, -0.0050, -0.0234]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.encoder_attention.q_lin.bias',\n",
              "               tensor([-0.0434,  0.0272, -0.0021,  ..., -0.0258, -0.0171,  0.0072],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0310, -0.0234,  0.0146,  ..., -0.0211, -0.0246,  0.0188],\n",
              "                       [-0.0051,  0.0185, -0.0342,  ..., -0.0105,  0.0134, -0.0035],\n",
              "                       [-0.0127,  0.0430,  0.0260,  ..., -0.0423,  0.0379, -0.0339],\n",
              "                       ...,\n",
              "                       [-0.0429, -0.0297,  0.0364,  ..., -0.0179,  0.0107, -0.0260],\n",
              "                       [ 0.0004, -0.0117,  0.0388,  ..., -0.0291, -0.0109,  0.0029],\n",
              "                       [-0.0347,  0.0021, -0.0307,  ..., -0.0399, -0.0005, -0.0120]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.9.encoder_attention.v_lin.weight',\n",
              "               tensor([[ 2.5597e-03,  4.8599e-03, -6.1951e-03,  ...,  1.3680e-02,\n",
              "                         6.6185e-03, -1.1543e-02],\n",
              "                       [ 1.8341e-02,  3.9001e-02, -5.7312e-02,  ..., -1.8845e-02,\n",
              "                         5.5611e-05,  5.5939e-02],\n",
              "                       [ 2.1408e-02,  1.8570e-02, -3.2806e-02,  ..., -3.5919e-02,\n",
              "                        -2.1912e-02, -4.8409e-03],\n",
              "                       ...,\n",
              "                       [ 5.3177e-03,  3.2349e-02,  2.2278e-02,  ...,  3.5950e-02,\n",
              "                        -3.1586e-02, -7.0801e-03],\n",
              "                       [ 3.8681e-03,  2.5589e-02,  1.1911e-03,  ...,  5.9700e-03,\n",
              "                         1.6571e-02, -1.8940e-03],\n",
              "                       [ 1.2703e-03,  6.1615e-02,  4.0474e-03,  ...,  6.0150e-02,\n",
              "                        -5.4321e-03,  4.8370e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.9.encoder_attention.v_lin.bias',\n",
              "               tensor([-1.5488e-02, -1.8537e-05,  1.0864e-02,  ..., -2.3560e-02,\n",
              "                       -6.2485e-03, -6.1340e-03], dtype=torch.float16)),\n",
              "              ('decoder.layers.9.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0246,  0.0165,  0.0150,  ..., -0.0158,  0.0194,  0.0093],\n",
              "                       [ 0.0136, -0.0194, -0.0263,  ...,  0.0446,  0.0251, -0.0200],\n",
              "                       [-0.0065,  0.0211,  0.0360,  ...,  0.0300,  0.0168,  0.0029],\n",
              "                       ...,\n",
              "                       [-0.0228,  0.0423,  0.0089,  ...,  0.0389,  0.0354,  0.0125],\n",
              "                       [ 0.0228,  0.0222, -0.0140,  ..., -0.0449, -0.0160, -0.0109],\n",
              "                       [-0.0026, -0.0061,  0.0006,  ...,  0.0371, -0.0108,  0.0158]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.encoder_attention.out_lin.bias',\n",
              "               tensor([-0.0105, -0.0186,  0.0047,  ...,  0.0093,  0.0091, -0.0245],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.9.norm2.bias',\n",
              "               tensor([ 0.0252,  0.0640,  0.0097,  ...,  0.0341,  0.0454, -0.0623],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.ffn.lin1.weight',\n",
              "               tensor([[ 0.0401, -0.0033, -0.0062,  ..., -0.0014, -0.0523, -0.0303],\n",
              "                       [-0.0143,  0.0221,  0.0044,  ..., -0.0039, -0.0156,  0.0232],\n",
              "                       [-0.0083,  0.0124, -0.0362,  ...,  0.0151,  0.0238, -0.0146],\n",
              "                       ...,\n",
              "                       [-0.0067,  0.0155, -0.0436,  ..., -0.0193,  0.0149,  0.0068],\n",
              "                       [-0.0149,  0.0063,  0.0232,  ...,  0.0313,  0.0175,  0.0377],\n",
              "                       [ 0.0397,  0.0035,  0.0128,  ..., -0.0407, -0.0466,  0.0134]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.ffn.lin1.bias',\n",
              "               tensor([-0.0095, -0.0104, -0.0146,  ..., -0.0107, -0.0130, -0.0106],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.ffn.lin2.weight',\n",
              "               tensor([[-0.0222,  0.0325, -0.0074,  ...,  0.0396,  0.0240, -0.0531],\n",
              "                       [ 0.0119,  0.0002,  0.0043,  ...,  0.0118, -0.0257,  0.0106],\n",
              "                       [-0.0163,  0.0327,  0.0258,  ..., -0.0112, -0.0643,  0.0329],\n",
              "                       ...,\n",
              "                       [-0.0090, -0.0223, -0.0272,  ..., -0.0094, -0.0176,  0.0300],\n",
              "                       [ 0.0087, -0.0155, -0.0339,  ...,  0.0026, -0.0333,  0.0085],\n",
              "                       [ 0.0313, -0.0121,  0.0305,  ...,  0.0100, -0.0536,  0.0292]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.ffn.lin2.bias',\n",
              "               tensor([-0.0065, -0.0105, -0.0286,  ...,  0.0094, -0.0021, -0.0154],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.9.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.9.norm3.bias',\n",
              "               tensor([ 0.0107,  0.0059, -0.0168,  ...,  0.0331,  0.0575, -0.0672],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.self_attention.q_lin.weight',\n",
              "               tensor([[-0.0014, -0.0178,  0.0198,  ...,  0.0096,  0.0095, -0.0017],\n",
              "                       [-0.0044,  0.0046, -0.0013,  ..., -0.0087,  0.0186, -0.0153],\n",
              "                       [ 0.0090, -0.0014, -0.0142,  ...,  0.0476, -0.0134, -0.0124],\n",
              "                       ...,\n",
              "                       [-0.0018,  0.0191,  0.0172,  ...,  0.0089, -0.0133,  0.0100],\n",
              "                       [ 0.0154,  0.0230, -0.0350,  ...,  0.0321,  0.0073, -0.0280],\n",
              "                       [ 0.0172, -0.0153, -0.0134,  ...,  0.0120,  0.0583,  0.0235]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.self_attention.q_lin.bias',\n",
              "               tensor([-0.0076,  0.0026,  0.0048,  ..., -0.0056, -0.0063,  0.0227],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.self_attention.k_lin.weight',\n",
              "               tensor([[-0.0117, -0.0293, -0.0336,  ..., -0.0257,  0.0015,  0.0032],\n",
              "                       [ 0.0182, -0.0314, -0.0374,  ..., -0.0317,  0.0338, -0.0028],\n",
              "                       [ 0.0060, -0.0093, -0.0156,  ...,  0.0110, -0.0379,  0.0194],\n",
              "                       ...,\n",
              "                       [-0.0166,  0.0050,  0.0285,  ...,  0.0152,  0.0101, -0.0263],\n",
              "                       [ 0.0064, -0.0386,  0.0252,  ...,  0.0117, -0.0177,  0.0326],\n",
              "                       [-0.0156,  0.0218,  0.0081,  ..., -0.0299,  0.0182,  0.0301]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.10.self_attention.v_lin.weight',\n",
              "               tensor([[-1.5732e-02, -2.0924e-03, -4.6051e-02,  ..., -4.1351e-02,\n",
              "                         6.2500e-02,  1.1574e-02],\n",
              "                       [ 3.6530e-02,  4.1485e-05, -1.2062e-02,  ..., -2.2488e-03,\n",
              "                        -7.6828e-03,  1.1963e-04],\n",
              "                       [-3.7231e-02,  1.2154e-02, -1.1383e-02,  ..., -7.8812e-03,\n",
              "                         3.1097e-02, -1.1406e-02],\n",
              "                       ...,\n",
              "                       [-3.3203e-02, -2.2751e-02,  2.0935e-02,  ..., -2.6184e-02,\n",
              "                         3.1281e-02,  9.9335e-03],\n",
              "                       [ 6.2683e-02,  3.1067e-02, -3.2759e-04,  ..., -4.3678e-03,\n",
              "                         1.2360e-03, -4.9805e-02],\n",
              "                       [-2.6749e-02, -1.6174e-02, -2.5558e-02,  ..., -3.1158e-02,\n",
              "                         1.2932e-02, -3.3600e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.10.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0030,  0.0086,  0.0096,  ..., -0.0049, -0.0046, -0.0211],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.self_attention.out_lin.weight',\n",
              "               tensor([[ 0.0057, -0.0499,  0.0165,  ..., -0.0160, -0.0329, -0.0037],\n",
              "                       [ 0.0104, -0.0288,  0.0558,  ...,  0.0195,  0.0058,  0.0301],\n",
              "                       [-0.0676, -0.0323, -0.0434,  ...,  0.0051, -0.0262, -0.0102],\n",
              "                       ...,\n",
              "                       [-0.0203,  0.0103, -0.0042,  ...,  0.0145, -0.0171, -0.0129],\n",
              "                       [ 0.0080,  0.0143,  0.0314,  ..., -0.0033,  0.0081,  0.0291],\n",
              "                       [ 0.0162,  0.0195,  0.0054,  ...,  0.0290, -0.0318,  0.0022]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.self_attention.out_lin.bias',\n",
              "               tensor([-0.0087, -0.0199,  0.0073,  ...,  0.0039,  0.0231, -0.0256],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.10.norm1.bias',\n",
              "               tensor([-0.0031, -0.0314, -0.0049,  ..., -0.0246, -0.0148,  0.0300],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.encoder_attention.q_lin.weight',\n",
              "               tensor([[ 0.0137,  0.0316,  0.0217,  ...,  0.0067,  0.0131, -0.0249],\n",
              "                       [ 0.0051,  0.0168, -0.0046,  ...,  0.0265,  0.0126,  0.0159],\n",
              "                       [ 0.0219, -0.0174, -0.0150,  ...,  0.0139,  0.0154,  0.0119],\n",
              "                       ...,\n",
              "                       [ 0.0098,  0.0213,  0.0037,  ...,  0.0080,  0.0011, -0.0083],\n",
              "                       [-0.0319,  0.0335,  0.0075,  ...,  0.0212, -0.0144,  0.0193],\n",
              "                       [-0.0056, -0.0049, -0.0395,  ...,  0.0182,  0.0376,  0.0178]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.encoder_attention.q_lin.bias',\n",
              "               tensor([-0.0246, -0.0045, -0.0007,  ...,  0.0287,  0.0008,  0.0296],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0071, -0.0504, -0.0102,  ..., -0.0045,  0.0043, -0.0202],\n",
              "                       [ 0.0081, -0.0012, -0.0111,  ..., -0.0054,  0.0142,  0.0432],\n",
              "                       [ 0.0263,  0.0259,  0.0059,  ..., -0.0508, -0.0084, -0.0062],\n",
              "                       ...,\n",
              "                       [-0.0134,  0.0058, -0.0068,  ...,  0.0543,  0.0080,  0.0243],\n",
              "                       [-0.0093, -0.0229,  0.0154,  ...,  0.0074,  0.0466, -0.0072],\n",
              "                       [-0.0118, -0.0083, -0.0224,  ..., -0.0036,  0.0270,  0.0065]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.10.encoder_attention.v_lin.weight',\n",
              "               tensor([[ 0.0342,  0.0133, -0.0190,  ...,  0.0041, -0.0246, -0.0222],\n",
              "                       [-0.0076, -0.0324, -0.0188,  ...,  0.0278, -0.0244, -0.0085],\n",
              "                       [-0.0077,  0.0159, -0.0280,  ...,  0.0129, -0.0155,  0.0037],\n",
              "                       ...,\n",
              "                       [-0.0124, -0.0050,  0.0057,  ..., -0.0118,  0.0108,  0.0121],\n",
              "                       [-0.0087,  0.0020, -0.0287,  ..., -0.0325, -0.0042,  0.0069],\n",
              "                       [-0.0053,  0.0089,  0.0055,  ...,  0.0510, -0.0272, -0.0458]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.encoder_attention.v_lin.bias',\n",
              "               tensor([ 0.0273,  0.0319,  0.0174,  ..., -0.0021,  0.0115, -0.0083],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 3.5458e-03, -3.0716e-02,  4.5738e-03,  ...,  3.2013e-02,\n",
              "                        -2.0981e-02, -1.6190e-02],\n",
              "                       [-7.8125e-03,  3.9581e-02,  9.9659e-04,  ..., -5.9700e-03,\n",
              "                        -1.2383e-02,  2.1408e-02],\n",
              "                       [ 7.5102e-05, -2.3575e-02, -4.6272e-03,  ...,  2.7752e-04,\n",
              "                         5.1994e-03,  4.0054e-05],\n",
              "                       ...,\n",
              "                       [-1.3588e-02,  3.9062e-03,  3.2288e-02,  ...,  2.3499e-02,\n",
              "                         9.6741e-03, -1.6034e-04],\n",
              "                       [ 2.7100e-02, -7.3776e-03,  1.3893e-02,  ...,  2.3804e-02,\n",
              "                         7.6904e-03,  3.7270e-03],\n",
              "                       [ 7.9880e-03,  4.3030e-02, -2.9800e-02,  ..., -9.9487e-03,\n",
              "                        -4.3243e-02, -2.3575e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.10.encoder_attention.out_lin.bias',\n",
              "               tensor([-0.0041, -0.0185, -0.0046,  ..., -0.0078,  0.0142, -0.0232],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.10.norm2.bias',\n",
              "               tensor([ 0.0121,  0.0623,  0.0550,  ...,  0.0359,  0.0632, -0.0503],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.ffn.lin1.weight',\n",
              "               tensor([[ 0.0269,  0.0362,  0.0070,  ...,  0.0098, -0.0075,  0.0079],\n",
              "                       [ 0.0015,  0.0191,  0.0243,  ..., -0.0027, -0.0079, -0.0007],\n",
              "                       [-0.0045,  0.0163,  0.0094,  ...,  0.0101,  0.0456, -0.0112],\n",
              "                       ...,\n",
              "                       [ 0.0038,  0.0328,  0.0052,  ..., -0.0105,  0.0264,  0.0088],\n",
              "                       [ 0.0076,  0.0511, -0.0075,  ...,  0.0096, -0.0290, -0.0121],\n",
              "                       [ 0.0185, -0.0349,  0.0027,  ..., -0.0004, -0.0189, -0.0524]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.ffn.lin1.bias',\n",
              "               tensor([-0.0118, -0.0149, -0.0069,  ..., -0.0125, -0.0125, -0.0033],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.ffn.lin2.weight',\n",
              "               tensor([[-0.0097,  0.0019,  0.0180,  ..., -0.0103, -0.0320,  0.0294],\n",
              "                       [-0.0222,  0.0022, -0.0365,  ...,  0.0122, -0.0464, -0.0298],\n",
              "                       [ 0.0170, -0.0162, -0.0075,  ...,  0.0042, -0.0177, -0.0077],\n",
              "                       ...,\n",
              "                       [-0.0629,  0.0176, -0.0509,  ..., -0.0001,  0.0018, -0.0081],\n",
              "                       [-0.0116, -0.0024, -0.0403,  ...,  0.0337,  0.0384,  0.0373],\n",
              "                       [-0.0245, -0.0017,  0.0408,  ...,  0.0061, -0.0313, -0.0366]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.ffn.lin2.bias',\n",
              "               tensor([-0.0111, -0.0119, -0.0076,  ...,  0.0116, -0.0164, -0.0103],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.10.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.10.norm3.bias',\n",
              "               tensor([ 0.0082, -0.0036, -0.0117,  ...,  0.0435,  0.0475, -0.0675],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.self_attention.q_lin.weight',\n",
              "               tensor([[ 0.0288, -0.0240, -0.0126,  ...,  0.0285, -0.0257,  0.0317],\n",
              "                       [ 0.0383, -0.0278, -0.0514,  ...,  0.0143, -0.0139,  0.0196],\n",
              "                       [ 0.0245,  0.0375, -0.0319,  ..., -0.0316, -0.0143, -0.0020],\n",
              "                       ...,\n",
              "                       [ 0.0272,  0.0080, -0.0217,  ...,  0.0042, -0.0009,  0.0148],\n",
              "                       [-0.0370,  0.0239,  0.0118,  ..., -0.0254, -0.0008, -0.0324],\n",
              "                       [-0.0061,  0.0131, -0.0492,  ...,  0.0026, -0.0012, -0.0307]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0062,  0.0181, -0.0041,  ...,  0.0137, -0.0164, -0.0096],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0045,  0.0008, -0.0019,  ...,  0.0173, -0.0511,  0.0255],\n",
              "                       [-0.0014, -0.0067, -0.0181,  ...,  0.0212,  0.0019,  0.0345],\n",
              "                       [ 0.0026, -0.0352,  0.0352,  ...,  0.0346,  0.0073,  0.0435],\n",
              "                       ...,\n",
              "                       [-0.0485, -0.0238,  0.0170,  ..., -0.0103,  0.0144, -0.0421],\n",
              "                       [ 0.0544,  0.0048, -0.0147,  ..., -0.0171, -0.0519, -0.0203],\n",
              "                       [-0.0101,  0.0084,  0.0188,  ...,  0.0235, -0.0054, -0.0029]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.11.self_attention.v_lin.weight',\n",
              "               tensor([[ 0.0061,  0.0045, -0.0298,  ..., -0.0109, -0.0100, -0.0064],\n",
              "                       [-0.0111,  0.0153,  0.0015,  ..., -0.0087,  0.0188,  0.0287],\n",
              "                       [ 0.0210,  0.0118,  0.0117,  ..., -0.0098, -0.0100,  0.0432],\n",
              "                       ...,\n",
              "                       [-0.0124,  0.0317, -0.0057,  ...,  0.0087,  0.0111, -0.0128],\n",
              "                       [-0.0091,  0.0130, -0.0024,  ..., -0.0019, -0.0233, -0.0153],\n",
              "                       [ 0.0170, -0.0040, -0.0015,  ...,  0.0194,  0.0098, -0.0058]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0038, -0.0152, -0.0152,  ...,  0.0144,  0.0089,  0.0113],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.self_attention.out_lin.weight',\n",
              "               tensor([[ 3.0777e-02, -5.8212e-03, -4.1122e-03,  ...,  8.0872e-03,\n",
              "                         7.9880e-03, -2.6627e-02],\n",
              "                       [ 1.3046e-02,  9.2926e-03, -3.5248e-02,  ..., -2.0370e-02,\n",
              "                        -1.4782e-05, -3.6774e-02],\n",
              "                       [ 4.1260e-02, -1.1185e-02, -1.7441e-02,  ...,  3.3295e-02,\n",
              "                        -2.0523e-02, -2.8625e-02],\n",
              "                       ...,\n",
              "                       [ 1.1246e-02, -4.4670e-03, -2.9770e-02,  ...,  2.0554e-02,\n",
              "                        -4.0817e-03, -1.8738e-02],\n",
              "                       [ 1.1894e-02, -2.4368e-02, -4.5807e-02,  ...,  4.0817e-03,\n",
              "                        -7.9346e-03,  1.9669e-02],\n",
              "                       [-8.3313e-03, -4.9324e-03, -3.6621e-02,  ..., -2.9617e-02,\n",
              "                         1.4999e-02, -8.8806e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.11.self_attention.out_lin.bias',\n",
              "               tensor([-0.0066, -0.0124,  0.0012,  ...,  0.0051,  0.0270, -0.0187],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.11.norm1.bias',\n",
              "               tensor([-0.0013, -0.0266, -0.0033,  ..., -0.0172, -0.0211,  0.0348],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.encoder_attention.q_lin.weight',\n",
              "               tensor([[ 0.0219, -0.0319,  0.0264,  ..., -0.0188,  0.0394,  0.0022],\n",
              "                       [-0.0192, -0.0213, -0.0117,  ..., -0.0153, -0.0115,  0.0367],\n",
              "                       [-0.0273,  0.0081, -0.0498,  ..., -0.0128,  0.0351,  0.0012],\n",
              "                       ...,\n",
              "                       [ 0.0149,  0.0081,  0.0037,  ..., -0.0181, -0.0501, -0.0092],\n",
              "                       [ 0.0087, -0.0403,  0.0104,  ...,  0.0171, -0.0086, -0.0125],\n",
              "                       [ 0.0409,  0.0398, -0.0222,  ...,  0.0051, -0.0200,  0.0224]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0181, -0.0245,  0.0145,  ...,  0.0137, -0.0228,  0.0132],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0228, -0.0273,  0.0314,  ..., -0.0046, -0.0365,  0.0111],\n",
              "                       [ 0.0222, -0.0350,  0.0543,  ...,  0.0120,  0.0242, -0.0187],\n",
              "                       [-0.0184,  0.0008,  0.0414,  ..., -0.0021,  0.0110, -0.0373],\n",
              "                       ...,\n",
              "                       [-0.0050, -0.0040, -0.0191,  ..., -0.0264,  0.0154,  0.0110],\n",
              "                       [-0.0193, -0.0158, -0.0184,  ..., -0.0363, -0.0179,  0.0033],\n",
              "                       [-0.0417,  0.0314, -0.0017,  ...,  0.0315, -0.0326, -0.0125]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.11.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0214, -0.0440,  0.0383,  ...,  0.0248,  0.0435,  0.0112],\n",
              "                       [-0.0137,  0.0486, -0.0299,  ...,  0.0173, -0.0338, -0.0072],\n",
              "                       [-0.0115,  0.0436,  0.0106,  ..., -0.0137, -0.0349,  0.0183],\n",
              "                       ...,\n",
              "                       [ 0.0097, -0.0237,  0.0299,  ...,  0.0267, -0.0430,  0.0438],\n",
              "                       [ 0.0188, -0.0294,  0.0342,  ...,  0.0091,  0.0150,  0.0249],\n",
              "                       [ 0.0612,  0.0328, -0.0533,  ...,  0.0349, -0.0062, -0.0350]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.encoder_attention.v_lin.bias',\n",
              "               tensor([-0.0121,  0.0054,  0.0142,  ...,  0.0268, -0.0190,  0.0052],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0026,  0.0161,  0.0135,  ..., -0.0049,  0.0279,  0.0201],\n",
              "                       [ 0.0108,  0.0322, -0.0403,  ...,  0.0086, -0.0194, -0.0150],\n",
              "                       [-0.0290, -0.0135,  0.0018,  ..., -0.0105, -0.0198,  0.0242],\n",
              "                       ...,\n",
              "                       [-0.0389, -0.0267, -0.0092,  ..., -0.0123,  0.0117, -0.0052],\n",
              "                       [-0.0388,  0.0129, -0.0038,  ...,  0.0067, -0.0077, -0.0305],\n",
              "                       [ 0.0500,  0.0076, -0.0022,  ..., -0.0366,  0.0195,  0.0018]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.encoder_attention.out_lin.bias',\n",
              "               tensor([-0.0039, -0.0152, -0.0012,  ..., -0.0057,  0.0168, -0.0265],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.11.norm2.bias',\n",
              "               tensor([ 0.0166,  0.0626,  0.0301,  ...,  0.0337,  0.0624, -0.0428],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.ffn.lin1.weight',\n",
              "               tensor([[ 0.0028,  0.0181,  0.0458,  ...,  0.0068,  0.0489,  0.0060],\n",
              "                       [-0.0030,  0.0049, -0.0073,  ...,  0.0055,  0.0200, -0.0193],\n",
              "                       [ 0.0211, -0.0023, -0.0017,  ...,  0.0388, -0.0176,  0.0098],\n",
              "                       ...,\n",
              "                       [-0.0002, -0.0004,  0.0341,  ..., -0.0307,  0.0045,  0.0335],\n",
              "                       [-0.0324, -0.0152,  0.0193,  ...,  0.0182,  0.0014, -0.0141],\n",
              "                       [-0.0090,  0.0138, -0.0048,  ...,  0.0171,  0.0104,  0.0089]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.ffn.lin1.bias',\n",
              "               tensor([-0.0047, -0.0068, -0.0130,  ..., -0.0217, -0.0096, -0.0089],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.ffn.lin2.weight',\n",
              "               tensor([[-8.0872e-03, -1.0452e-02,  3.3936e-02,  ..., -1.0773e-02,\n",
              "                        -3.1586e-02, -4.3518e-02],\n",
              "                       [-5.3406e-03, -5.0545e-03,  3.6591e-02,  ...,  3.3600e-02,\n",
              "                         3.3661e-02, -3.1250e-02],\n",
              "                       [ 6.2752e-03, -2.4033e-02,  2.9053e-02,  ..., -1.9577e-02,\n",
              "                        -3.1921e-02, -1.3176e-02],\n",
              "                       ...,\n",
              "                       [-3.9825e-03,  2.8336e-02, -4.6997e-02,  ...,  3.3112e-02,\n",
              "                        -5.0568e-02, -4.3579e-02],\n",
              "                       [-5.6732e-02, -3.1738e-02, -5.1537e-03,  ...,  9.5010e-05,\n",
              "                        -9.8648e-03,  5.1041e-03],\n",
              "                       [-1.1681e-02,  1.2428e-02,  2.2141e-02,  ..., -5.5542e-03,\n",
              "                        -7.4844e-03, -3.2410e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.11.ffn.lin2.bias',\n",
              "               tensor([-0.0210, -0.0118, -0.0182,  ..., -0.0013, -0.0244, -0.0124],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.11.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.11.norm3.bias',\n",
              "               tensor([-0.0093, -0.0039, -0.0003,  ...,  0.0410,  0.0233, -0.0645],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.self_attention.q_lin.weight',\n",
              "               tensor([[ 0.0136,  0.0122,  0.0624,  ..., -0.0045,  0.0009,  0.0331],\n",
              "                       [ 0.0038,  0.0387, -0.0180,  ...,  0.0080, -0.0366,  0.0161],\n",
              "                       [-0.0269,  0.0330,  0.0230,  ...,  0.0052,  0.0638, -0.0159],\n",
              "                       ...,\n",
              "                       [ 0.0002, -0.0141,  0.0046,  ...,  0.0074,  0.0120, -0.0223],\n",
              "                       [-0.0234,  0.0316,  0.0039,  ..., -0.0019,  0.0357, -0.0175],\n",
              "                       [-0.0076,  0.0325, -0.0119,  ..., -0.0467, -0.0280, -0.0039]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0236, -0.0181,  0.0107,  ...,  0.0124, -0.0099,  0.0081],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.self_attention.k_lin.weight',\n",
              "               tensor([[ 2.3270e-02, -1.9165e-02, -2.6810e-02,  ...,  2.2308e-02,\n",
              "                        -1.4107e-02,  2.1378e-02],\n",
              "                       [-5.4283e-03, -8.7509e-03, -4.8798e-02,  ..., -7.5012e-02,\n",
              "                         2.8564e-02,  2.0386e-02],\n",
              "                       [-2.4460e-02, -4.5013e-04, -4.3106e-03,  ..., -2.8015e-02,\n",
              "                        -2.2873e-02,  8.6746e-03],\n",
              "                       ...,\n",
              "                       [-1.1124e-02,  3.8624e-03, -3.5095e-02,  ..., -2.7191e-02,\n",
              "                        -3.5065e-02, -2.3365e-05],\n",
              "                       [ 7.4446e-05,  5.5267e-02, -9.9030e-03,  ...,  3.3112e-02,\n",
              "                        -1.2422e-04, -3.8391e-02],\n",
              "                       [ 1.1909e-02, -1.3447e-04, -1.5930e-02,  ..., -4.4128e-02,\n",
              "                         1.0918e-02,  2.2385e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.12.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.12.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0239,  0.0111, -0.0271,  ...,  0.0332, -0.0297,  0.0146],\n",
              "                       [-0.0106, -0.0239, -0.0112,  ..., -0.0067, -0.0312, -0.0179],\n",
              "                       [ 0.0120,  0.0005,  0.0209,  ..., -0.0033, -0.0318, -0.0316],\n",
              "                       ...,\n",
              "                       [ 0.0090, -0.0183, -0.0121,  ...,  0.0183,  0.0123, -0.0150],\n",
              "                       [ 0.0321,  0.0373,  0.0099,  ...,  0.0226, -0.0163,  0.0244],\n",
              "                       [-0.0273, -0.0053,  0.0328,  ..., -0.0137,  0.0144, -0.0161]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0057, -0.0155,  0.0101,  ...,  0.0060,  0.0078, -0.0040],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.self_attention.out_lin.weight',\n",
              "               tensor([[-0.0227,  0.0011, -0.0218,  ...,  0.0040,  0.0105,  0.0092],\n",
              "                       [-0.0289, -0.0598, -0.0315,  ...,  0.0156, -0.0239, -0.0118],\n",
              "                       [ 0.0079, -0.0148, -0.0174,  ...,  0.0156, -0.0391, -0.0037],\n",
              "                       ...,\n",
              "                       [ 0.0084,  0.0215, -0.0147,  ..., -0.0079,  0.0114,  0.0211],\n",
              "                       [-0.0348,  0.0182, -0.0188,  ...,  0.0042, -0.0098,  0.0104],\n",
              "                       [ 0.0504,  0.0044,  0.0477,  ...,  0.0095, -0.0006,  0.0210]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0067, -0.0053,  0.0109,  ...,  0.0026,  0.0306, -0.0245],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.12.norm1.bias',\n",
              "               tensor([-0.0030, -0.0221, -0.0119,  ..., -0.0189, -0.0249,  0.0367],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0047, -0.0016, -0.0346,  ...,  0.0116, -0.0081,  0.0025],\n",
              "                       [ 0.0385, -0.0390,  0.0059,  ..., -0.0339, -0.0028,  0.0413],\n",
              "                       [ 0.0089, -0.0165,  0.0135,  ...,  0.0046,  0.0519,  0.0100],\n",
              "                       ...,\n",
              "                       [ 0.0001, -0.0234,  0.0424,  ..., -0.0062,  0.0072, -0.0219],\n",
              "                       [ 0.0008,  0.0202,  0.0475,  ..., -0.0016,  0.0010,  0.0181],\n",
              "                       [-0.0160,  0.0098, -0.0105,  ..., -0.0023, -0.0216,  0.0302]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.encoder_attention.q_lin.bias',\n",
              "               tensor([-0.0230, -0.0048,  0.0186,  ..., -0.0013,  0.0188, -0.0119],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 1.6724e-02,  7.9498e-03,  2.6306e-02,  ...,  5.4245e-03,\n",
              "                         7.9966e-04,  4.6959e-03],\n",
              "                       [ 8.3618e-03,  2.6932e-03, -1.3382e-02,  ...,  1.8921e-02,\n",
              "                         1.5617e-02, -3.8452e-03],\n",
              "                       [-6.3934e-03,  1.2520e-02,  2.3712e-02,  ...,  4.2999e-02,\n",
              "                         2.2339e-02, -1.4526e-02],\n",
              "                       ...,\n",
              "                       [-1.0818e-02,  1.7090e-02, -4.5280e-03,  ...,  2.9617e-02,\n",
              "                        -7.1945e-03, -2.0462e-02],\n",
              "                       [-9.1095e-03, -5.8115e-05, -5.3520e-03,  ..., -4.0985e-02,\n",
              "                        -3.4607e-02,  1.2093e-02],\n",
              "                       [-1.1040e-02, -8.4610e-03,  4.2267e-02,  ..., -1.9806e-02,\n",
              "                        -1.2962e-02, -3.2837e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.12.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.12.encoder_attention.v_lin.weight',\n",
              "               tensor([[ 0.0087,  0.0294, -0.0058,  ...,  0.0459,  0.0265, -0.0230],\n",
              "                       [-0.0182, -0.0133, -0.0111,  ..., -0.0057,  0.0049, -0.0055],\n",
              "                       [ 0.0551,  0.0321,  0.0024,  ..., -0.0278, -0.0188, -0.0241],\n",
              "                       ...,\n",
              "                       [ 0.0248, -0.0197, -0.0089,  ..., -0.0102,  0.0117,  0.0334],\n",
              "                       [ 0.0083,  0.0088,  0.0147,  ..., -0.0397,  0.0234,  0.0342],\n",
              "                       [-0.0106, -0.0067, -0.0083,  ..., -0.0051, -0.0080,  0.0305]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.encoder_attention.v_lin.bias',\n",
              "               tensor([-0.0056,  0.0029,  0.0028,  ..., -0.0058,  0.0008,  0.0025],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.encoder_attention.out_lin.weight',\n",
              "               tensor([[-2.1881e-02, -1.5503e-02,  1.5564e-02,  ...,  1.3580e-02,\n",
              "                         5.2786e-04,  1.6525e-02],\n",
              "                       [ 1.5350e-02,  2.6062e-02,  4.6265e-02,  ..., -3.0243e-02,\n",
              "                         6.1157e-02, -6.4964e-03],\n",
              "                       [ 2.0691e-02, -8.9264e-03, -6.1188e-03,  ...,  3.5000e-03,\n",
              "                        -4.9114e-05,  2.3651e-02],\n",
              "                       ...,\n",
              "                       [-8.9874e-03, -2.7039e-02,  8.2626e-03,  ..., -5.2124e-02,\n",
              "                         1.4366e-02,  1.0271e-03],\n",
              "                       [ 1.2642e-02, -1.5038e-02,  3.3245e-03,  ...,  4.2381e-03,\n",
              "                        -6.7863e-03, -2.1805e-02],\n",
              "                       [-3.1281e-02,  1.2741e-02, -2.0599e-02,  ..., -3.5217e-02,\n",
              "                        -1.1749e-02,  9.9182e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.12.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0028, -0.0150,  0.0029,  ...,  0.0074,  0.0219, -0.0310],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.12.norm2.bias',\n",
              "               tensor([-0.0094,  0.0702,  0.0620,  ...,  0.0239,  0.0657, -0.0466],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.ffn.lin1.weight',\n",
              "               tensor([[-0.0622,  0.0072, -0.0057,  ...,  0.0225, -0.0183,  0.0005],\n",
              "                       [-0.0199, -0.0172, -0.0070,  ...,  0.0002,  0.0050,  0.0472],\n",
              "                       [ 0.0522,  0.0006, -0.0172,  ...,  0.0231, -0.0456,  0.0273],\n",
              "                       ...,\n",
              "                       [ 0.0547, -0.0112,  0.0134,  ..., -0.0082, -0.0033,  0.0244],\n",
              "                       [ 0.0078,  0.0038, -0.0061,  ..., -0.0288, -0.0212,  0.0279],\n",
              "                       [-0.0565,  0.0157,  0.0426,  ..., -0.0022, -0.0081, -0.0058]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.ffn.lin1.bias',\n",
              "               tensor([-0.0088, -0.0100, -0.0140,  ..., -0.0218, -0.0123,  0.0001],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.ffn.lin2.weight',\n",
              "               tensor([[ 0.0460, -0.0030, -0.0168,  ...,  0.0275, -0.0251, -0.0026],\n",
              "                       [ 0.0194,  0.0133,  0.0005,  ...,  0.0230, -0.0109, -0.0246],\n",
              "                       [ 0.0100,  0.0269,  0.0266,  ..., -0.0249,  0.0017,  0.0407],\n",
              "                       ...,\n",
              "                       [ 0.0174, -0.0254, -0.0149,  ..., -0.0316,  0.0394, -0.0249],\n",
              "                       [-0.0099,  0.0375,  0.0256,  ..., -0.0018,  0.0329, -0.0051],\n",
              "                       [ 0.0041, -0.0147, -0.0544,  ...,  0.0425,  0.0007,  0.0077]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.ffn.lin2.bias',\n",
              "               tensor([-0.0031,  0.0045, -0.0172,  ...,  0.0056, -0.0136, -0.0055],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.12.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.12.norm3.bias',\n",
              "               tensor([-0.0092, -0.0081, -0.0039,  ...,  0.0306,  0.0492, -0.0714],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.self_attention.q_lin.weight',\n",
              "               tensor([[ 0.0066, -0.0071, -0.0113,  ...,  0.0120,  0.0096, -0.0185],\n",
              "                       [-0.0056,  0.0167,  0.0069,  ...,  0.0074, -0.0326,  0.0192],\n",
              "                       [ 0.0194,  0.0071, -0.0455,  ..., -0.0209,  0.0106, -0.0169],\n",
              "                       ...,\n",
              "                       [ 0.0160,  0.0045, -0.0054,  ..., -0.0139, -0.0121,  0.0013],\n",
              "                       [-0.0078,  0.0308, -0.0203,  ..., -0.0157,  0.0301,  0.0230],\n",
              "                       [ 0.0108, -0.0437,  0.0611,  ...,  0.0096,  0.0359,  0.0193]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0069, -0.0064,  0.0027,  ..., -0.0039,  0.0093, -0.0055],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0210,  0.0019,  0.0005,  ..., -0.0006, -0.0242,  0.0104],\n",
              "                       [ 0.0157,  0.0078, -0.0016,  ..., -0.0135, -0.0143, -0.0079],\n",
              "                       [ 0.0134,  0.0024, -0.0061,  ...,  0.0007,  0.0056,  0.0264],\n",
              "                       ...,\n",
              "                       [ 0.0013,  0.0396, -0.0335,  ...,  0.0390,  0.0131,  0.0169],\n",
              "                       [ 0.0258, -0.0145,  0.0208,  ..., -0.0315,  0.0157, -0.0259],\n",
              "                       [-0.0438, -0.0459, -0.0148,  ...,  0.0149,  0.0425,  0.0055]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.13.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0280, -0.0057,  0.0386,  ..., -0.0192, -0.0306,  0.0566],\n",
              "                       [-0.0099, -0.0174,  0.0037,  ..., -0.0068, -0.0601, -0.0084],\n",
              "                       [-0.0079, -0.0069,  0.0178,  ...,  0.0374,  0.0333,  0.0129],\n",
              "                       ...,\n",
              "                       [-0.0483, -0.0139, -0.0140,  ..., -0.0295, -0.0111, -0.0169],\n",
              "                       [-0.0315,  0.0317,  0.0388,  ..., -0.0252,  0.0020, -0.0057],\n",
              "                       [ 0.0123,  0.0159, -0.0184,  ...,  0.0138, -0.0284, -0.0280]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.self_attention.v_lin.bias',\n",
              "               tensor([-0.0034, -0.0088, -0.0021,  ...,  0.0157, -0.0023, -0.0111],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.self_attention.out_lin.weight',\n",
              "               tensor([[ 0.0129, -0.0278,  0.0412,  ..., -0.0228,  0.0276,  0.0293],\n",
              "                       [ 0.0047,  0.0269,  0.0124,  ...,  0.0261, -0.0086,  0.0318],\n",
              "                       [-0.0285, -0.0161,  0.0008,  ..., -0.0151,  0.0243,  0.0184],\n",
              "                       ...,\n",
              "                       [-0.0177,  0.0099, -0.0089,  ...,  0.0219, -0.0479, -0.0060],\n",
              "                       [ 0.0125,  0.0338, -0.0420,  ..., -0.0047, -0.0218, -0.0287],\n",
              "                       [ 0.0089,  0.0124, -0.0027,  ..., -0.0512, -0.0075,  0.0609]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0147, -0.0157,  0.0039,  ...,  0.0064,  0.0309, -0.0212],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.13.norm1.bias',\n",
              "               tensor([-0.0084, -0.0051, -0.0105,  ..., -0.0190, -0.0246,  0.0439],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.encoder_attention.q_lin.weight',\n",
              "               tensor([[-9.8038e-03, -2.8915e-02, -4.4189e-02,  ...,  1.6632e-03,\n",
              "                        -1.2560e-03,  1.9012e-02],\n",
              "                       [ 1.4587e-02, -3.8834e-03,  8.0338e-03,  ...,  9.2316e-03,\n",
              "                         3.1860e-02, -2.2980e-02],\n",
              "                       [ 4.1809e-03,  3.2196e-02, -2.3270e-03,  ..., -1.5045e-02,\n",
              "                        -4.2152e-03,  1.0826e-02],\n",
              "                       ...,\n",
              "                       [-3.3557e-05,  3.8666e-02,  4.5349e-02,  ...,  2.2011e-03,\n",
              "                         1.1566e-02, -3.1372e-02],\n",
              "                       [-1.4801e-02, -3.4733e-03,  1.1627e-02,  ..., -2.8214e-02,\n",
              "                         2.2293e-02,  2.5055e-02],\n",
              "                       [-4.0436e-02, -2.9816e-02,  2.6749e-02,  ..., -1.5182e-02,\n",
              "                         1.9547e-02, -4.1962e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.13.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0002,  0.0126, -0.0043,  ...,  0.0303, -0.0054, -0.0146],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0255,  0.0188,  0.0189,  ...,  0.0064,  0.0291, -0.0100],\n",
              "                       [-0.0148,  0.0238,  0.0011,  ...,  0.0104,  0.0018, -0.0209],\n",
              "                       [ 0.0069, -0.0064, -0.0017,  ...,  0.0075, -0.0186, -0.0105],\n",
              "                       ...,\n",
              "                       [-0.0019, -0.0314, -0.0237,  ..., -0.0037, -0.0333, -0.0218],\n",
              "                       [ 0.0061,  0.0563, -0.0345,  ..., -0.0374,  0.0188, -0.0226],\n",
              "                       [-0.0209, -0.0251,  0.0107,  ..., -0.0172,  0.0338,  0.0108]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.13.encoder_attention.v_lin.weight',\n",
              "               tensor([[ 0.0082,  0.0179, -0.0199,  ..., -0.0529,  0.0171,  0.0485],\n",
              "                       [-0.0081,  0.0017, -0.0059,  ...,  0.0022, -0.0247,  0.0003],\n",
              "                       [-0.0399,  0.0258,  0.0015,  ..., -0.0401,  0.0089, -0.0271],\n",
              "                       ...,\n",
              "                       [-0.0259,  0.0121,  0.0217,  ..., -0.0275, -0.0317,  0.0004],\n",
              "                       [ 0.0307, -0.0123, -0.0532,  ...,  0.0400,  0.0014, -0.0312],\n",
              "                       [ 0.0061,  0.0219, -0.0103,  ..., -0.0082,  0.0191,  0.0081]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.encoder_attention.v_lin.bias',\n",
              "               tensor([0.0106, 0.0076, 0.0056,  ..., 0.0117, 0.0050, 0.0029],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 2.5272e-05,  2.5406e-02,  4.4250e-02,  ..., -1.0857e-02,\n",
              "                         1.3206e-02,  3.9825e-03],\n",
              "                       [ 6.2294e-03, -1.5869e-02, -1.5068e-02,  ...,  1.0864e-02,\n",
              "                         5.9624e-03,  2.3743e-02],\n",
              "                       [ 1.6495e-02, -8.4829e-04,  5.3345e-02,  ..., -5.1842e-03,\n",
              "                        -1.2306e-02,  1.9989e-02],\n",
              "                       ...,\n",
              "                       [ 3.2532e-02,  1.7410e-02,  9.1095e-03,  ...,  7.8154e-04,\n",
              "                         1.9287e-02,  7.8659e-03],\n",
              "                       [-1.5366e-02,  4.7363e-02,  4.6806e-03,  ...,  1.8326e-02,\n",
              "                        -7.6675e-03,  1.9379e-02],\n",
              "                       [-5.0446e-02,  1.9653e-02, -2.4300e-03,  ...,  2.2522e-02,\n",
              "                         2.2644e-02,  3.3691e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.13.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0042, -0.0158,  0.0015,  ..., -0.0037,  0.0215, -0.0210],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.13.norm2.bias',\n",
              "               tensor([ 0.0015,  0.0625,  0.0625,  ...,  0.0455,  0.0626, -0.0561],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.ffn.lin1.weight',\n",
              "               tensor([[-0.0604,  0.0031,  0.0450,  ...,  0.0482,  0.0015, -0.0485],\n",
              "                       [ 0.0155,  0.0360, -0.0043,  ...,  0.0319,  0.0053, -0.0051],\n",
              "                       [-0.0087,  0.0236,  0.0118,  ...,  0.0172,  0.0120,  0.0055],\n",
              "                       ...,\n",
              "                       [-0.0015, -0.0184, -0.0122,  ..., -0.0177,  0.0298, -0.0144],\n",
              "                       [ 0.0239, -0.0458,  0.0185,  ..., -0.0056, -0.0240, -0.0013],\n",
              "                       [-0.0269,  0.0201,  0.0198,  ...,  0.0190, -0.0215, -0.0191]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.ffn.lin1.bias',\n",
              "               tensor([-0.0115, -0.0184, -0.0086,  ..., -0.0100, -0.0169, -0.0185],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.ffn.lin2.weight',\n",
              "               tensor([[ 0.0225, -0.0112,  0.0008,  ...,  0.0012, -0.0059, -0.0156],\n",
              "                       [ 0.0066, -0.0195, -0.0002,  ..., -0.0137, -0.0137,  0.0125],\n",
              "                       [-0.0110,  0.0355,  0.0055,  ...,  0.0275,  0.0347, -0.0553],\n",
              "                       ...,\n",
              "                       [ 0.0255, -0.0171, -0.0038,  ..., -0.0007,  0.0356, -0.0243],\n",
              "                       [-0.0576, -0.0113, -0.0210,  ...,  0.0352, -0.0256, -0.0311],\n",
              "                       [ 0.0173, -0.0017, -0.0034,  ...,  0.0020,  0.0317, -0.0587]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.ffn.lin2.bias',\n",
              "               tensor([ 0.0054,  0.0046, -0.0060,  ..., -0.0034, -0.0092,  0.0098],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.13.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.13.norm3.bias',\n",
              "               tensor([ 0.0011, -0.0148,  0.0207,  ...,  0.0319,  0.0482, -0.0655],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.self_attention.q_lin.weight',\n",
              "               tensor([[ 0.0134,  0.0063,  0.0544,  ..., -0.0287,  0.0009, -0.0109],\n",
              "                       [ 0.0356, -0.0080,  0.0270,  ...,  0.0325,  0.0025,  0.0203],\n",
              "                       [-0.0542, -0.0389,  0.0259,  ..., -0.0052,  0.0209,  0.0084],\n",
              "                       ...,\n",
              "                       [-0.0216, -0.0435,  0.0545,  ...,  0.0366, -0.0482, -0.0142],\n",
              "                       [-0.0161, -0.0191, -0.0046,  ..., -0.0103,  0.0077,  0.0074],\n",
              "                       [ 0.0328,  0.0346,  0.0122,  ...,  0.0168,  0.0394,  0.0008]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0146,  0.0019,  0.0162,  ..., -0.0009,  0.0005, -0.0032],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0234, -0.0199, -0.0403,  ...,  0.0055,  0.0526,  0.0258],\n",
              "                       [ 0.0206, -0.0009,  0.0015,  ..., -0.0313, -0.0264,  0.0156],\n",
              "                       [ 0.0050, -0.0128,  0.0094,  ...,  0.0049, -0.0373,  0.0132],\n",
              "                       ...,\n",
              "                       [ 0.0252, -0.0350,  0.0169,  ...,  0.0036,  0.0185, -0.0254],\n",
              "                       [-0.0297, -0.0085, -0.0399,  ...,  0.0419, -0.0182,  0.0101],\n",
              "                       [ 0.0047,  0.0159, -0.0162,  ..., -0.0296, -0.0101,  0.0272]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.14.self_attention.v_lin.weight',\n",
              "               tensor([[ 0.0046,  0.0100,  0.0180,  ...,  0.0004, -0.0294,  0.0052],\n",
              "                       [ 0.0331,  0.0253,  0.0318,  ..., -0.0398,  0.0201, -0.0322],\n",
              "                       [-0.0084, -0.0184, -0.0096,  ...,  0.0056, -0.0041,  0.0370],\n",
              "                       ...,\n",
              "                       [-0.0011, -0.0045, -0.0002,  ...,  0.0033,  0.0032,  0.0144],\n",
              "                       [ 0.0368, -0.0159,  0.0181,  ..., -0.0112, -0.0269, -0.0056],\n",
              "                       [-0.0388, -0.0339, -0.0014,  ..., -0.0256, -0.0171, -0.0231]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0008,  0.0042, -0.0033,  ...,  0.0155, -0.0106, -0.0107],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.self_attention.out_lin.weight',\n",
              "               tensor([[-0.0352,  0.0029,  0.0445,  ...,  0.0082, -0.0027, -0.0081],\n",
              "                       [ 0.0180,  0.0019, -0.0509,  ..., -0.0264,  0.0172, -0.0022],\n",
              "                       [-0.0131,  0.0101, -0.0529,  ..., -0.0012,  0.0325, -0.0033],\n",
              "                       ...,\n",
              "                       [-0.0284, -0.0349,  0.0084,  ..., -0.0043, -0.0051,  0.0181],\n",
              "                       [-0.0323,  0.0079, -0.0272,  ..., -0.0136, -0.0109,  0.0043],\n",
              "                       [ 0.0343, -0.0071,  0.0096,  ..., -0.0115, -0.0182, -0.0137]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0089,  0.0022,  0.0011,  ...,  0.0083,  0.0394, -0.0167],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.14.norm1.bias',\n",
              "               tensor([-0.0102, -0.0088, -0.0028,  ..., -0.0176, -0.0312,  0.0305],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.encoder_attention.q_lin.weight',\n",
              "               tensor([[ 0.0005,  0.0175,  0.0285,  ..., -0.0250,  0.0036,  0.0015],\n",
              "                       [-0.0013,  0.0003, -0.0408,  ..., -0.0334, -0.0460,  0.0224],\n",
              "                       [-0.0159,  0.0117, -0.0235,  ...,  0.0081, -0.0177,  0.0149],\n",
              "                       ...,\n",
              "                       [ 0.0280, -0.0088, -0.0079,  ...,  0.0327, -0.0008,  0.0430],\n",
              "                       [-0.0307,  0.0155,  0.0063,  ...,  0.0069, -0.0214,  0.0194],\n",
              "                       [-0.0110, -0.0362, -0.0147,  ...,  0.0296,  0.0011, -0.0222]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0410, -0.0155,  0.0049,  ..., -0.0185,  0.0004,  0.0314],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.encoder_attention.k_lin.weight',\n",
              "               tensor([[-0.0078, -0.0077,  0.0018,  ..., -0.0065,  0.0104,  0.0166],\n",
              "                       [-0.0194,  0.0399,  0.0421,  ...,  0.0043,  0.0033,  0.0119],\n",
              "                       [-0.0064,  0.0119, -0.0329,  ..., -0.0217,  0.0036,  0.0005],\n",
              "                       ...,\n",
              "                       [ 0.0330, -0.0076, -0.0186,  ...,  0.0062,  0.0474,  0.0498],\n",
              "                       [-0.0206,  0.0117,  0.0250,  ...,  0.0321, -0.0240, -0.0167],\n",
              "                       [ 0.0175, -0.0104,  0.0116,  ...,  0.0073,  0.0163, -0.0368]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.14.encoder_attention.v_lin.weight',\n",
              "               tensor([[-5.7030e-03,  2.0874e-02, -9.7084e-04,  ..., -2.5330e-02,\n",
              "                        -2.5681e-02,  2.2125e-02],\n",
              "                       [-2.3468e-02, -1.6357e-02,  5.1994e-03,  ..., -2.4673e-02,\n",
              "                         8.1482e-03,  3.8940e-02],\n",
              "                       [-7.8278e-03, -4.9225e-02, -3.4851e-02,  ..., -5.0537e-02,\n",
              "                         3.4760e-02,  4.7302e-02],\n",
              "                       ...,\n",
              "                       [-3.5522e-02,  1.5659e-03,  1.4870e-02,  ...,  8.1253e-03,\n",
              "                         1.3916e-02,  5.6028e-06],\n",
              "                       [-1.6876e-02,  1.2985e-02, -6.0577e-03,  ...,  3.2166e-02,\n",
              "                         3.2074e-02, -6.0844e-03],\n",
              "                       [ 2.5345e-02,  8.6823e-03, -5.3314e-02,  ...,  1.6947e-03,\n",
              "                        -1.4267e-02,  9.5062e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.14.encoder_attention.v_lin.bias',\n",
              "               tensor([-0.0046,  0.0035,  0.0071,  ...,  0.0124,  0.0016,  0.0022],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0531,  0.0136, -0.0146,  ..., -0.0046, -0.0319,  0.0220],\n",
              "                       [-0.0160, -0.0101,  0.0110,  ..., -0.0105,  0.0024,  0.0037],\n",
              "                       [ 0.0216,  0.0317,  0.0064,  ..., -0.0047,  0.0112,  0.0007],\n",
              "                       ...,\n",
              "                       [ 0.0374, -0.0358, -0.0214,  ...,  0.0059, -0.0130,  0.0051],\n",
              "                       [ 0.0057, -0.0293,  0.0466,  ..., -0.0137, -0.0065,  0.0100],\n",
              "                       [ 0.0258,  0.0122, -0.0211,  ...,  0.0138,  0.0193, -0.0008]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0162, -0.0138, -0.0069,  ..., -0.0045,  0.0332, -0.0234],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.14.norm2.bias',\n",
              "               tensor([ 0.0617,  0.0626,  0.0621,  ...,  0.0334,  0.0623, -0.0576],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.ffn.lin1.weight',\n",
              "               tensor([[-0.0160,  0.0396,  0.0030,  ..., -0.0208, -0.0327, -0.0122],\n",
              "                       [ 0.0202,  0.0150, -0.0125,  ..., -0.0134, -0.0408,  0.0314],\n",
              "                       [-0.0031, -0.0279,  0.0086,  ..., -0.0073,  0.0105, -0.0052],\n",
              "                       ...,\n",
              "                       [-0.0086,  0.0107, -0.0324,  ...,  0.0142, -0.0219, -0.0046],\n",
              "                       [-0.0365,  0.0215,  0.0099,  ...,  0.0167, -0.0193,  0.0439],\n",
              "                       [ 0.0268,  0.0069, -0.0040,  ...,  0.0312, -0.0234,  0.0424]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.ffn.lin1.bias',\n",
              "               tensor([-0.0087, -0.0074, -0.0153,  ..., -0.0105, -0.0079, -0.0062],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.ffn.lin2.weight',\n",
              "               tensor([[-1.4105e-03, -4.0833e-02,  2.4979e-02,  ...,  2.1866e-02,\n",
              "                        -7.6981e-03, -6.1493e-02],\n",
              "                       [ 6.1226e-03,  2.6062e-02,  3.2898e-02,  ..., -1.8875e-02,\n",
              "                        -2.5116e-02,  6.1607e-03],\n",
              "                       [ 5.1392e-02,  2.9404e-02,  5.4108e-02,  ...,  1.0033e-02,\n",
              "                        -3.5339e-02,  7.1754e-03],\n",
              "                       ...,\n",
              "                       [ 1.1078e-02,  1.6022e-02, -7.9727e-03,  ...,  3.1555e-02,\n",
              "                         2.9709e-02, -3.9399e-05],\n",
              "                       [-2.4185e-02,  1.5884e-02, -1.0590e-02,  ...,  3.2562e-02,\n",
              "                         3.4149e-02,  5.4665e-03],\n",
              "                       [ 5.8289e-03,  1.8301e-03,  5.5817e-02,  ...,  2.3682e-02,\n",
              "                        -1.2238e-02, -3.9551e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.14.ffn.lin2.bias',\n",
              "               tensor([ 0.0127, -0.0051,  0.0005,  ..., -0.0025, -0.0147,  0.0083],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.14.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.14.norm3.bias',\n",
              "               tensor([ 0.0120, -0.0061,  0.0131,  ...,  0.0400,  0.0389, -0.0406],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.self_attention.q_lin.weight',\n",
              "               tensor([[ 0.0375,  0.0005, -0.0223,  ...,  0.0232, -0.0354,  0.0023],\n",
              "                       [-0.0082,  0.0181,  0.0199,  ...,  0.0299, -0.0272, -0.0222],\n",
              "                       [ 0.0095,  0.0013,  0.0132,  ...,  0.0078,  0.0098,  0.0251],\n",
              "                       ...,\n",
              "                       [-0.0112, -0.0016,  0.0002,  ...,  0.0057, -0.0231, -0.0022],\n",
              "                       [-0.0019,  0.0061, -0.0165,  ..., -0.0089,  0.0011, -0.0123],\n",
              "                       [ 0.0205, -0.0034, -0.0194,  ...,  0.0073, -0.0078,  0.0569]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0176,  0.0073, -0.0033,  ..., -0.0192,  0.0119, -0.0172],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.self_attention.k_lin.weight',\n",
              "               tensor([[-0.0317, -0.0247, -0.0154,  ...,  0.0304, -0.0520,  0.0344],\n",
              "                       [ 0.0163,  0.0346,  0.0089,  ...,  0.0546, -0.0531,  0.0342],\n",
              "                       [-0.0081, -0.0099,  0.0125,  ...,  0.0042,  0.0013,  0.0051],\n",
              "                       ...,\n",
              "                       [-0.0037, -0.0265, -0.0129,  ..., -0.0114,  0.0198,  0.0400],\n",
              "                       [-0.0297,  0.0628,  0.0013,  ...,  0.0330, -0.0087, -0.0154],\n",
              "                       [-0.0056, -0.0008,  0.0174,  ...,  0.0614, -0.0528,  0.0051]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.15.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0442, -0.0103, -0.0491,  ...,  0.0365,  0.0119,  0.0124],\n",
              "                       [ 0.0378, -0.0099, -0.0349,  ...,  0.0277,  0.0086,  0.0117],\n",
              "                       [-0.0231,  0.0139,  0.0014,  ...,  0.0041,  0.0196,  0.0024],\n",
              "                       ...,\n",
              "                       [ 0.0325,  0.0347,  0.0088,  ..., -0.0339,  0.0294, -0.0128],\n",
              "                       [ 0.0154,  0.0122,  0.0365,  ...,  0.0058,  0.0328,  0.0054],\n",
              "                       [-0.0339,  0.0082, -0.0393,  ...,  0.0349, -0.0154, -0.0159]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.self_attention.v_lin.bias',\n",
              "               tensor([ 7.3738e-03, -2.4557e-04, -1.0862e-03,  ...,  4.5848e-04,\n",
              "                        1.1269e-02, -8.8155e-05], dtype=torch.float16)),\n",
              "              ('decoder.layers.15.self_attention.out_lin.weight',\n",
              "               tensor([[ 0.0147,  0.0089, -0.0005,  ..., -0.0320,  0.0206,  0.0077],\n",
              "                       [-0.0097, -0.0067, -0.0003,  ...,  0.0176,  0.0102, -0.0276],\n",
              "                       [-0.0025, -0.0125, -0.0232,  ..., -0.0087, -0.0355,  0.0108],\n",
              "                       ...,\n",
              "                       [ 0.0296, -0.0128,  0.0179,  ...,  0.0207, -0.0237, -0.0238],\n",
              "                       [-0.0352, -0.0051, -0.0047,  ...,  0.0061, -0.0314,  0.0134],\n",
              "                       [-0.0127, -0.0031,  0.0063,  ...,  0.0340, -0.0285, -0.0115]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0135, -0.0044,  0.0131,  ...,  0.0068,  0.0333, -0.0238],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.15.norm1.bias',\n",
              "               tensor([-0.0192, -0.0015,  0.0041,  ..., -0.0145, -0.0231,  0.0323],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.encoder_attention.q_lin.weight',\n",
              "               tensor([[ 0.0209,  0.0562, -0.0313,  ...,  0.0225, -0.0228, -0.0201],\n",
              "                       [-0.0066, -0.0135, -0.0323,  ...,  0.0141, -0.0209, -0.0511],\n",
              "                       [ 0.0024,  0.0015,  0.0121,  ...,  0.0245,  0.0103,  0.0155],\n",
              "                       ...,\n",
              "                       [-0.0260,  0.0198, -0.0279,  ..., -0.0185, -0.0237, -0.0199],\n",
              "                       [ 0.0201,  0.0153,  0.0379,  ...,  0.0279,  0.0206, -0.0216],\n",
              "                       [ 0.0361,  0.0013,  0.0174,  ..., -0.0259, -0.0155, -0.0012]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0014,  0.0018, -0.0009,  ...,  0.0077,  0.0078,  0.0041],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0422, -0.0381,  0.0343,  ..., -0.0040,  0.0019, -0.0313],\n",
              "                       [ 0.0100,  0.0154, -0.0118,  ...,  0.0138, -0.0442, -0.0210],\n",
              "                       [-0.0125, -0.0339,  0.0016,  ...,  0.0515, -0.0112,  0.0214],\n",
              "                       ...,\n",
              "                       [-0.0071, -0.0331, -0.0194,  ...,  0.0316,  0.0060, -0.0011],\n",
              "                       [ 0.0105, -0.0353, -0.0375,  ..., -0.0401,  0.0165, -0.0297],\n",
              "                       [-0.0147,  0.0179, -0.0117,  ...,  0.0162, -0.0158,  0.0169]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.15.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0337,  0.0169,  0.0198,  ...,  0.0258, -0.0230,  0.0224],\n",
              "                       [-0.0028, -0.0023, -0.0014,  ...,  0.0103,  0.0041, -0.0329],\n",
              "                       [ 0.0078,  0.0390, -0.0395,  ...,  0.0036,  0.0323,  0.0107],\n",
              "                       ...,\n",
              "                       [-0.0179, -0.0135,  0.0280,  ...,  0.0015,  0.0111,  0.0027],\n",
              "                       [ 0.0261,  0.0053,  0.0079,  ..., -0.0250, -0.0010, -0.0042],\n",
              "                       [-0.0216,  0.0162,  0.0146,  ..., -0.0359,  0.0024,  0.0320]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.encoder_attention.v_lin.bias',\n",
              "               tensor([-0.0187,  0.0032, -0.0043,  ..., -0.0155, -0.0059, -0.0009],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0349,  0.0112,  0.0069,  ...,  0.0253,  0.0053,  0.0041],\n",
              "                       [-0.0280, -0.0339, -0.0049,  ..., -0.0232,  0.0093, -0.0040],\n",
              "                       [-0.0025,  0.0191, -0.0013,  ..., -0.0024, -0.0357,  0.0204],\n",
              "                       ...,\n",
              "                       [ 0.0088,  0.0082,  0.0259,  ...,  0.0384,  0.0100, -0.0159],\n",
              "                       [ 0.0093,  0.0429,  0.0346,  ..., -0.0104, -0.0079, -0.0373],\n",
              "                       [-0.0318,  0.0350, -0.0502,  ..., -0.0018, -0.0384,  0.0240]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0166, -0.0099,  0.0008,  ..., -0.0057,  0.0327, -0.0231],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.15.norm2.bias',\n",
              "               tensor([ 0.0284,  0.0723,  0.0548,  ...,  0.0460,  0.0622, -0.0317],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.ffn.lin1.weight',\n",
              "               tensor([[ 0.0079, -0.0035, -0.0343,  ...,  0.0114, -0.0016, -0.0209],\n",
              "                       [-0.0289,  0.0076, -0.0163,  ..., -0.0164, -0.0048, -0.0059],\n",
              "                       [ 0.0078,  0.0156,  0.0034,  ..., -0.0626, -0.0028, -0.0238],\n",
              "                       ...,\n",
              "                       [ 0.0037, -0.0011, -0.0385,  ..., -0.0148, -0.0126, -0.0190],\n",
              "                       [ 0.0016,  0.0143,  0.0172,  ..., -0.0021, -0.0481, -0.0070],\n",
              "                       [-0.0050,  0.0220, -0.0411,  ..., -0.0637,  0.0287, -0.0041]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.ffn.lin1.bias',\n",
              "               tensor([-0.0178, -0.0102, -0.0105,  ..., -0.0162, -0.0079, -0.0095],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.ffn.lin2.weight',\n",
              "               tensor([[-0.0008,  0.0533, -0.0051,  ..., -0.0330, -0.0303, -0.0122],\n",
              "                       [-0.0126,  0.0190, -0.0336,  ..., -0.0050,  0.0435,  0.0070],\n",
              "                       [-0.0031, -0.0351,  0.0146,  ..., -0.0091, -0.0315,  0.0080],\n",
              "                       ...,\n",
              "                       [ 0.0165, -0.0072, -0.0214,  ...,  0.0088,  0.0098,  0.0218],\n",
              "                       [ 0.0069,  0.0097, -0.0007,  ...,  0.0310,  0.0033,  0.0526],\n",
              "                       [-0.0204, -0.0146, -0.0161,  ...,  0.0217,  0.0429, -0.0144]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.ffn.lin2.bias',\n",
              "               tensor([ 0.0223, -0.0135, -0.0138,  ...,  0.0086,  0.0070,  0.0028],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.15.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.15.norm3.bias',\n",
              "               tensor([ 0.0098, -0.0017,  0.0293,  ...,  0.0428,  0.0748, -0.0316],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.self_attention.q_lin.weight',\n",
              "               tensor([[-0.0139,  0.0341, -0.0085,  ...,  0.0374,  0.0066, -0.0018],\n",
              "                       [ 0.0112, -0.0021,  0.0023,  ...,  0.0586,  0.0104, -0.0331],\n",
              "                       [-0.0143,  0.0035,  0.0082,  ..., -0.0136,  0.0157, -0.0056],\n",
              "                       ...,\n",
              "                       [-0.0191,  0.0327, -0.0045,  ...,  0.0062,  0.0051,  0.0140],\n",
              "                       [ 0.0101,  0.0051, -0.0585,  ..., -0.0814,  0.0486, -0.0246],\n",
              "                       [ 0.0068,  0.0271,  0.0054,  ...,  0.0320, -0.0130,  0.0127]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.self_attention.q_lin.bias',\n",
              "               tensor([0.0097, 0.0156, 0.0120,  ..., 0.0078, 0.0090, 0.0174],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0093,  0.0049, -0.0318,  ...,  0.0188, -0.0179, -0.0013],\n",
              "                       [-0.0272,  0.0490, -0.0093,  ...,  0.0238, -0.0163, -0.0124],\n",
              "                       [ 0.0174,  0.0144,  0.0023,  ..., -0.0123, -0.0060,  0.0234],\n",
              "                       ...,\n",
              "                       [ 0.0143, -0.0349, -0.0284,  ..., -0.0126,  0.0066, -0.0138],\n",
              "                       [-0.0030, -0.0118, -0.0004,  ..., -0.0024,  0.0296, -0.0255],\n",
              "                       [ 0.0418,  0.0613,  0.0043,  ..., -0.0308,  0.0013,  0.0018]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.16.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0156,  0.0125, -0.0151,  ..., -0.0515, -0.0109, -0.0047],\n",
              "                       [-0.0487,  0.0034, -0.0128,  ..., -0.0263, -0.0142,  0.0316],\n",
              "                       [ 0.0624, -0.0249, -0.0061,  ..., -0.0329,  0.0107,  0.0265],\n",
              "                       ...,\n",
              "                       [ 0.0415, -0.0317, -0.0136,  ...,  0.0305,  0.0237, -0.0245],\n",
              "                       [-0.0189, -0.0151, -0.0398,  ..., -0.0316,  0.0003, -0.0244],\n",
              "                       [ 0.0191,  0.0252, -0.0233,  ...,  0.0017, -0.0432,  0.0307]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.self_attention.v_lin.bias',\n",
              "               tensor([-0.0152,  0.0155,  0.0057,  ..., -0.0032,  0.0099, -0.0108],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.self_attention.out_lin.weight',\n",
              "               tensor([[ 0.0262,  0.0089,  0.0304,  ...,  0.0094,  0.0080,  0.0473],\n",
              "                       [-0.0266, -0.0092,  0.0022,  ..., -0.0063, -0.0038,  0.0345],\n",
              "                       [-0.0158,  0.0362, -0.0317,  ..., -0.0787,  0.0044, -0.0555],\n",
              "                       ...,\n",
              "                       [-0.0120,  0.0209,  0.0049,  ...,  0.0257,  0.0314,  0.0117],\n",
              "                       [-0.0176, -0.0008, -0.0249,  ..., -0.0341, -0.0087,  0.0109],\n",
              "                       [ 0.0464,  0.0016, -0.0221,  ...,  0.0342,  0.0013,  0.0006]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0213,  0.0037, -0.0061,  ..., -0.0044,  0.0296, -0.0267],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.16.norm1.bias',\n",
              "               tensor([-0.0116, -0.0116,  0.0005,  ..., -0.0295, -0.0386,  0.0311],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.encoder_attention.q_lin.weight',\n",
              "               tensor([[ 5.2032e-03,  5.2452e-06, -6.6833e-03,  ..., -1.4885e-02,\n",
              "                        -9.7733e-03, -4.1260e-02],\n",
              "                       [ 2.8671e-02,  7.8506e-03, -1.0078e-02,  ..., -3.2485e-05,\n",
              "                        -5.6648e-04,  2.8610e-02],\n",
              "                       [-1.0345e-02,  2.6379e-03, -1.4740e-02,  ..., -7.0381e-03,\n",
              "                        -2.8671e-02, -1.1246e-02],\n",
              "                       ...,\n",
              "                       [-3.6560e-02, -3.0594e-03, -1.7700e-02,  ...,  2.0355e-02,\n",
              "                        -3.7346e-03, -1.3046e-02],\n",
              "                       [ 2.7828e-03,  1.7670e-02,  1.3412e-02,  ...,  7.7438e-03,\n",
              "                        -4.1351e-03,  2.2400e-02],\n",
              "                       [ 3.4973e-02, -9.5520e-03, -1.5625e-02,  ...,  1.7471e-02,\n",
              "                        -1.9989e-03,  4.0710e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.16.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0084, -0.0035, -0.0608,  ..., -0.0215,  0.0070, -0.0318],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0328,  0.0159, -0.0146,  ...,  0.0261, -0.0368,  0.0175],\n",
              "                       [ 0.0590,  0.0004,  0.0244,  ...,  0.0033, -0.0207,  0.0028],\n",
              "                       [-0.0006,  0.0209, -0.0203,  ..., -0.0017, -0.0156,  0.0020],\n",
              "                       ...,\n",
              "                       [ 0.0036, -0.0156, -0.0237,  ...,  0.0289, -0.0041, -0.0101],\n",
              "                       [-0.0067, -0.0042, -0.0073,  ..., -0.0076, -0.0005, -0.0245],\n",
              "                       [ 0.0250,  0.0242, -0.0181,  ..., -0.0110, -0.0133,  0.0017]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.16.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0127, -0.0360,  0.0284,  ..., -0.0108,  0.0157, -0.0430],\n",
              "                       [ 0.0331, -0.0146,  0.0283,  ...,  0.0002,  0.0101,  0.0104],\n",
              "                       [-0.0073,  0.0223,  0.0217,  ..., -0.0348,  0.0205, -0.0148],\n",
              "                       ...,\n",
              "                       [-0.0147, -0.0008,  0.0293,  ..., -0.0271,  0.0255, -0.0012],\n",
              "                       [ 0.0013, -0.0327,  0.0576,  ..., -0.0011,  0.0189, -0.0039],\n",
              "                       [ 0.0331,  0.0074,  0.0205,  ...,  0.0105,  0.0115,  0.0131]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.encoder_attention.v_lin.bias',\n",
              "               tensor([-0.0213, -0.0315, -0.0374,  ...,  0.0102, -0.0078, -0.0002],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 3.5095e-02, -6.2927e-02,  1.1301e-04,  ...,  2.1423e-02,\n",
              "                        -5.3704e-05,  1.4526e-02],\n",
              "                       [-4.8065e-03,  1.6678e-02, -2.0477e-02,  ...,  2.3636e-02,\n",
              "                        -2.9633e-02, -1.2426e-03],\n",
              "                       [-3.6335e-03, -2.3682e-02, -3.4447e-03,  ..., -4.4899e-03,\n",
              "                         1.2527e-02,  4.0985e-02],\n",
              "                       ...,\n",
              "                       [ 1.1688e-02,  3.3234e-02,  1.2980e-03,  ..., -1.7242e-02,\n",
              "                        -1.6891e-02,  2.2400e-02],\n",
              "                       [-1.8425e-03, -8.0872e-03,  4.4556e-02,  ...,  9.8801e-03,\n",
              "                         2.1561e-02, -2.7267e-02],\n",
              "                       [-1.6815e-02, -1.1429e-02,  4.3640e-03,  ...,  2.2964e-02,\n",
              "                        -1.9379e-02,  9.1019e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.16.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0121, -0.0103, -0.0038,  ...,  0.0006,  0.0276, -0.0166],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.16.norm2.bias',\n",
              "               tensor([ 0.0220,  0.0620,  0.0626,  ...,  0.0504,  0.0556, -0.0338],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.ffn.lin1.weight',\n",
              "               tensor([[-0.0374, -0.0156,  0.0012,  ..., -0.0515, -0.0268, -0.0188],\n",
              "                       [-0.0121,  0.0136,  0.0242,  ...,  0.0045, -0.0076,  0.0180],\n",
              "                       [ 0.0487,  0.0493,  0.0112,  ..., -0.0216,  0.0057, -0.0047],\n",
              "                       ...,\n",
              "                       [ 0.0156,  0.0402,  0.0035,  ...,  0.0113,  0.0084, -0.0093],\n",
              "                       [ 0.0076, -0.0109, -0.0096,  ...,  0.0285, -0.0528,  0.0155],\n",
              "                       [-0.0234, -0.0065, -0.0105,  ...,  0.0353, -0.0179,  0.0660]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.ffn.lin1.bias',\n",
              "               tensor([-1.2314e-02, -1.4221e-02, -3.0518e-03,  ...,  4.3154e-05,\n",
              "                       -6.3848e-04, -1.3710e-02], dtype=torch.float16)),\n",
              "              ('decoder.layers.16.ffn.lin2.weight',\n",
              "               tensor([[ 0.0172, -0.0027, -0.0087,  ..., -0.0153, -0.0271, -0.0217],\n",
              "                       [-0.0129, -0.0341, -0.0349,  ...,  0.0021,  0.0216,  0.0035],\n",
              "                       [-0.0181,  0.0158,  0.0193,  ...,  0.0232, -0.0047, -0.0190],\n",
              "                       ...,\n",
              "                       [-0.0046, -0.0047,  0.0500,  ...,  0.0094, -0.0428, -0.0041],\n",
              "                       [-0.0254, -0.0122,  0.0037,  ..., -0.0105, -0.0014, -0.0014],\n",
              "                       [ 0.0278, -0.0301,  0.0338,  ..., -0.0148,  0.0027, -0.0268]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.ffn.lin2.bias',\n",
              "               tensor([ 0.0257, -0.0047, -0.0067,  ...,  0.0030,  0.0242,  0.0071],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.16.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.16.norm3.bias',\n",
              "               tensor([ 0.0423,  0.0073,  0.0184,  ...,  0.0428,  0.0825, -0.0506],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.self_attention.q_lin.weight',\n",
              "               tensor([[ 0.0058, -0.0136,  0.0255,  ...,  0.0452, -0.0373, -0.0083],\n",
              "                       [-0.0046,  0.0006,  0.0052,  ..., -0.0059,  0.0246, -0.0134],\n",
              "                       [-0.0021,  0.0107,  0.0147,  ...,  0.0270, -0.0028, -0.0326],\n",
              "                       ...,\n",
              "                       [-0.0082,  0.0100,  0.0015,  ...,  0.0272, -0.0066, -0.0205],\n",
              "                       [ 0.0514, -0.0403,  0.0278,  ..., -0.0371,  0.0113,  0.0053],\n",
              "                       [-0.0075,  0.0257, -0.0044,  ...,  0.0385,  0.0431, -0.0014]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0280,  0.0071, -0.0050,  ..., -0.0018,  0.0140,  0.0031],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0038,  0.0344, -0.0257,  ...,  0.0161, -0.0111,  0.0109],\n",
              "                       [-0.0528,  0.0329,  0.0044,  ..., -0.0133,  0.0222, -0.0078],\n",
              "                       [-0.0475, -0.0035, -0.0079,  ...,  0.0126, -0.0199, -0.0180],\n",
              "                       ...,\n",
              "                       [-0.0308, -0.0317, -0.0259,  ..., -0.0008,  0.0227, -0.0028],\n",
              "                       [-0.0010,  0.0474,  0.0036,  ..., -0.0062,  0.0375, -0.0138],\n",
              "                       [-0.0079, -0.0350,  0.0114,  ..., -0.0027, -0.0190,  0.0269]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.17.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0455,  0.0168, -0.0009,  ..., -0.0291, -0.0254,  0.0039],\n",
              "                       [-0.0424,  0.0197, -0.0569,  ...,  0.0423, -0.0124, -0.0063],\n",
              "                       [ 0.0589, -0.0446,  0.0231,  ..., -0.0144,  0.0296,  0.0152],\n",
              "                       ...,\n",
              "                       [-0.0284,  0.0041, -0.0092,  ...,  0.0527,  0.0049, -0.0139],\n",
              "                       [ 0.0283, -0.0170,  0.0254,  ..., -0.0296,  0.0012, -0.0211],\n",
              "                       [-0.0209,  0.0019, -0.0069,  ...,  0.0298,  0.0157, -0.0154]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0022, -0.0147, -0.0112,  ..., -0.0009, -0.0037,  0.0076],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.self_attention.out_lin.weight',\n",
              "               tensor([[-2.5787e-02,  4.3793e-02, -3.0914e-02,  ..., -2.6962e-02,\n",
              "                         1.7639e-02,  3.8242e-03],\n",
              "                       [ 1.1307e-02,  1.9302e-02,  3.4607e-02,  ..., -5.3978e-03,\n",
              "                        -1.1803e-02,  2.0950e-02],\n",
              "                       [-3.3813e-02,  5.0049e-03,  4.1473e-02,  ..., -1.2245e-02,\n",
              "                         4.6463e-03, -3.1006e-02],\n",
              "                       ...,\n",
              "                       [ 2.3636e-02, -4.2389e-02, -4.9553e-03,  ...,  2.9510e-02,\n",
              "                        -2.1317e-02, -1.2703e-02],\n",
              "                       [ 3.2043e-02, -6.0181e-02,  8.0719e-03,  ...,  1.1421e-02,\n",
              "                         1.8051e-02,  1.0475e-02],\n",
              "                       [-1.8509e-02, -5.0842e-02,  9.6023e-05,  ..., -6.2561e-02,\n",
              "                        -1.0124e-02, -3.7567e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.17.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0219,  0.0024,  0.0035,  ...,  0.0057,  0.0325, -0.0221],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.17.norm1.bias',\n",
              "               tensor([-0.0079,  0.0137,  0.0004,  ..., -0.0221, -0.0340,  0.0330],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.encoder_attention.q_lin.weight',\n",
              "               tensor([[ 0.0072,  0.0167,  0.0209,  ...,  0.0133,  0.0313, -0.0217],\n",
              "                       [-0.0300, -0.0173, -0.0112,  ...,  0.0050, -0.0054,  0.0055],\n",
              "                       [-0.0286, -0.0022,  0.0310,  ...,  0.0107, -0.0072, -0.0042],\n",
              "                       ...,\n",
              "                       [-0.0209,  0.0273,  0.0059,  ...,  0.0098, -0.0332, -0.0092],\n",
              "                       [-0.0223,  0.0117,  0.0071,  ..., -0.0089, -0.0006,  0.0047],\n",
              "                       [-0.0069,  0.0277, -0.0154,  ...,  0.0386,  0.0076,  0.0107]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0388,  0.0162,  0.0149,  ..., -0.0075, -0.0016, -0.0084],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.encoder_attention.k_lin.weight',\n",
              "               tensor([[-3.2306e-05,  6.3477e-03, -2.7504e-03,  ..., -2.9106e-03,\n",
              "                         1.6678e-02,  2.5864e-02],\n",
              "                       [ 1.5755e-03,  1.6464e-02, -1.2146e-02,  ..., -2.7420e-02,\n",
              "                        -4.4084e-04, -3.7689e-02],\n",
              "                       [-3.6831e-03,  3.1799e-02, -6.9160e-03,  ...,  4.2224e-04,\n",
              "                         3.9703e-02,  4.5433e-03],\n",
              "                       ...,\n",
              "                       [-5.3482e-03, -6.2103e-03,  2.1698e-02,  ..., -1.8816e-03,\n",
              "                         2.0218e-02, -4.8798e-02],\n",
              "                       [ 1.4391e-03,  3.8818e-02,  4.9652e-02,  ...,  2.7466e-02,\n",
              "                        -2.0943e-03,  2.5253e-02],\n",
              "                       [-1.5945e-02,  9.5520e-03, -4.6158e-03,  ...,  2.1881e-02,\n",
              "                        -8.6060e-03, -1.6006e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.17.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.17.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0078, -0.0083,  0.0013,  ...,  0.0413,  0.0270,  0.0295],\n",
              "                       [ 0.0094, -0.0238, -0.0210,  ..., -0.0300, -0.0417,  0.0283],\n",
              "                       [ 0.0307,  0.0037, -0.0301,  ...,  0.0581,  0.0047, -0.0097],\n",
              "                       ...,\n",
              "                       [ 0.0383, -0.0257,  0.0569,  ..., -0.0072,  0.0008,  0.0170],\n",
              "                       [ 0.0202,  0.0130,  0.0371,  ...,  0.0174,  0.0517, -0.0001],\n",
              "                       [-0.0008, -0.0212,  0.0354,  ...,  0.0066,  0.0206,  0.0256]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.encoder_attention.v_lin.bias',\n",
              "               tensor([-0.0115, -0.0265,  0.0065,  ...,  0.0151,  0.0294,  0.0011],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0467,  0.0196, -0.0062,  ..., -0.0125, -0.0027,  0.0007],\n",
              "                       [-0.0248,  0.0045,  0.0244,  ..., -0.0287,  0.0326,  0.0356],\n",
              "                       [-0.0140,  0.0040,  0.0446,  ..., -0.0318,  0.0161,  0.0113],\n",
              "                       ...,\n",
              "                       [-0.0079, -0.0045, -0.0032,  ...,  0.0111,  0.0159, -0.0026],\n",
              "                       [-0.0183, -0.0128, -0.0025,  ...,  0.0035, -0.0110, -0.0052],\n",
              "                       [-0.0174, -0.0205, -0.0160,  ...,  0.0012,  0.0003,  0.0143]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0327, -0.0121, -0.0126,  ..., -0.0040,  0.0309, -0.0331],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.17.norm2.bias',\n",
              "               tensor([ 0.0179,  0.0715,  0.0592,  ...,  0.0415,  0.0624, -0.0370],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.ffn.lin1.weight',\n",
              "               tensor([[ 0.0171, -0.0340,  0.0097,  ..., -0.0237, -0.0054,  0.0142],\n",
              "                       [-0.0511, -0.0051,  0.0355,  ...,  0.0217,  0.0015, -0.0329],\n",
              "                       [ 0.0238,  0.0170,  0.0019,  ...,  0.0147, -0.0226,  0.0156],\n",
              "                       ...,\n",
              "                       [-0.0354,  0.0197,  0.0348,  ...,  0.0020, -0.0190, -0.0316],\n",
              "                       [-0.0177, -0.0265,  0.0478,  ...,  0.0146,  0.0372, -0.0452],\n",
              "                       [ 0.0397, -0.0292, -0.0029,  ..., -0.0113,  0.0354, -0.0130]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.ffn.lin1.bias',\n",
              "               tensor([-0.0029, -0.0121, -0.0202,  ..., -0.0003, -0.0070, -0.0088],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.ffn.lin2.weight',\n",
              "               tensor([[ 0.0071,  0.0063,  0.0193,  ...,  0.0515,  0.0030, -0.0217],\n",
              "                       [ 0.0155, -0.0182,  0.0311,  ..., -0.0373, -0.0006, -0.0237],\n",
              "                       [ 0.0056,  0.0217, -0.0086,  ..., -0.0111, -0.0546, -0.0173],\n",
              "                       ...,\n",
              "                       [-0.0032, -0.0489, -0.0357,  ...,  0.0140, -0.0153, -0.0021],\n",
              "                       [-0.0493, -0.0115,  0.0078,  ...,  0.0167, -0.0137,  0.0309],\n",
              "                       [-0.0035, -0.0413, -0.0313,  ...,  0.0084,  0.0622, -0.0028]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.ffn.lin2.bias',\n",
              "               tensor([ 0.0236, -0.0014, -0.0134,  ...,  0.0066,  0.0311, -0.0013],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.17.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.17.norm3.bias',\n",
              "               tensor([ 0.0374,  0.0282,  0.0155,  ...,  0.0326,  0.0489, -0.0164],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.self_attention.q_lin.weight',\n",
              "               tensor([[ 3.5065e-02,  6.2866e-03,  1.3344e-02,  ..., -8.6021e-04,\n",
              "                         1.2581e-02, -2.0428e-03],\n",
              "                       [-2.3895e-02, -9.1553e-03,  3.6377e-02,  ..., -2.5497e-02,\n",
              "                         3.0334e-02,  9.1171e-03],\n",
              "                       [ 3.5143e-04,  1.7014e-02,  3.8300e-03,  ...,  4.2725e-03,\n",
              "                        -7.2145e-04,  4.9553e-03],\n",
              "                       ...,\n",
              "                       [ 1.8097e-02, -9.2010e-03, -8.6594e-03,  ...,  1.5480e-02,\n",
              "                        -3.4695e-03,  1.5266e-02],\n",
              "                       [-3.9825e-02, -3.5858e-02,  2.0966e-02,  ..., -1.8494e-02,\n",
              "                        -1.6220e-02, -6.2981e-03],\n",
              "                       [-1.9012e-02,  3.4580e-03, -1.4076e-02,  ...,  8.5831e-05,\n",
              "                        -7.8888e-03,  9.8114e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.18.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0381,  0.0012, -0.0188,  ..., -0.0093, -0.0094,  0.0042],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.self_attention.k_lin.weight',\n",
              "               tensor([[-0.0078,  0.0156,  0.0084,  ...,  0.0279,  0.0236, -0.0163],\n",
              "                       [-0.0241, -0.0240, -0.0010,  ...,  0.0027, -0.0116, -0.0141],\n",
              "                       [ 0.0298, -0.0225, -0.0048,  ..., -0.0125, -0.0184, -0.0369],\n",
              "                       ...,\n",
              "                       [ 0.0226,  0.0052,  0.0089,  ...,  0.0139, -0.0041,  0.0177],\n",
              "                       [-0.0211,  0.0289,  0.0351,  ..., -0.0331, -0.0042, -0.0138],\n",
              "                       [-0.0062,  0.0072, -0.0260,  ..., -0.0204, -0.0128,  0.0270]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.18.self_attention.v_lin.weight',\n",
              "               tensor([[ 0.0236, -0.0048,  0.0112,  ...,  0.0393,  0.0144, -0.0021],\n",
              "                       [ 0.0197, -0.0130, -0.0480,  ..., -0.0364,  0.0092,  0.0237],\n",
              "                       [-0.0281,  0.0177, -0.0165,  ..., -0.0289,  0.0078, -0.0061],\n",
              "                       ...,\n",
              "                       [-0.0085,  0.0406,  0.0173,  ..., -0.0288, -0.0034,  0.0160],\n",
              "                       [-0.0229, -0.0090,  0.0097,  ..., -0.0340,  0.0291,  0.0380],\n",
              "                       [-0.0236, -0.0288, -0.0304,  ..., -0.0023,  0.0195, -0.0086]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.self_attention.v_lin.bias',\n",
              "               tensor([-0.0013, -0.0003,  0.0032,  ..., -0.0094, -0.0130, -0.0091],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.self_attention.out_lin.weight',\n",
              "               tensor([[-0.0108,  0.0098, -0.0051,  ..., -0.0185, -0.0275, -0.0173],\n",
              "                       [-0.0213,  0.0262,  0.0326,  ...,  0.0140, -0.0344,  0.0647],\n",
              "                       [ 0.0125, -0.0138,  0.0298,  ...,  0.0365, -0.0270,  0.0539],\n",
              "                       ...,\n",
              "                       [ 0.0234, -0.0013,  0.0138,  ...,  0.0128,  0.0211,  0.0061],\n",
              "                       [-0.0340, -0.0058,  0.0211,  ..., -0.0028,  0.0157,  0.0475],\n",
              "                       [ 0.0219,  0.0037, -0.0012,  ..., -0.0028,  0.0053,  0.0035]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0256, -0.0036, -0.0087,  ...,  0.0046,  0.0317, -0.0244],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.18.norm1.bias',\n",
              "               tensor([-0.0124,  0.0162,  0.0110,  ..., -0.0241, -0.0311,  0.0313],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0258, -0.0043,  0.0318,  ..., -0.0487,  0.0125, -0.0254],\n",
              "                       [ 0.0293,  0.0112,  0.0118,  ..., -0.0055, -0.0365, -0.0312],\n",
              "                       [-0.0034,  0.0199, -0.0153,  ...,  0.0229, -0.0023,  0.0036],\n",
              "                       ...,\n",
              "                       [ 0.0253, -0.0137, -0.0029,  ...,  0.0282, -0.0151,  0.0347],\n",
              "                       [ 0.0274, -0.0084, -0.0100,  ...,  0.0001, -0.0201,  0.0587],\n",
              "                       [-0.0082, -0.0132, -0.0153,  ...,  0.0332,  0.0056,  0.0002]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.encoder_attention.q_lin.bias',\n",
              "               tensor([-0.0145, -0.0515,  0.0271,  ...,  0.0040, -0.0263,  0.0171],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0032, -0.0421, -0.0131,  ..., -0.0148, -0.0049,  0.0220],\n",
              "                       [-0.0250,  0.0212,  0.0338,  ...,  0.0139, -0.0297,  0.0132],\n",
              "                       [-0.0218,  0.0265,  0.0331,  ...,  0.0028,  0.0080, -0.0173],\n",
              "                       ...,\n",
              "                       [-0.0251,  0.0379, -0.0091,  ...,  0.0033,  0.0207, -0.0081],\n",
              "                       [-0.0460,  0.0111,  0.0166,  ...,  0.0130, -0.0289,  0.0349],\n",
              "                       [-0.0318, -0.0064, -0.0090,  ..., -0.0022,  0.0028, -0.0264]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.18.encoder_attention.v_lin.weight',\n",
              "               tensor([[ 0.0315, -0.0236,  0.0139,  ...,  0.0231, -0.0129,  0.0361],\n",
              "                       [-0.0018, -0.0540,  0.0018,  ..., -0.0035, -0.0246, -0.0239],\n",
              "                       [ 0.0628, -0.0034, -0.0520,  ..., -0.0041,  0.0322, -0.0359],\n",
              "                       ...,\n",
              "                       [ 0.0108, -0.0263,  0.0247,  ...,  0.0025,  0.0209,  0.0569],\n",
              "                       [-0.0267, -0.0053,  0.0209,  ..., -0.0376,  0.0424,  0.0248],\n",
              "                       [-0.0155,  0.0068,  0.0363,  ...,  0.0242, -0.0166, -0.0396]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.encoder_attention.v_lin.bias',\n",
              "               tensor([ 0.0071,  0.0233, -0.0026,  ...,  0.0234, -0.0157, -0.0215],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0309,  0.0122,  0.0622,  ..., -0.0045, -0.0046, -0.0330],\n",
              "                       [ 0.0099, -0.0594,  0.0122,  ..., -0.0322,  0.0172,  0.0135],\n",
              "                       [ 0.0116,  0.0295, -0.0226,  ...,  0.0366, -0.0313,  0.0181],\n",
              "                       ...,\n",
              "                       [ 0.0380,  0.0010,  0.0006,  ...,  0.0050, -0.0223, -0.0025],\n",
              "                       [-0.0175,  0.0138, -0.0114,  ...,  0.0136,  0.0282,  0.0111],\n",
              "                       [ 0.0606, -0.0126, -0.0050,  ...,  0.0363,  0.0256,  0.0127]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0268, -0.0132, -0.0096,  ...,  0.0037,  0.0301, -0.0189],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.18.norm2.bias',\n",
              "               tensor([ 0.0142,  0.0634,  0.0403,  ...,  0.0381,  0.0586, -0.0329],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.ffn.lin1.weight',\n",
              "               tensor([[ 0.0479,  0.0094,  0.0301,  ..., -0.0023, -0.0188, -0.0196],\n",
              "                       [-0.0260, -0.0175, -0.0333,  ..., -0.0233, -0.0217,  0.0026],\n",
              "                       [-0.0102, -0.0111,  0.0229,  ...,  0.0241, -0.0159,  0.0307],\n",
              "                       ...,\n",
              "                       [-0.0415,  0.0145,  0.0406,  ..., -0.0079, -0.0064,  0.0062],\n",
              "                       [ 0.0010, -0.0144, -0.0163,  ..., -0.0421,  0.0299, -0.0141],\n",
              "                       [-0.0111, -0.0120, -0.0282,  ...,  0.0285,  0.0132, -0.0212]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.ffn.lin1.bias',\n",
              "               tensor([-0.0249,  0.0094, -0.0183,  ..., -0.0169, -0.0143, -0.0213],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.ffn.lin2.weight',\n",
              "               tensor([[-0.0316,  0.0041,  0.0194,  ..., -0.0079,  0.0217, -0.0076],\n",
              "                       [-0.0250, -0.0324,  0.0071,  ..., -0.0199,  0.0489,  0.0478],\n",
              "                       [ 0.0625,  0.0362, -0.0349,  ...,  0.0104, -0.0037, -0.0185],\n",
              "                       ...,\n",
              "                       [ 0.0480, -0.0342, -0.0101,  ..., -0.0067, -0.0320,  0.0625],\n",
              "                       [ 0.0418,  0.0205,  0.0150,  ...,  0.0133, -0.0342,  0.0185],\n",
              "                       [ 0.0343, -0.0335,  0.0006,  ...,  0.0059, -0.0007,  0.0300]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.ffn.lin2.bias',\n",
              "               tensor([ 0.0059,  0.0296, -0.0074,  ..., -0.0054,  0.0288,  0.0100],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.18.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.18.norm3.bias',\n",
              "               tensor([ 0.0518,  0.0154,  0.0036,  ...,  0.0459,  0.0543, -0.0339],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.self_attention.q_lin.weight',\n",
              "               tensor([[-0.0017,  0.0026, -0.0125,  ..., -0.0365, -0.0594, -0.0034],\n",
              "                       [-0.0316,  0.0287, -0.0080,  ...,  0.0192, -0.0224,  0.0093],\n",
              "                       [-0.0095, -0.0100,  0.0310,  ..., -0.0226,  0.0017,  0.0169],\n",
              "                       ...,\n",
              "                       [-0.0135,  0.0047,  0.0391,  ..., -0.0164, -0.0172, -0.0342],\n",
              "                       [ 0.0317, -0.0338, -0.0399,  ..., -0.0228, -0.0242, -0.0124],\n",
              "                       [ 0.0193, -0.0184,  0.0067,  ..., -0.0501, -0.0049,  0.0251]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.self_attention.q_lin.bias',\n",
              "               tensor([-0.0623,  0.0004,  0.0058,  ...,  0.0194,  0.0040, -0.0060],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.self_attention.k_lin.weight',\n",
              "               tensor([[-0.0047, -0.0026, -0.0041,  ...,  0.0016, -0.0386, -0.0438],\n",
              "                       [ 0.0158,  0.0007, -0.0169,  ...,  0.0028, -0.0305,  0.0011],\n",
              "                       [-0.0078, -0.0257,  0.0148,  ..., -0.0035,  0.0238,  0.0310],\n",
              "                       ...,\n",
              "                       [-0.0013, -0.0311, -0.0051,  ..., -0.0085, -0.0152,  0.0008],\n",
              "                       [-0.0299,  0.0366, -0.0130,  ..., -0.0235, -0.0218,  0.0195],\n",
              "                       [-0.0288,  0.0083,  0.0319,  ...,  0.0037,  0.0161, -0.0440]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.19.self_attention.v_lin.weight',\n",
              "               tensor([[-1.4496e-02,  3.4180e-02, -4.7058e-02,  ...,  1.7334e-02,\n",
              "                        -3.9482e-03, -8.5373e-03],\n",
              "                       [-2.7599e-03, -1.0748e-03, -7.0751e-05,  ..., -2.1912e-02,\n",
              "                        -3.4912e-02,  1.3298e-02],\n",
              "                       [ 3.8239e-02,  1.7044e-02, -2.2400e-02,  ...,  3.1769e-02,\n",
              "                         1.3321e-02,  2.4414e-02],\n",
              "                       ...,\n",
              "                       [ 3.6804e-02,  1.0918e-02, -4.0710e-02,  ..., -4.3602e-03,\n",
              "                        -3.4912e-02,  3.2196e-02],\n",
              "                       [ 1.7151e-02, -9.1095e-03, -4.7722e-03,  ..., -1.8768e-02,\n",
              "                        -2.6550e-02,  2.4586e-03],\n",
              "                       [-8.4305e-03, -1.3695e-02, -2.9068e-02,  ...,  3.2745e-02,\n",
              "                        -1.7258e-02, -6.6338e-03]], dtype=torch.float16)),\n",
              "              ('decoder.layers.19.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0062,  0.0085,  0.0039,  ...,  0.0037, -0.0102, -0.0078],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.self_attention.out_lin.weight',\n",
              "               tensor([[-0.0298, -0.0367,  0.0008,  ..., -0.0152,  0.0107,  0.0329],\n",
              "                       [ 0.0347, -0.0061,  0.0021,  ...,  0.0076, -0.0137, -0.0197],\n",
              "                       [ 0.0260, -0.0320,  0.0417,  ...,  0.0357,  0.0023,  0.0310],\n",
              "                       ...,\n",
              "                       [ 0.0029,  0.0022,  0.0179,  ...,  0.0236,  0.0331, -0.0092],\n",
              "                       [ 0.0044,  0.0551,  0.0213,  ..., -0.0399,  0.0323,  0.0352],\n",
              "                       [-0.0335, -0.0206,  0.0288,  ..., -0.0420,  0.0066,  0.0067]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0282, -0.0100, -0.0128,  ...,  0.0047,  0.0325, -0.0208],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.19.norm1.bias',\n",
              "               tensor([-0.0157,  0.0132,  0.0045,  ..., -0.0070, -0.0303,  0.0312],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.encoder_attention.q_lin.weight',\n",
              "               tensor([[ 0.0099, -0.0456,  0.0006,  ...,  0.0527, -0.0155, -0.0144],\n",
              "                       [-0.0181,  0.0063, -0.0012,  ..., -0.0205, -0.0532,  0.0111],\n",
              "                       [ 0.0282, -0.0081, -0.0100,  ..., -0.0237,  0.0224, -0.0145],\n",
              "                       ...,\n",
              "                       [ 0.0149,  0.0401, -0.0173,  ..., -0.0202, -0.0316,  0.0309],\n",
              "                       [-0.0498, -0.0012,  0.0192,  ...,  0.0369,  0.0047,  0.0026],\n",
              "                       [-0.0310, -0.0229,  0.0081,  ..., -0.0002,  0.0071,  0.0049]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.encoder_attention.q_lin.bias',\n",
              "               tensor([-0.0039,  0.0176, -0.0091,  ...,  0.0088,  0.0065, -0.0216],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.encoder_attention.k_lin.weight',\n",
              "               tensor([[-0.0021, -0.0199, -0.0036,  ...,  0.0382,  0.0181, -0.0069],\n",
              "                       [ 0.0067,  0.0076, -0.0055,  ...,  0.0037,  0.0125,  0.0080],\n",
              "                       [-0.0228,  0.0344,  0.0116,  ..., -0.0337, -0.0027, -0.0042],\n",
              "                       ...,\n",
              "                       [ 0.0291, -0.0234,  0.0032,  ..., -0.0010,  0.0026, -0.0295],\n",
              "                       [ 0.0005, -0.0241,  0.0097,  ...,  0.0200, -0.0200,  0.0057],\n",
              "                       [ 0.0142, -0.0111,  0.0082,  ...,  0.0001,  0.0135, -0.0026]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.19.encoder_attention.v_lin.weight',\n",
              "               tensor([[ 0.0454, -0.0312,  0.0225,  ...,  0.0386, -0.0461, -0.0019],\n",
              "                       [-0.0049,  0.0314, -0.0034,  ..., -0.0234,  0.0185,  0.0273],\n",
              "                       [-0.0040, -0.0020,  0.0060,  ...,  0.0061,  0.0098, -0.0028],\n",
              "                       ...,\n",
              "                       [-0.0072,  0.0070,  0.0156,  ...,  0.0358, -0.0443,  0.0090],\n",
              "                       [-0.0213, -0.0016, -0.0031,  ...,  0.0234, -0.0351, -0.0478],\n",
              "                       [ 0.0032, -0.0521,  0.0385,  ...,  0.0336,  0.0449, -0.0309]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.encoder_attention.v_lin.bias',\n",
              "               tensor([-0.0009, -0.0100, -0.0018,  ...,  0.0616,  0.0324, -0.0201],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.encoder_attention.out_lin.weight',\n",
              "               tensor([[-0.0067, -0.0130, -0.0231,  ..., -0.0204, -0.0259,  0.0129],\n",
              "                       [-0.0257,  0.0282, -0.0174,  ...,  0.0102, -0.0003, -0.0061],\n",
              "                       [ 0.0508, -0.0316,  0.0127,  ..., -0.0203,  0.0097,  0.0265],\n",
              "                       ...,\n",
              "                       [ 0.0202,  0.0108, -0.0062,  ...,  0.0249,  0.0245,  0.0310],\n",
              "                       [-0.0381,  0.0044,  0.0280,  ..., -0.0027, -0.0142,  0.0124],\n",
              "                       [-0.0076,  0.0365,  0.0179,  ..., -0.0394, -0.0347,  0.0119]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0225, -0.0074, -0.0022,  ..., -0.0010,  0.0200, -0.0168],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.19.norm2.bias',\n",
              "               tensor([ 0.0309,  0.0686,  0.0526,  ...,  0.0443,  0.0451, -0.0178],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.ffn.lin1.weight',\n",
              "               tensor([[-0.0473, -0.0006,  0.0240,  ..., -0.0378, -0.0040,  0.0052],\n",
              "                       [ 0.0621, -0.0161, -0.0337,  ...,  0.0029, -0.0144, -0.0109],\n",
              "                       [ 0.0245, -0.0172,  0.0358,  ...,  0.0002, -0.0065, -0.0221],\n",
              "                       ...,\n",
              "                       [ 0.0125,  0.0158,  0.0496,  ...,  0.0193, -0.0283,  0.0367],\n",
              "                       [-0.0169,  0.0049, -0.0106,  ...,  0.0118,  0.0433, -0.0378],\n",
              "                       [ 0.0095,  0.0099, -0.0123,  ...,  0.0385, -0.0094, -0.0080]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.ffn.lin1.bias',\n",
              "               tensor([-0.0215, -0.0203, -0.0147,  ..., -0.0117, -0.0102, -0.0144],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.ffn.lin2.weight',\n",
              "               tensor([[-4.3983e-03,  8.0395e-04, -1.8982e-02,  ..., -7.9041e-03,\n",
              "                         6.4659e-03, -4.5105e-02],\n",
              "                       [ 2.0081e-02, -1.4542e-02,  2.5635e-02,  ..., -1.8295e-02,\n",
              "                         1.6312e-02,  7.2746e-03],\n",
              "                       [-5.2673e-02,  4.6448e-02, -3.8330e-02,  ...,  1.1147e-02,\n",
              "                         2.5234e-03,  1.7609e-02],\n",
              "                       ...,\n",
              "                       [ 2.7267e-02,  5.0812e-02, -6.8188e-05,  ..., -2.9984e-02,\n",
              "                         2.6825e-02,  1.1246e-02],\n",
              "                       [-1.1945e-04,  1.1154e-02,  1.7975e-02,  ..., -1.9318e-02,\n",
              "                        -5.7312e-02, -2.9846e-02],\n",
              "                       [-3.7537e-02,  1.7303e-02, -2.8610e-02,  ...,  1.3977e-02,\n",
              "                         3.0869e-02, -1.1452e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.19.ffn.lin2.bias',\n",
              "               tensor([-0.0249,  0.0235, -0.0203,  ...,  0.0104,  0.0326,  0.0042],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.19.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.19.norm3.bias',\n",
              "               tensor([ 0.0406,  0.0457,  0.0132,  ...,  0.0319,  0.0424, -0.0340],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.self_attention.q_lin.weight',\n",
              "               tensor([[ 0.0101, -0.0249, -0.0053,  ..., -0.0320, -0.0018, -0.0070],\n",
              "                       [ 0.0203, -0.0060,  0.0030,  ..., -0.0216,  0.0316,  0.0102],\n",
              "                       [ 0.0178,  0.0168, -0.0367,  ...,  0.0035,  0.0135,  0.0098],\n",
              "                       ...,\n",
              "                       [-0.0082, -0.0020, -0.0029,  ..., -0.0193,  0.0149,  0.0287],\n",
              "                       [-0.0024,  0.0006, -0.0006,  ...,  0.0157,  0.0133, -0.0387],\n",
              "                       [-0.0172,  0.0041,  0.0054,  ...,  0.0162,  0.0079, -0.0227]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0247, -0.0197,  0.0145,  ...,  0.0186, -0.0263, -0.0072],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0192,  0.0068,  0.0339,  ...,  0.0090, -0.0121,  0.0156],\n",
              "                       [ 0.0014,  0.0340, -0.0042,  ..., -0.0173,  0.0167,  0.0219],\n",
              "                       [-0.0194, -0.0106, -0.0010,  ...,  0.0262, -0.0068,  0.0018],\n",
              "                       ...,\n",
              "                       [-0.0103,  0.0360, -0.0271,  ..., -0.0238,  0.0353,  0.0351],\n",
              "                       [ 0.0124,  0.0144,  0.0026,  ...,  0.0114, -0.0074, -0.0143],\n",
              "                       [-0.0199, -0.0021, -0.0347,  ...,  0.0203,  0.0248, -0.0068]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.20.self_attention.v_lin.weight',\n",
              "               tensor([[ 0.0392,  0.0036, -0.0214,  ...,  0.0007, -0.0300,  0.0324],\n",
              "                       [ 0.0095,  0.0111, -0.0044,  ..., -0.0394, -0.0214,  0.0408],\n",
              "                       [ 0.0285, -0.0146, -0.0188,  ..., -0.0085, -0.0150, -0.0156],\n",
              "                       ...,\n",
              "                       [-0.0231, -0.0276,  0.0486,  ..., -0.0274, -0.0035,  0.0240],\n",
              "                       [-0.0168, -0.0306,  0.0049,  ..., -0.0120, -0.0432,  0.0197],\n",
              "                       [ 0.0374, -0.0138, -0.0091,  ...,  0.0507, -0.0162,  0.0300]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0074,  0.0048, -0.0035,  ..., -0.0057, -0.0005, -0.0007],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.self_attention.out_lin.weight',\n",
              "               tensor([[ 0.0167, -0.0315, -0.0109,  ..., -0.0406,  0.0088,  0.0170],\n",
              "                       [-0.0113, -0.0303,  0.0014,  ..., -0.0173,  0.0187, -0.0313],\n",
              "                       [-0.0422,  0.0280,  0.0282,  ...,  0.0171,  0.0265,  0.0124],\n",
              "                       ...,\n",
              "                       [ 0.0285, -0.0319, -0.0122,  ..., -0.0048,  0.0066,  0.0530],\n",
              "                       [-0.0320,  0.0320, -0.0187,  ..., -0.0119, -0.0070, -0.0173],\n",
              "                       [ 0.0546, -0.0020, -0.0298,  ..., -0.0112,  0.0183, -0.0005]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0311, -0.0170, -0.0080,  ...,  0.0044,  0.0327, -0.0228],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.20.norm1.bias',\n",
              "               tensor([-0.0069,  0.0119,  0.0128,  ..., -0.0091, -0.0191,  0.0314],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0220, -0.0023, -0.0146,  ...,  0.0255,  0.0069, -0.0029],\n",
              "                       [-0.0097,  0.0284, -0.0292,  ...,  0.0088, -0.0306, -0.0101],\n",
              "                       [ 0.0397, -0.0133, -0.0140,  ...,  0.0356,  0.0225, -0.0161],\n",
              "                       ...,\n",
              "                       [ 0.0328, -0.0328, -0.0182,  ..., -0.0213, -0.0380, -0.0478],\n",
              "                       [ 0.0347, -0.0014,  0.0566,  ...,  0.0005, -0.0208, -0.0005],\n",
              "                       [-0.0173, -0.0092,  0.0208,  ...,  0.0320,  0.0340, -0.0087]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.encoder_attention.q_lin.bias',\n",
              "               tensor([-0.0434,  0.0026,  0.0215,  ..., -0.0022,  0.0205, -0.0003],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0147, -0.0018, -0.0603,  ..., -0.0059, -0.0037,  0.0333],\n",
              "                       [-0.0164,  0.0170,  0.0140,  ..., -0.0116, -0.0035, -0.0011],\n",
              "                       [-0.0135,  0.0020, -0.0179,  ...,  0.0063,  0.0094, -0.0381],\n",
              "                       ...,\n",
              "                       [-0.0356,  0.0013, -0.0272,  ...,  0.0405, -0.0051,  0.0114],\n",
              "                       [ 0.0042, -0.0136,  0.0261,  ..., -0.0133,  0.0074,  0.0110],\n",
              "                       [-0.0303, -0.0121,  0.0048,  ...,  0.0093, -0.0041, -0.0002]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.20.encoder_attention.v_lin.weight',\n",
              "               tensor([[ 0.0354, -0.0284, -0.0063,  ..., -0.0368, -0.0207, -0.0038],\n",
              "                       [ 0.0014,  0.0031,  0.0346,  ...,  0.0281,  0.0339,  0.0081],\n",
              "                       [-0.0488,  0.0281,  0.0037,  ..., -0.0086,  0.0158, -0.0110],\n",
              "                       ...,\n",
              "                       [-0.0371,  0.0322, -0.0098,  ...,  0.0094, -0.0038,  0.0501],\n",
              "                       [ 0.0026, -0.0241,  0.0021,  ..., -0.0334,  0.0039,  0.0298],\n",
              "                       [ 0.0547, -0.0069,  0.0256,  ...,  0.0137, -0.0302, -0.0128]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.encoder_attention.v_lin.bias',\n",
              "               tensor([ 0.0135,  0.0036, -0.0063,  ..., -0.0069,  0.0153,  0.0012],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.encoder_attention.out_lin.weight',\n",
              "               tensor([[-0.0100, -0.0129, -0.0426,  ...,  0.0019,  0.0080,  0.0167],\n",
              "                       [-0.0179, -0.0285, -0.0265,  ..., -0.0055, -0.0027, -0.0036],\n",
              "                       [ 0.0037,  0.0065,  0.0143,  ..., -0.0003,  0.0026,  0.0016],\n",
              "                       ...,\n",
              "                       [-0.0200, -0.0240, -0.0031,  ...,  0.0141, -0.0453, -0.0066],\n",
              "                       [-0.0133, -0.0016, -0.0188,  ...,  0.0118,  0.0527, -0.0313],\n",
              "                       [ 0.0150,  0.0351, -0.0133,  ...,  0.0509, -0.0122,  0.0147]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0254, -0.0172, -0.0154,  ..., -0.0030,  0.0255, -0.0233],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.20.norm2.bias',\n",
              "               tensor([ 0.0347,  0.0648,  0.0343,  ...,  0.0295,  0.0414, -0.0273],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.ffn.lin1.weight',\n",
              "               tensor([[-1.2703e-02,  9.9258e-03,  1.8829e-02,  ..., -1.8127e-02,\n",
              "                         1.5251e-02,  1.1627e-02],\n",
              "                       [-3.3142e-02,  1.1711e-02, -7.9498e-03,  ...,  4.7638e-02,\n",
              "                         2.0203e-02,  6.0730e-03],\n",
              "                       [-1.5373e-02,  3.7354e-02, -5.8289e-02,  ..., -3.2227e-02,\n",
              "                        -4.2603e-02, -1.3037e-03],\n",
              "                       ...,\n",
              "                       [ 1.3062e-02, -6.0225e-04, -1.6525e-02,  ...,  7.5281e-05,\n",
              "                         1.1429e-02,  1.1353e-02],\n",
              "                       [ 1.9627e-03,  2.0218e-02, -1.7700e-03,  ...,  2.8885e-02,\n",
              "                        -8.0338e-03, -1.2978e-02],\n",
              "                       [-2.4963e-02,  6.1371e-02, -1.5602e-02,  ...,  6.9122e-03,\n",
              "                         1.0529e-02,  2.8000e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.20.ffn.lin1.bias',\n",
              "               tensor([-1.5747e-02, -2.4021e-05, -2.1072e-02,  ..., -2.1744e-02,\n",
              "                       -5.3310e-04, -1.2955e-02], dtype=torch.float16)),\n",
              "              ('decoder.layers.20.ffn.lin2.weight',\n",
              "               tensor([[ 0.0237, -0.0126, -0.0216,  ...,  0.0093,  0.0256,  0.0087],\n",
              "                       [-0.0049,  0.0385,  0.0095,  ..., -0.0057, -0.0048, -0.0260],\n",
              "                       [-0.0168,  0.0510, -0.0350,  ...,  0.0113, -0.0229,  0.0334],\n",
              "                       ...,\n",
              "                       [ 0.0112, -0.0136,  0.0359,  ..., -0.0080, -0.0267, -0.0106],\n",
              "                       [ 0.0355,  0.0020,  0.0359,  ..., -0.0461,  0.0350, -0.0110],\n",
              "                       [ 0.0018,  0.0089, -0.0065,  ..., -0.0233,  0.0038,  0.0113]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.20.ffn.lin2.bias',\n",
              "               tensor([-1.9928e-02,  8.0032e-03, -4.2839e-03,  ...,  1.0292e-02,\n",
              "                        3.1982e-02, -2.5153e-05], dtype=torch.float16)),\n",
              "              ('decoder.layers.20.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.20.norm3.bias',\n",
              "               tensor([ 0.0410,  0.0486,  0.0055,  ...,  0.0319,  0.0446, -0.0234],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.self_attention.q_lin.weight',\n",
              "               tensor([[ 0.0356, -0.0089, -0.0335,  ...,  0.0203, -0.0111, -0.0372],\n",
              "                       [ 0.0140, -0.0130,  0.0150,  ...,  0.0142, -0.0068,  0.0111],\n",
              "                       [-0.0263,  0.0155, -0.0312,  ...,  0.0118, -0.0101,  0.0314],\n",
              "                       ...,\n",
              "                       [ 0.0045,  0.0042, -0.0083,  ..., -0.0173,  0.0019, -0.0177],\n",
              "                       [ 0.0031, -0.0039, -0.0066,  ...,  0.0123, -0.0003, -0.0124],\n",
              "                       [ 0.0250,  0.0005, -0.0115,  ..., -0.0151,  0.0069, -0.0481]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0265,  0.0022,  0.0091,  ...,  0.0358,  0.0327, -0.0034],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0026,  0.0101, -0.0120,  ...,  0.0168, -0.0476,  0.0107],\n",
              "                       [-0.0265,  0.0146, -0.0193,  ...,  0.0389, -0.0201,  0.0346],\n",
              "                       [ 0.0032, -0.0102,  0.0355,  ...,  0.0201,  0.0097,  0.0263],\n",
              "                       ...,\n",
              "                       [ 0.0147,  0.0242,  0.0218,  ..., -0.0060,  0.0117, -0.0039],\n",
              "                       [-0.0103,  0.0530, -0.0207,  ...,  0.0110, -0.0158,  0.0099],\n",
              "                       [ 0.0114,  0.0162,  0.0352,  ..., -0.0046, -0.0193, -0.0016]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.21.self_attention.v_lin.weight',\n",
              "               tensor([[ 0.0245,  0.0146,  0.0128,  ...,  0.0123, -0.0140, -0.0274],\n",
              "                       [ 0.0112,  0.0314,  0.0025,  ..., -0.0194, -0.0202,  0.0360],\n",
              "                       [ 0.0032,  0.0111,  0.0394,  ...,  0.0201, -0.0181,  0.0113],\n",
              "                       ...,\n",
              "                       [ 0.0188, -0.0398, -0.0216,  ..., -0.0149,  0.0207,  0.0138],\n",
              "                       [ 0.0667, -0.0201, -0.0151,  ...,  0.0540, -0.0031, -0.0099],\n",
              "                       [ 0.0172,  0.0146,  0.0231,  ..., -0.0309,  0.0362,  0.0007]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0079, -0.0069,  0.0023,  ...,  0.0034, -0.0021, -0.0001],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.self_attention.out_lin.weight',\n",
              "               tensor([[-0.0053, -0.0176,  0.0167,  ...,  0.0078,  0.0399,  0.0133],\n",
              "                       [-0.0311, -0.0274,  0.0104,  ...,  0.0023,  0.0268, -0.0025],\n",
              "                       [ 0.0094,  0.0325, -0.0005,  ...,  0.0524,  0.0009, -0.0084],\n",
              "                       ...,\n",
              "                       [-0.0219,  0.0088, -0.0181,  ...,  0.0182,  0.0083, -0.0523],\n",
              "                       [-0.0190,  0.0116, -0.0207,  ..., -0.0041, -0.0165,  0.0208],\n",
              "                       [-0.0072,  0.0434,  0.0360,  ...,  0.0046, -0.0095, -0.0178]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0251, -0.0102, -0.0143,  ...,  0.0068,  0.0300, -0.0271],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.21.norm1.bias',\n",
              "               tensor([-0.0148,  0.0179,  0.0199,  ..., -0.0021, -0.0291,  0.0338],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0355, -0.0031, -0.0263,  ...,  0.0135,  0.0118,  0.0202],\n",
              "                       [-0.0328,  0.0184, -0.0180,  ...,  0.0090, -0.0068,  0.0015],\n",
              "                       [-0.0199, -0.0101,  0.0302,  ..., -0.0274, -0.0068,  0.0181],\n",
              "                       ...,\n",
              "                       [ 0.0066, -0.0005,  0.0128,  ...,  0.0008,  0.0608,  0.0189],\n",
              "                       [-0.0291,  0.0048, -0.0102,  ...,  0.0433, -0.0392,  0.0042],\n",
              "                       [ 0.0316,  0.0146, -0.0059,  ...,  0.0031,  0.0611, -0.0050]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0256,  0.0007,  0.0169,  ...,  0.0094, -0.0062,  0.0308],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.encoder_attention.k_lin.weight',\n",
              "               tensor([[-1.8482e-03,  1.0757e-02,  6.7759e-04,  ..., -3.7956e-03,\n",
              "                        -3.9368e-02,  1.3702e-02],\n",
              "                       [ 1.9913e-02, -2.8172e-03, -1.5450e-02,  ..., -1.3145e-02,\n",
              "                         1.4862e-02,  3.9673e-03],\n",
              "                       [-3.0319e-02, -1.6251e-03,  7.6752e-03,  ...,  6.1264e-03,\n",
              "                        -1.4397e-02,  3.3905e-02],\n",
              "                       ...,\n",
              "                       [-2.5177e-02,  5.4550e-03,  8.0395e-04,  ..., -2.2186e-02,\n",
              "                        -3.3752e-02, -1.1490e-02],\n",
              "                       [-2.1454e-02,  5.2979e-02,  1.7395e-03,  ...,  4.9686e-04,\n",
              "                        -2.0920e-02, -3.2654e-02],\n",
              "                       [-5.0934e-02,  1.2115e-02,  2.0859e-02,  ...,  7.4863e-05,\n",
              "                         1.7834e-03, -1.2230e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.21.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.21.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0175, -0.0049, -0.0046,  ...,  0.0042,  0.0357,  0.0345],\n",
              "                       [-0.0351,  0.0341, -0.0121,  ..., -0.0060, -0.0191, -0.0087],\n",
              "                       [ 0.0047, -0.0506,  0.0251,  ...,  0.0069,  0.0270,  0.0312],\n",
              "                       ...,\n",
              "                       [ 0.0013, -0.0002,  0.0021,  ..., -0.0509, -0.0026,  0.0215],\n",
              "                       [-0.0007, -0.0259,  0.0056,  ..., -0.0449,  0.0342,  0.0233],\n",
              "                       [ 0.0385,  0.0023,  0.0222,  ..., -0.0020, -0.0079, -0.0180]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.encoder_attention.v_lin.bias',\n",
              "               tensor([ 0.0214,  0.0160, -0.0102,  ..., -0.0110,  0.0233,  0.0105],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.encoder_attention.out_lin.weight',\n",
              "               tensor([[ 0.0356, -0.0315,  0.0177,  ...,  0.0271, -0.0283,  0.0085],\n",
              "                       [ 0.0063,  0.0177,  0.0005,  ..., -0.0250, -0.0425,  0.0137],\n",
              "                       [ 0.0123,  0.0090, -0.0136,  ...,  0.0148,  0.0096,  0.0406],\n",
              "                       ...,\n",
              "                       [ 0.0493,  0.0315,  0.0098,  ...,  0.0174, -0.0127, -0.0069],\n",
              "                       [ 0.0019, -0.0119,  0.0299,  ...,  0.0129,  0.0164,  0.0166],\n",
              "                       [ 0.0339, -0.0536,  0.0068,  ..., -0.0312,  0.0214,  0.0302]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0229, -0.0151, -0.0180,  ..., -0.0012,  0.0297, -0.0246],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.21.norm2.bias',\n",
              "               tensor([ 0.0391,  0.0616,  0.0393,  ...,  0.0233,  0.0370, -0.0147],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.ffn.lin1.weight',\n",
              "               tensor([[-0.0197,  0.0413, -0.0010,  ..., -0.0002,  0.0142,  0.0068],\n",
              "                       [ 0.0212,  0.0184, -0.0187,  ..., -0.0447,  0.0103, -0.0114],\n",
              "                       [-0.0524, -0.0341, -0.0435,  ..., -0.0281,  0.0313,  0.0287],\n",
              "                       ...,\n",
              "                       [ 0.0143,  0.0004,  0.0151,  ..., -0.0319, -0.0050, -0.0192],\n",
              "                       [-0.0022, -0.0077,  0.0248,  ...,  0.0131, -0.0502,  0.0171],\n",
              "                       [ 0.0106, -0.0058,  0.0063,  ...,  0.0081, -0.0133, -0.0175]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.ffn.lin1.bias',\n",
              "               tensor([-0.0053, -0.0127, -0.0161,  ..., -0.0129,  0.0029, -0.0120],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.ffn.lin2.weight',\n",
              "               tensor([[-0.0057,  0.0034,  0.0275,  ..., -0.0024,  0.0089, -0.0043],\n",
              "                       [-0.0042,  0.0257,  0.0276,  ...,  0.0228,  0.0166,  0.0578],\n",
              "                       [-0.0060,  0.0473,  0.0356,  ..., -0.0318,  0.0016,  0.0147],\n",
              "                       ...,\n",
              "                       [-0.0031,  0.0265,  0.0233,  ...,  0.0204, -0.0063, -0.0131],\n",
              "                       [-0.0122, -0.0057, -0.0119,  ..., -0.0157,  0.0332,  0.0316],\n",
              "                       [-0.0315,  0.0269, -0.0635,  ..., -0.0103,  0.0327,  0.0043]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.ffn.lin2.bias',\n",
              "               tensor([-0.0217,  0.0248,  0.0010,  ...,  0.0044,  0.0351, -0.0044],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.21.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.21.norm3.bias',\n",
              "               tensor([ 0.0564,  0.0456,  0.0064,  ...,  0.0244,  0.0188, -0.0369],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.self_attention.q_lin.weight',\n",
              "               tensor([[-0.0199, -0.0003, -0.0157,  ..., -0.0010,  0.0068, -0.0331],\n",
              "                       [-0.0430, -0.0580, -0.0040,  ..., -0.0399, -0.0019, -0.0439],\n",
              "                       [ 0.0174, -0.0043, -0.0274,  ..., -0.0075,  0.0050,  0.0455],\n",
              "                       ...,\n",
              "                       [-0.0165,  0.0403, -0.0065,  ..., -0.0085, -0.0114, -0.0031],\n",
              "                       [-0.0035, -0.0036,  0.0126,  ..., -0.0130, -0.0083,  0.0240],\n",
              "                       [ 0.0175, -0.0237,  0.0165,  ..., -0.0078, -0.0008, -0.0501]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.self_attention.q_lin.bias',\n",
              "               tensor([ 0.0195, -0.0150,  0.0032,  ...,  0.0266, -0.0298,  0.0378],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.self_attention.k_lin.weight',\n",
              "               tensor([[-0.0079,  0.0109, -0.0209,  ...,  0.0005,  0.0318, -0.0219],\n",
              "                       [-0.0290, -0.0252, -0.0048,  ..., -0.0174, -0.0302, -0.0396],\n",
              "                       [-0.0222,  0.0199, -0.0244,  ..., -0.0116, -0.0230, -0.0149],\n",
              "                       ...,\n",
              "                       [ 0.0040, -0.0070, -0.0345,  ..., -0.0087, -0.0005, -0.0171],\n",
              "                       [ 0.0166,  0.0108,  0.0175,  ..., -0.0252,  0.0017, -0.0231],\n",
              "                       [-0.0387,  0.0174, -0.0220,  ..., -0.0113,  0.0224, -0.0185]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.22.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0316, -0.0014, -0.0226,  ..., -0.0121, -0.0159, -0.0070],\n",
              "                       [ 0.0334, -0.0349, -0.0505,  ...,  0.0228,  0.0356,  0.0015],\n",
              "                       [-0.0113,  0.0118, -0.0170,  ..., -0.0029, -0.0096, -0.0367],\n",
              "                       ...,\n",
              "                       [-0.0136,  0.0257, -0.0187,  ...,  0.0334, -0.0242, -0.0050],\n",
              "                       [ 0.0485, -0.0046, -0.0395,  ..., -0.0156, -0.0048,  0.0233],\n",
              "                       [ 0.0054,  0.0078, -0.0595,  ..., -0.0151,  0.0185,  0.0220]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.self_attention.v_lin.bias',\n",
              "               tensor([ 0.0036, -0.0038,  0.0002,  ...,  0.0088, -0.0009, -0.0042],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.self_attention.out_lin.weight',\n",
              "               tensor([[-0.0160,  0.0286, -0.0142,  ..., -0.0227, -0.0157,  0.0278],\n",
              "                       [-0.0303,  0.0251,  0.0219,  ...,  0.0135, -0.0329, -0.0122],\n",
              "                       [ 0.0124, -0.0077, -0.0027,  ...,  0.0233, -0.0232, -0.0264],\n",
              "                       ...,\n",
              "                       [ 0.0006,  0.0010, -0.0352,  ..., -0.0193, -0.0404,  0.0187],\n",
              "                       [ 0.0260,  0.0236, -0.0323,  ..., -0.0364, -0.0464, -0.0428],\n",
              "                       [-0.0176, -0.0066, -0.0287,  ...,  0.0065, -0.0235,  0.0312]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0268, -0.0153, -0.0098,  ..., -0.0051,  0.0337, -0.0305],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.22.norm1.bias',\n",
              "               tensor([-0.0192,  0.0351,  0.0198,  ..., -0.0021, -0.0626,  0.0314],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.encoder_attention.q_lin.weight',\n",
              "               tensor([[ 0.0301, -0.0154,  0.0070,  ...,  0.0133,  0.0140,  0.0119],\n",
              "                       [-0.0069,  0.0055, -0.0181,  ...,  0.0093, -0.0169,  0.0240],\n",
              "                       [ 0.0343,  0.0160,  0.0039,  ...,  0.0292, -0.0016, -0.0413],\n",
              "                       ...,\n",
              "                       [ 0.0325, -0.0071,  0.0047,  ..., -0.0140, -0.0249,  0.0105],\n",
              "                       [-0.0032,  0.0122, -0.0260,  ..., -0.0045, -0.0020,  0.0050],\n",
              "                       [-0.0218, -0.0187, -0.0120,  ...,  0.0146,  0.0328,  0.0120]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.encoder_attention.q_lin.bias',\n",
              "               tensor([ 0.0006, -0.0024, -0.0048,  ..., -0.0160,  0.0181,  0.0043],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.encoder_attention.k_lin.weight',\n",
              "               tensor([[-2.9068e-02, -8.3008e-03, -3.8528e-03,  ...,  3.5706e-03,\n",
              "                         1.2047e-02, -1.7410e-02],\n",
              "                       [ 7.7515e-03,  4.4159e-02,  2.0081e-02,  ...,  4.9591e-03,\n",
              "                        -9.5596e-03, -8.5983e-03],\n",
              "                       [-1.3840e-02, -9.9030e-03,  1.1520e-02,  ..., -3.5801e-03,\n",
              "                         1.2787e-02,  4.8828e-02],\n",
              "                       ...,\n",
              "                       [-9.3765e-03,  1.2024e-02,  3.7975e-03,  ...,  5.8594e-03,\n",
              "                         1.6342e-02,  2.7405e-02],\n",
              "                       [ 7.4196e-03, -1.2665e-02,  3.1888e-05,  ...,  1.3924e-02,\n",
              "                        -2.7752e-03, -9.5215e-03],\n",
              "                       [-1.3336e-02,  1.6098e-02, -4.9095e-03,  ...,  4.8256e-03,\n",
              "                         3.8986e-03,  1.4542e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.22.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.22.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0396, -0.0143,  0.0023,  ...,  0.0475,  0.0233,  0.0322],\n",
              "                       [-0.0051,  0.0087,  0.0141,  ...,  0.0056, -0.0016,  0.0271],\n",
              "                       [ 0.0255,  0.0297, -0.0160,  ...,  0.0040, -0.0426,  0.0348],\n",
              "                       ...,\n",
              "                       [ 0.0090,  0.0278,  0.0335,  ..., -0.0339, -0.0128, -0.0323],\n",
              "                       [-0.0056,  0.0123,  0.0047,  ...,  0.0302,  0.0163, -0.0024],\n",
              "                       [ 0.0258,  0.0197,  0.0080,  ..., -0.0236, -0.0231,  0.0142]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.encoder_attention.v_lin.bias',\n",
              "               tensor([ 0.0059, -0.0061,  0.0105,  ...,  0.0113,  0.0006, -0.0272],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.encoder_attention.out_lin.weight',\n",
              "               tensor([[-0.0413,  0.0061,  0.0079,  ..., -0.0270, -0.0168,  0.0137],\n",
              "                       [-0.0029,  0.0078,  0.0013,  ..., -0.0367,  0.0039,  0.0431],\n",
              "                       [-0.0141,  0.0215,  0.0135,  ...,  0.0059, -0.0053, -0.0032],\n",
              "                       ...,\n",
              "                       [ 0.0145, -0.0160, -0.0132,  ...,  0.0401, -0.0004, -0.0399],\n",
              "                       [-0.0137, -0.0146, -0.0019,  ...,  0.0059,  0.0060,  0.0249],\n",
              "                       [ 0.0332,  0.0320,  0.0269,  ...,  0.0148, -0.0048,  0.0008]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0159, -0.0182, -0.0127,  ..., -0.0179,  0.0246, -0.0153],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.22.norm2.bias',\n",
              "               tensor([0.0260, 0.0611, 0.0096,  ..., 0.0414, 0.0265, 0.0203],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.ffn.lin1.weight',\n",
              "               tensor([[-0.0404,  0.0097, -0.0245,  ...,  0.0038,  0.0320, -0.0159],\n",
              "                       [-0.0202,  0.0259,  0.0004,  ...,  0.0120,  0.0066,  0.0294],\n",
              "                       [-0.0260,  0.0391, -0.0153,  ...,  0.0234,  0.0002, -0.0001],\n",
              "                       ...,\n",
              "                       [ 0.0118,  0.0161, -0.0078,  ...,  0.0341, -0.0042, -0.0048],\n",
              "                       [-0.0322,  0.0216, -0.0050,  ...,  0.0080, -0.0162, -0.0073],\n",
              "                       [-0.0397, -0.0013, -0.0041,  ...,  0.0322,  0.0153, -0.0028]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.ffn.lin1.bias',\n",
              "               tensor([-0.0099, -0.0031,  0.0013,  ..., -0.0129, -0.0145, -0.0209],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.ffn.lin2.weight',\n",
              "               tensor([[-0.0067, -0.0024,  0.0374,  ...,  0.0285,  0.0078, -0.0031],\n",
              "                       [ 0.0242,  0.0467, -0.0307,  ..., -0.0392,  0.0160,  0.0036],\n",
              "                       [ 0.0040,  0.0267, -0.0108,  ..., -0.0180, -0.0055,  0.0101],\n",
              "                       ...,\n",
              "                       [ 0.0177,  0.0189, -0.0401,  ..., -0.0376, -0.0265, -0.0337],\n",
              "                       [-0.0312, -0.0291, -0.0069,  ...,  0.0038,  0.0258, -0.0036],\n",
              "                       [-0.0049,  0.0145,  0.0009,  ..., -0.0209, -0.0108, -0.0094]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.ffn.lin2.bias',\n",
              "               tensor([-0.0343,  0.0327,  0.0067,  ...,  0.0050,  0.0277,  0.0019],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.22.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.22.norm3.bias',\n",
              "               tensor([ 0.0057,  0.0706, -0.0112,  ..., -0.0012, -0.0265,  0.0168],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.self_attention.q_lin.weight',\n",
              "               tensor([[-0.0172, -0.0158,  0.0063,  ..., -0.0103,  0.0107, -0.0178],\n",
              "                       [-0.0069, -0.0111,  0.0006,  ..., -0.0053, -0.0145,  0.0052],\n",
              "                       [ 0.0188,  0.0113, -0.0210,  ..., -0.0053, -0.0247,  0.0169],\n",
              "                       ...,\n",
              "                       [-0.0104,  0.0323,  0.0275,  ..., -0.0264, -0.0240,  0.0049],\n",
              "                       [-0.0016,  0.0098, -0.0126,  ..., -0.0284,  0.0192,  0.0090],\n",
              "                       [ 0.0018,  0.0023,  0.0078,  ..., -0.0016, -0.0518,  0.0062]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.self_attention.q_lin.bias',\n",
              "               tensor([ 5.4550e-04, -7.5698e-05, -8.3160e-03,  ..., -2.0050e-02,\n",
              "                        1.8860e-02,  5.9366e-04], dtype=torch.float16)),\n",
              "              ('decoder.layers.23.self_attention.k_lin.weight',\n",
              "               tensor([[ 0.0156, -0.0044, -0.0035,  ...,  0.0259,  0.0358, -0.0185],\n",
              "                       [-0.0196, -0.0145, -0.0053,  ..., -0.0068,  0.0002,  0.0122],\n",
              "                       [ 0.0075, -0.0103,  0.0017,  ...,  0.0156, -0.0159,  0.0003],\n",
              "                       ...,\n",
              "                       [ 0.0046, -0.0211, -0.0126,  ...,  0.0027,  0.0130,  0.0179],\n",
              "                       [ 0.0038, -0.0134, -0.0113,  ...,  0.0036, -0.0356,  0.0205],\n",
              "                       [ 0.0201,  0.0188,  0.0134,  ..., -0.0159,  0.0144,  0.0080]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.self_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.23.self_attention.v_lin.weight',\n",
              "               tensor([[-0.0046,  0.0166, -0.0326,  ..., -0.0181, -0.0146, -0.0074],\n",
              "                       [-0.0274,  0.0082,  0.0010,  ...,  0.0125, -0.0084, -0.0039],\n",
              "                       [ 0.0328, -0.0455, -0.0023,  ..., -0.0248,  0.0192, -0.0107],\n",
              "                       ...,\n",
              "                       [-0.0212, -0.0260, -0.0168,  ...,  0.0159, -0.0258,  0.0110],\n",
              "                       [-0.0057,  0.0120, -0.0049,  ..., -0.0378,  0.0187, -0.0162],\n",
              "                       [-0.0168, -0.0079,  0.0388,  ..., -0.0214, -0.0436, -0.0002]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.self_attention.v_lin.bias',\n",
              "               tensor([-0.0059,  0.0013, -0.0021,  ..., -0.0035, -0.0176, -0.0017],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.self_attention.out_lin.weight',\n",
              "               tensor([[ 0.0284, -0.0135,  0.0169,  ..., -0.0065,  0.0036,  0.0060],\n",
              "                       [-0.0386,  0.0114, -0.0249,  ..., -0.0282, -0.0514,  0.0221],\n",
              "                       [-0.0045,  0.0092,  0.0452,  ..., -0.0219,  0.0019,  0.0061],\n",
              "                       ...,\n",
              "                       [-0.0235,  0.0106,  0.0247,  ..., -0.0112, -0.0165, -0.0058],\n",
              "                       [ 0.0325, -0.0145,  0.0104,  ..., -0.0395, -0.0052,  0.0117],\n",
              "                       [-0.0254, -0.0244,  0.0067,  ...,  0.0332, -0.0142, -0.0005]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.self_attention.out_lin.bias',\n",
              "               tensor([ 0.0140, -0.0067,  0.0050,  ..., -0.0079,  0.0130, -0.0145],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.norm1.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.23.norm1.bias',\n",
              "               tensor([-0.0141,  0.0330,  0.0285,  ..., -0.0054, -0.0641,  0.0512],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.encoder_attention.q_lin.weight',\n",
              "               tensor([[-0.0088,  0.0228, -0.0071,  ..., -0.0024,  0.0040, -0.0230],\n",
              "                       [-0.0105, -0.0070,  0.0133,  ...,  0.0029,  0.0287, -0.0285],\n",
              "                       [-0.0085,  0.0115, -0.0081,  ...,  0.0203,  0.0350, -0.0024],\n",
              "                       ...,\n",
              "                       [-0.0346,  0.0080,  0.0199,  ..., -0.0266,  0.0134,  0.0017],\n",
              "                       [-0.0088, -0.0025, -0.0015,  ..., -0.0055,  0.0180, -0.0344],\n",
              "                       [-0.0357,  0.0005, -0.0075,  ..., -0.0293, -0.0419, -0.0401]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.encoder_attention.q_lin.bias',\n",
              "               tensor([-0.0005,  0.0202,  0.0019,  ..., -0.0015,  0.0060, -0.0308],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.encoder_attention.k_lin.weight',\n",
              "               tensor([[ 0.0141,  0.0057,  0.0102,  ..., -0.0178,  0.0020,  0.0049],\n",
              "                       [ 0.0006, -0.0492, -0.0091,  ...,  0.0024,  0.0092,  0.0019],\n",
              "                       [-0.0309,  0.0101, -0.0501,  ..., -0.0184, -0.0219, -0.0242],\n",
              "                       ...,\n",
              "                       [ 0.0272, -0.0144, -0.0110,  ...,  0.0081,  0.0154, -0.0024],\n",
              "                       [ 0.0010, -0.0170,  0.0090,  ...,  0.0452, -0.0384, -0.0317],\n",
              "                       [-0.0119,  0.0154,  0.0237,  ...,  0.0029, -0.0245,  0.0063]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.encoder_attention.k_lin.bias',\n",
              "               tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
              "              ('decoder.layers.23.encoder_attention.v_lin.weight',\n",
              "               tensor([[-0.0579, -0.0173,  0.0322,  ..., -0.0222, -0.0189,  0.0129],\n",
              "                       [ 0.0049, -0.0291,  0.0363,  ..., -0.0340,  0.0189, -0.0218],\n",
              "                       [-0.0704, -0.0067, -0.0019,  ...,  0.0334,  0.0175,  0.0603],\n",
              "                       ...,\n",
              "                       [-0.0073,  0.0073,  0.0137,  ...,  0.0080, -0.0263,  0.0226],\n",
              "                       [-0.0177,  0.0052,  0.0240,  ..., -0.0156, -0.0419,  0.0430],\n",
              "                       [ 0.0046,  0.0211,  0.0294,  ..., -0.0225, -0.0014, -0.0152]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.encoder_attention.v_lin.bias',\n",
              "               tensor([ 0.0075,  0.0190,  0.0190,  ...,  0.0278, -0.0147, -0.0401],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.encoder_attention.out_lin.weight',\n",
              "               tensor([[-0.0221, -0.0537,  0.0168,  ...,  0.0106, -0.0056,  0.0007],\n",
              "                       [ 0.0634, -0.0262,  0.0201,  ..., -0.0167,  0.0274,  0.0194],\n",
              "                       [ 0.0173, -0.0089,  0.0089,  ...,  0.0254,  0.0172,  0.0069],\n",
              "                       ...,\n",
              "                       [ 0.0082, -0.0003, -0.0306,  ...,  0.0185, -0.0095,  0.0019],\n",
              "                       [ 0.0324,  0.0213,  0.0373,  ..., -0.0354, -0.0410, -0.0024],\n",
              "                       [ 0.0305,  0.0087, -0.0086,  ..., -0.0213,  0.0497, -0.0076]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.encoder_attention.out_lin.bias',\n",
              "               tensor([ 0.0020,  0.0027, -0.0012,  ..., -0.0046,  0.0193, -0.0092],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.norm2.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.23.norm2.bias',\n",
              "               tensor([0.0085, 0.0185, 0.0414,  ..., 0.0341, 0.0336, 0.0486],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.ffn.lin1.weight',\n",
              "               tensor([[-2.5681e-02,  1.1276e-02,  2.1610e-03,  ...,  1.8082e-02,\n",
              "                        -2.5330e-02,  5.3772e-02],\n",
              "                       [ 7.4744e-05, -8.1253e-03,  2.7069e-02,  ...,  1.5556e-02,\n",
              "                         1.4290e-02, -1.2161e-02],\n",
              "                       [-3.3112e-02,  1.3313e-02, -6.1655e-04,  ..., -1.4603e-02,\n",
              "                         3.3325e-02,  3.7781e-02],\n",
              "                       ...,\n",
              "                       [-2.5497e-02,  2.9312e-02, -2.5879e-02,  ..., -1.6327e-02,\n",
              "                         8.4152e-03, -4.6631e-02],\n",
              "                       [-1.3306e-02, -6.7177e-03, -1.8950e-03,  ...,  8.6670e-03,\n",
              "                        -2.0920e-02, -6.1035e-03],\n",
              "                       [-2.5314e-02,  6.7940e-03,  3.8177e-02,  ..., -7.6828e-03,\n",
              "                         2.6474e-02,  1.8280e-02]], dtype=torch.float16)),\n",
              "              ('decoder.layers.23.ffn.lin1.bias',\n",
              "               tensor([-0.0043, -0.0085,  0.0042,  ...,  0.0047,  0.0081,  0.0181],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.ffn.lin2.weight',\n",
              "               tensor([[-0.0112,  0.0215,  0.0320,  ...,  0.0443, -0.0329,  0.0326],\n",
              "                       [-0.0368,  0.0348,  0.0178,  ..., -0.0512, -0.0055, -0.0083],\n",
              "                       [ 0.0327, -0.0160, -0.0131,  ...,  0.0225,  0.0295, -0.0240],\n",
              "                       ...,\n",
              "                       [ 0.0166, -0.0011,  0.0162,  ..., -0.0263,  0.0010, -0.0054],\n",
              "                       [ 0.0172,  0.0136, -0.0161,  ...,  0.0142, -0.0032,  0.0026],\n",
              "                       [-0.0061,  0.0202,  0.0243,  ...,  0.0341,  0.0285,  0.0295]],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.ffn.lin2.bias',\n",
              "               tensor([-0.0406,  0.0311, -0.0089,  ...,  0.0300,  0.0003,  0.0253],\n",
              "                      dtype=torch.float16)),\n",
              "              ('decoder.layers.23.norm3.weight',\n",
              "               tensor([1., 1., 1.,  ..., 1., 1., 1.], dtype=torch.float16)),\n",
              "              ('decoder.layers.23.norm3.bias',\n",
              "               tensor([ 0.0035,  0.0645,  0.0315,  ..., -0.0328, -0.0318,  0.0305],\n",
              "                      dtype=torch.float16))]),\n",
              " 'number_training_updates': 4939,\n",
              " 'optimizer_type': 'mem_eff_adam',\n",
              " 'warmup_scheduler': {'_get_lr_called_within_step': False,\n",
              "  '_last_lr': [7e-06],\n",
              "  '_step_count': 101,\n",
              "  'base_lrs': [7e-06],\n",
              "  'last_epoch': 100,\n",
              "  'lr_lambdas': [{}]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8mwpySvHGp5",
        "colab_type": "code",
        "outputId": "e25787d3-3966-402d-aab3-5ebe6c9d71e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEOqGxviHQUL",
        "colab_type": "code",
        "outputId": "337c3c7f-49cf-4bc8-d809-a7404e74d5c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.path.join(os.getcwd(),'model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/ParlAI/model'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyWvEsp7HdQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2da9e5c-82b6-436f-9b37-5a9269f26e25"
      },
      "source": [
        "!python parlai/scripts/safe_interactive.py -t blended_skill_talk -mf model\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['task'] to blended_skill_talk (previously: internal:blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues )]\n",
            "[ warning: overriding opt['model_file'] to model (previously: /checkpoint/edinan/20200331/finetune_bst_gen_baseline_convai2_normal/de6/model )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from model.dict\n",
            "[ num words =  8008 ]\n",
            "[TransformerGenerator: full interactive mode on.]\n",
            "hi\n",
            "Total parameters: 2,696,268,800 (2,695,613,440 trainable)\n",
            "[ Loading existing model params from model ]\n",
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  display_ignore_fields: label_candidates,text_candidates ]\n",
            "[  display_prettify: False ]\n",
            "[  interactive_task: True ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 128 ]\n",
            "[  datapath: /content/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /private/home/edinan/ParlAI/downloads ]\n",
            "[  dynamic_batching: None ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  init_opt: None ]\n",
            "[  multitask_weights: [1.0, 3.0, 3.0, 3.0] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: blended_skill_talk ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: /checkpoint/parlai/zoo/meena/20200319_meenav0data_tall_2.7B_adamoptimizer/20200319_13.3ppl_200kupdates/model ]\n",
            "[  model: transformer/generator ]\n",
            "[  model_file: model ]\n",
            "[ Safe Local Human Arguments: ] \n",
            "[  safety: all ]\n",
            "[ Local Human Arguments: ] \n",
            "[  local_human_candidates_file: None ]\n",
            "[  single_turn: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Transformer Arguments: ] \n",
            "[  activation: gelu ]\n",
            "[  attention_dropout: 0.0 ]\n",
            "[  dropout: 0.1 ]\n",
            "[  embedding_size: 2560 ]\n",
            "[  embeddings_scale: True ]\n",
            "[  ffn_size: 10240 ]\n",
            "[  learn_positional_embeddings: False ]\n",
            "[  model_parallel: True ]\n",
            "[  n_decoder_layers: 24 ]\n",
            "[  n_encoder_layers: 2 ]\n",
            "[  n_heads: 32 ]\n",
            "[  n_layers: 2 ]\n",
            "[  n_positions: 128 ]\n",
            "[  n_segments: 0 ]\n",
            "[  output_scaling: 1.0 ]\n",
            "[  relu_dropout: 0.0 ]\n",
            "[  share_word_embeddings: True ]\n",
            "[  variant: prelayernorm ]\n",
            "[ Torch Generator Agent: ] \n",
            "[  beam_blacklist_filename: None ]\n",
            "[  beam_block_ngram: 3 ]\n",
            "[  beam_context_block_ngram: 3 ]\n",
            "[  beam_delay: 30 ]\n",
            "[  beam_length_penalty: 0.65 ]\n",
            "[  beam_min_length: 20 ]\n",
            "[  beam_size: 10 ]\n",
            "[  compute_tokenized_bleu: False ]\n",
            "[  inference: beam ]\n",
            "[  skip_generation: False ]\n",
            "[  temperature: 1.0 ]\n",
            "[  topk: 10 ]\n",
            "[  topp: 0.9 ]\n",
            "[ TorchAgent Arguments: ] \n",
            "[  add_p1_after_newln: False ]\n",
            "[  delimiter:    ]\n",
            "[  embedding_projection: random ]\n",
            "[  embedding_type: random ]\n",
            "[  force_fp16_tokens: True ]\n",
            "[  fp16: True ]\n",
            "[  fp16_impl: mem_efficient ]\n",
            "[  gpu: -1 ]\n",
            "[  history_add_global_end_token: end ]\n",
            "[  history_size: -1 ]\n",
            "[  interactive_mode: True ]\n",
            "[  label_truncate: 128 ]\n",
            "[  no_cuda: False ]\n",
            "[  person_tokens: False ]\n",
            "[  rank_candidates: False ]\n",
            "[  split_lines: False ]\n",
            "[  text_truncate: 128 ]\n",
            "[  truncate: 128 ]\n",
            "[  use_reply: label ]\n",
            "[ Optimizer Arguments: ] \n",
            "[  adafactor_eps: [1e-30, 0.001] ]\n",
            "[  adam_eps: 1e-08 ]\n",
            "[  betas: [0.9, 0.999] ]\n",
            "[  gradient_clip: 0.1 ]\n",
            "[  learningrate: 7e-06 ]\n",
            "[  momentum: 0 ]\n",
            "[  nesterov: True ]\n",
            "[  nus: [0.7] ]\n",
            "[  optimizer: mem_eff_adam ]\n",
            "[  weight_decay: None ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: model.dict ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: bytelevelbpe ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "[ BPEHelper Arguments: ] \n",
            "[  bpe_add_prefix_space: True ]\n",
            "[  bpe_merge: model.dict-merges.txt ]\n",
            "[  bpe_vocab: model.dict-vocab.json ]\n",
            "[ Learning Rate Scheduler: ] \n",
            "[  invsqrt_lr_decay_gamma: -1 ]\n",
            "[  lr_scheduler: reduceonplateau ]\n",
            "[  lr_scheduler_decay: 0.5 ]\n",
            "[  lr_scheduler_patience: 3 ]\n",
            "[  max_lr_steps: -1 ]\n",
            "[  update_freq: 2 ]\n",
            "[  warmup_rate: 0.0001 ]\n",
            "[  warmup_updates: 100 ]\n",
            "[ BST Interactive World: ] \n",
            "[  display_partner_persona: True ]\n",
            "[  include_initial_utterances: False ]\n",
            "[  include_personas: True ]\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "[ warning: overriding opt['model'] to transformer/classifier (previously: transformer_classifier )]\n",
            "[ warning: overriding opt['model_file'] to /content/ParlAI/data/models/dialogue_safety/single_turn/model (previously: /checkpoint/edinan/20190828/safety_reddit/contiguous-dropout=0_multitask-weights=0.5,0.1,0.1,0.4,0.2_lr=5e-05_lr-scheduler-patience=3_lr-scheduler-decay=0.9_warmupupdates=1000/model )]\n",
            "[ warning: overriding opt['print_scores'] to True (previously: False )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from /content/ParlAI/data/models/dialogue_safety/single_turn/model.dict\n",
            "[ num words =  54944 ]\n",
            "Loading existing model parameters from /content/ParlAI/data/models/dialogue_safety/single_turn/model\n",
            "[creating task(s): blended_skill_talk]\n",
            "[ loading personas.. ]\n",
            "\n",
            "  [NOTE: In the BST paper both partner's have a persona.\n",
            "           You can choose to ignore yours, the model never sees it.\n",
            "           In the Blender paper, this was not used for humans.\n",
            "           You can also turn personas off with --include-personas False]\n",
            "\n",
            "\u001b[0;34m[context]:\u001b[0;0m \u001b[1myour persona: i hate green beans.\n",
            "your persona: i love to watch game shows.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m /pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mhello, how are you? do you have any pets? i have a dog and a cat.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m hi\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mwhat do you like to do for fun? i like to go out and listen to my neighbors argue.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m haha, thats funny that you like people arguing. I love literature\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi do too! what is your favorite book? mine is 1984 by George Orwell.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m my favorite book is sri aurobindo's life divine.\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi will have to check that one out. i like dystopian novels a lot.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m name one dystopian novel you like\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mi really like the hunger games. its a series of novels by james patterson.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m o i see. do you know freddie mercury?\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mno i don't. who is he? i know he was a singer and songwriter.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m yes, you guessed it right. \n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1moh wow. i didn't know that. he was born in 1964. how old are you?\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m no he was born in 1946\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1moh ok. what do you like to do in your spare time? i like to play video games.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m i like to play piano...\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mthat's awesome. i wish i could play an instrument. what kind of music do you play?\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m mostly pieces by frederic chopin\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mthat's cool. i've never heard of him. what genre of music is he known for?\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m western classical.\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1moh nice. i love classical music. do you have a favorite composer? mine is jimi hendrix.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m thats awesome..mine is beethoven\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mnice. what do you do for a living? i work at a grocery store as a cashier.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m i am an engineer. I develop softwares\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1mthat's really cool. what type of software do you develop? i've always been interested in software development.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m brother, i have to go and do some work....will see you later\n",
            "\u001b[0;34m[TransformerGenerator]:\u001b[0;0m \u001b[1msounds good. i look forward to it. have a great day!   p.s. i'm a sister.\u001b[0;0m\n",
            "\u001b[0;34mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "\n",
            "CHAT DONE.\n",
            "\n",
            "Your partner was playing the following persona:\n",
            "partner's persona: i want a dog , but that's a lot of commitment.\n",
            "partner's persona: i love to go outside at night and eavesdrop on my neighbor s arguments.\n",
            "EPOCH DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwVE1vuIZinC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef507de1-513a-417b-af56-e32dee97d5c9"
      },
      "source": [
        "!ls agents"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'agents': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zeY7iqef2f2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}