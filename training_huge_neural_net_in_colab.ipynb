{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training huge neural net in colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1M2v6DlXsXj",
        "colab_type": "code",
        "outputId": "6e9a8355-3886-4668-b8c6-51c4980fa19e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr  9 16:21:59 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7xNYpSUXuVk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "a641e267-85db-4fbf-c218-185f3ace7337"
      },
      "source": [
        "!pip install git+https://github.com/newcodevelop/transformers.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/newcodevelop/transformers.git\n",
            "  Cloning https://github.com/newcodevelop/transformers.git to /tmp/pip-req-build-j9l39upz\n",
            "  Running command git clone -q https://github.com/newcodevelop/transformers.git /tmp/pip-req-build-j9l39upz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.2)\n",
            "Collecting tokenizers==0.7.0rc3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/12/ef52510d173a347cec1939e61e7c5be88cc0d9cff995ba4ab1f30795bc31/tokenizers-0.7.0rc3-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.12.35)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 46.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 39.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.35 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.15.35)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->transformers==2.8.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->transformers==2.8.0) (2.8.1)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.8.0-cp36-none-any.whl size=559843 sha256=63c31eb0cd2a3f9de42f67889da92ca742e9fe4e838e567c83e61a639591b063\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c8ape6vs/wheels/21/89/ed/6b496711f7ffa23f54f03e5e1edfcee8b28a773701ca213a1a\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=49d0592bc94c13d45ca51e87fc930081c912082cdfa9a8cf334de1526c20fe65\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.7.0rc3 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzwMhAYrwIvA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd590772-40dc-4437-bad1-c0cd8042f0dc"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev-v1.1.json  evaluate-v1.1.py  sample_data  train-v1.1.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39K0O3H-wSR3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9e6e552d-5a98-4c98-aca8-9de7ac08b697"
      },
      "source": [
        "!git clone https://github.com/newcodevelop/transformers.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/80)\u001b[K\rremote: Counting objects:   2% (2/80)\u001b[K\rremote: Counting objects:   3% (3/80)\u001b[K\rremote: Counting objects:   5% (4/80)\u001b[K\rremote: Counting objects:   6% (5/80)\u001b[K\rremote: Counting objects:   7% (6/80)\u001b[K\rremote: Counting objects:   8% (7/80)\u001b[K\rremote: Counting objects:  10% (8/80)\u001b[K\rremote: Counting objects:  11% (9/80)\u001b[K\rremote: Counting objects:  12% (10/80)\u001b[K\rremote: Counting objects:  13% (11/80)\u001b[K\rremote: Counting objects:  15% (12/80)\u001b[K\rremote: Counting objects:  16% (13/80)\u001b[K\rremote: Counting objects:  17% (14/80)\u001b[K\rremote: Counting objects:  18% (15/80)\u001b[K\rremote: Counting objects:  20% (16/80)\u001b[K\rremote: Counting objects:  21% (17/80)\u001b[K\rremote: Counting objects:  22% (18/80)\u001b[K\rremote: Counting objects:  23% (19/80)\u001b[K\rremote: Counting objects:  25% (20/80)\u001b[K\rremote: Counting objects:  26% (21/80)\u001b[K\rremote: Counting objects:  27% (22/80)\u001b[K\rremote: Counting objects:  28% (23/80)\u001b[K\rremote: Counting objects:  30% (24/80)\u001b[K\rremote: Counting objects:  31% (25/80)\u001b[K\rremote: Counting objects:  32% (26/80)\u001b[K\rremote: Counting objects:  33% (27/80)\u001b[K\rremote: Counting objects:  35% (28/80)\u001b[K\rremote: Counting objects:  36% (29/80)\u001b[K\rremote: Counting objects:  37% (30/80)\u001b[K\rremote: Counting objects:  38% (31/80)\u001b[K\rremote: Counting objects:  40% (32/80)\u001b[K\rremote: Counting objects:  41% (33/80)\u001b[K\rremote: Counting objects:  42% (34/80)\u001b[K\rremote: Counting objects:  43% (35/80)\u001b[K\rremote: Counting objects:  45% (36/80)\rremote: Counting objects:  46% (37/80)\u001b[K\rremote: Counting objects:  47% (38/80)\u001b[K\rremote: Counting objects:  48% (39/80)\u001b[K\rremote: Counting objects:  50% (40/80)\u001b[K\rremote: Counting objects:  51% (41/80)\u001b[K\rremote: Counting objects:  52% (42/80)\u001b[K\rremote: Counting objects:  53% (43/80)\u001b[K\rremote: Counting objects:  55% (44/80)\u001b[K\rremote: Counting objects:  56% (45/80)\u001b[K\rremote: Counting objects:  57% (46/80)\u001b[K\rremote: Counting objects:  58% (47/80)\u001b[K\rremote: Counting objects:  60% (48/80)\u001b[K\rremote: Counting objects:  61% (49/80)\u001b[K\rremote: Counting objects:  62% (50/80)\u001b[K\rremote: Counting objects:  63% (51/80)\u001b[K\rremote: Counting objects:  65% (52/80)\u001b[K\rremote: Counting objects:  66% (53/80)\u001b[K\rremote: Counting objects:  67% (54/80)\u001b[K\rremote: Counting objects:  68% (55/80)\u001b[K\rremote: Counting objects:  70% (56/80)\u001b[K\rremote: Counting objects:  71% (57/80)\u001b[K\rremote: Counting objects:  72% (58/80)\u001b[K\rremote: Counting objects:  73% (59/80)\u001b[K\rremote: Counting objects:  75% (60/80)\u001b[K\rremote: Counting objects:  76% (61/80)\u001b[K\rremote: Counting objects:  77% (62/80)\u001b[K\rremote: Counting objects:  78% (63/80)\u001b[K\rremote: Counting objects:  80% (64/80)\u001b[K\rremote: Counting objects:  81% (65/80)\u001b[K\rremote: Counting objects:  82% (66/80)\u001b[K\rremote: Counting objects:  83% (67/80)\u001b[K\rremote: Counting objects:  85% (68/80)\u001b[K\rremote: Counting objects:  86% (69/80)\u001b[K\rremote: Counting objects:  87% (70/80)\u001b[K\rremote: Counting objects:  88% (71/80)\u001b[K\rremote: Counting objects:  90% (72/80)\u001b[K\rremote: Counting objects:  91% (73/80)\u001b[K\rremote: Counting objects:  92% (74/80)\u001b[K\rremote: Counting objects:  93% (75/80)\u001b[K\rremote: Counting objects:  95% (76/80)\u001b[K\rremote: Counting objects:  96% (77/80)\u001b[K\rremote: Counting objects:  97% (78/80)\u001b[K\rremote: Counting objects:  98% (79/80)\u001b[K\rremote: Counting objects: 100% (80/80)\u001b[K\rremote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 23650 (delta 36), reused 35 (delta 16), pack-reused 23570\u001b[K\n",
            "Receiving objects: 100% (23650/23650), 14.21 MiB | 18.31 MiB/s, done.\n",
            "Resolving deltas: 100% (16725/16725), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm6wtpK-wWHu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "537ba3e6-c178-4682-f10a-5275410518da"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev-v1.1.json  evaluate-v1.1.py  sample_data  train-v1.1.json  transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5aLuIYD5V7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f21d9018-6ac2-452c-cf29-8d00fde1032c"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'transformers': No such file or directory\n",
            "Uninstalling transformers-2.8.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.6/dist-packages/transformers-2.8.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/transformers/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoe-H3FQ6XUX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "503d2b70-6f0d-4dfc-f208-2bb4d33738bf"
      },
      "source": [
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
        "!wget https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-09 16:22:52--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.111.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30288272 (29M) [application/json]\n",
            "Saving to: ‘train-v1.1.json’\n",
            "\n",
            "\rtrain-v1.1.json       0%[                    ]       0  --.-KB/s               \rtrain-v1.1.json      16%[==>                 ]   4.75M  23.7MB/s               \rtrain-v1.1.json      77%[==============>     ]  22.50M  56.2MB/s               \rtrain-v1.1.json     100%[===================>]  28.88M  66.3MB/s    in 0.4s    \n",
            "\n",
            "2020-04-09 16:22:53 (66.3 MB/s) - ‘train-v1.1.json’ saved [30288272/30288272]\n",
            "\n",
            "--2020-04-09 16:22:56--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.111.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4854279 (4.6M) [application/json]\n",
            "Saving to: ‘dev-v1.1.json’\n",
            "\n",
            "dev-v1.1.json       100%[===================>]   4.63M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-04-09 16:22:56 (63.6 MB/s) - ‘dev-v1.1.json’ saved [4854279/4854279]\n",
            "\n",
            "--2020-04-09 16:22:58--  https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘evaluate-v1.1.py’\n",
            "\n",
            "evaluate-v1.1.py        [ <=>                ] 109.63K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-04-09 16:22:59 (4.63 MB/s) - ‘evaluate-v1.1.py’ saved [112263]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvxXXPfc6oLf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29898f4c-3b4a-4102-c98c-89eea2b8f37c"
      },
      "source": [
        "cmd = \"python transformers/examples/run_squad.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-large-uncased \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_lower_case \\\n",
        "    --train_file /content/train-v1.1.json \\\n",
        "    --predict_file /content/dev-v1.1.json \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --max_seq_length 384 \\\n",
        "    --doc_stride 128 \\\n",
        "    --output_dir /models/wwm_uncased_finetuned_squad1/ \\\n",
        "    --per_gpu_eval_batch_size=12   \\\n",
        "    --per_gpu_train_batch_size=12  \"\n",
        "\n",
        "!{cmd}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-09 16:24:40.256952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "04/09/2020 16:24:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/09/2020 16:24:42 - INFO - filelock -   Lock 139747392658176 acquired on /root/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.fc076a4d5f1edf25ea3a2bd66e9f6f295dcd64c81dfef5b3f5a3eb2a82751ad1.lock\n",
            "04/09/2020 16:24:42 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpj3ffkg5d\n",
            "Downloading: 100% 362/362 [00:00<00:00, 346kB/s]\n",
            "04/09/2020 16:24:42 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json in cache at /root/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.fc076a4d5f1edf25ea3a2bd66e9f6f295dcd64c81dfef5b3f5a3eb2a82751ad1\n",
            "04/09/2020 16:24:42 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.fc076a4d5f1edf25ea3a2bd66e9f6f295dcd64c81dfef5b3f5a3eb2a82751ad1\n",
            "04/09/2020 16:24:42 - INFO - filelock -   Lock 139747392658176 released on /root/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.fc076a4d5f1edf25ea3a2bd66e9f6f295dcd64c81dfef5b3f5a3eb2a82751ad1.lock\n",
            "04/09/2020 16:24:42 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /root/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.fc076a4d5f1edf25ea3a2bd66e9f6f295dcd64c81dfef5b3f5a3eb2a82751ad1\n",
            "04/09/2020 16:24:42 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/09/2020 16:24:42 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /root/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.fc076a4d5f1edf25ea3a2bd66e9f6f295dcd64c81dfef5b3f5a3eb2a82751ad1\n",
            "04/09/2020 16:24:42 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/09/2020 16:24:43 - INFO - filelock -   Lock 139747392658344 acquired on /root/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "04/09/2020 16:24:43 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp92rlx8l_\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.24MB/s]\n",
            "04/09/2020 16:24:43 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt in cache at /root/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/09/2020 16:24:43 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/09/2020 16:24:43 - INFO - filelock -   Lock 139747392658344 released on /root/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "04/09/2020 16:24:43 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /root/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/09/2020 16:24:44 - INFO - filelock -   Lock 139747392682472 acquired on /root/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6.lock\n",
            "04/09/2020 16:24:44 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp5atdhb4t\n",
            "Downloading: 100% 1.34G/1.34G [00:30<00:00, 43.6MB/s]\n",
            "04/09/2020 16:25:15 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
            "04/09/2020 16:25:15 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
            "04/09/2020 16:25:15 - INFO - filelock -   Lock 139747392682472 released on /root/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6.lock\n",
            "04/09/2020 16:25:15 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
            "04/09/2020 16:25:23 - INFO - transformers.modeling_utils -   Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "04/09/2020 16:25:23 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "04/09/2020 16:25:39 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='bert-large-uncased', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=2.0, output_dir='/models/wwm_uncased_finetuned_squad1/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=12, predict_file='/content/dev-v1.1.json', save_steps=500, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file='/content/train-v1.1.json', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
            "04/09/2020 16:25:39 - INFO - __main__ -   Creating features from dataset file at .\n",
            "100% 442/442 [00:28<00:00, 15.76it/s]\n",
            "convert squad examples to features: 100% 87599/87599 [10:58<00:00, 133.05it/s]\n",
            "add example index and unique id: 100% 87599/87599 [00:00<00:00, 886249.38it/s]\n",
            "04/09/2020 16:37:09 - INFO - __main__ -   Saving features into cached file ./cached_train_bert-large-uncased_384\n",
            "tcmalloc: large alloc 1179140096 bytes == 0x7f182548a000 @  0x7f1a654722a4 0x592727 0x4dddf7 0x4e2667 0x4e2aab 0x4e30d0 0x4e14d8 0x4e2c5b 0x4e2a3b 0x4e30d0 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50d390 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245\n",
            "04/09/2020 16:39:18 - INFO - __main__ -   ***** Running training *****\n",
            "04/09/2020 16:39:18 - INFO - __main__ -     Num examples = 88641\n",
            "04/09/2020 16:39:18 - INFO - __main__ -     Num Epochs = 2\n",
            "04/09/2020 16:39:18 - INFO - __main__ -     Instantaneous batch size per GPU = 12\n",
            "04/09/2020 16:39:18 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "04/09/2020 16:39:18 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "04/09/2020 16:39:18 - INFO - __main__ -     Total optimization steps = 14774\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/7387 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/7387 [00:02<5:03:15,  2.46s/it]\u001b[A\n",
            "Iteration:   0% 2/7387 [00:04<4:46:52,  2.33s/it]\u001b[A\n",
            "Iteration:   0% 3/7387 [00:06<4:35:22,  2.24s/it]\u001b[A\n",
            "Iteration:   0% 4/7387 [00:08<4:27:13,  2.17s/it]\u001b[A\n",
            "Iteration:   0% 5/7387 [00:10<4:21:30,  2.13s/it]\u001b[A\n",
            "Iteration:   0% 6/7387 [00:12<4:17:37,  2.09s/it]\u001b[A\n",
            "Iteration:   0% 7/7387 [00:14<4:15:03,  2.07s/it]\u001b[A\n",
            "Iteration:   0% 8/7387 [00:16<4:13:16,  2.06s/it]\u001b[A\n",
            "Iteration:   0% 9/7387 [00:18<4:12:00,  2.05s/it]\u001b[A\n",
            "Iteration:   0% 10/7387 [00:20<4:10:51,  2.04s/it]\u001b[A\n",
            "Iteration:   0% 11/7387 [00:22<4:10:19,  2.04s/it]\u001b[A\n",
            "Iteration:   0% 12/7387 [00:24<4:09:47,  2.03s/it]\u001b[A\n",
            "Iteration:   0% 13/7387 [00:26<4:09:20,  2.03s/it]\u001b[A\n",
            "Iteration:   0% 14/7387 [00:28<4:08:57,  2.03s/it]\u001b[A\n",
            "Iteration:   0% 15/7387 [00:30<4:08:45,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 16/7387 [00:32<4:08:38,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 17/7387 [00:34<4:08:28,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 18/7387 [00:36<4:08:14,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 19/7387 [00:38<4:08:09,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 20/7387 [00:40<4:08:00,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 21/7387 [00:42<4:08:10,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 22/7387 [00:44<4:08:03,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 23/7387 [00:46<4:08:04,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 24/7387 [00:48<4:08:03,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 25/7387 [00:50<4:07:54,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 26/7387 [00:52<4:07:55,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 27/7387 [00:55<4:07:53,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 28/7387 [00:57<4:07:58,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 29/7387 [00:59<4:07:52,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 30/7387 [01:01<4:07:59,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 31/7387 [01:03<4:07:57,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 32/7387 [01:05<4:07:47,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 33/7387 [01:07<4:07:47,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 34/7387 [01:09<4:07:45,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 35/7387 [01:11<4:07:43,  2.02s/it]\u001b[A\n",
            "Iteration:   0% 36/7387 [01:13<4:07:44,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 37/7387 [01:15<4:07:36,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 38/7387 [01:17<4:07:31,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 39/7387 [01:19<4:07:30,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 40/7387 [01:21<4:07:37,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 41/7387 [01:23<4:07:28,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 42/7387 [01:25<4:07:22,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 43/7387 [01:27<4:07:18,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 44/7387 [01:29<4:07:19,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 45/7387 [01:31<4:07:14,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 46/7387 [01:33<4:07:14,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 47/7387 [01:35<4:07:11,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 48/7387 [01:37<4:07:15,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 49/7387 [01:39<4:07:16,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 50/7387 [01:41<4:07:14,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 51/7387 [01:43<4:07:09,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 52/7387 [01:45<4:07:08,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 53/7387 [01:47<4:07:03,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 54/7387 [01:49<4:07:03,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 55/7387 [01:51<4:07:04,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 56/7387 [01:53<4:07:08,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 57/7387 [01:55<4:07:05,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 58/7387 [01:57<4:07:04,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 59/7387 [01:59<4:07:03,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 60/7387 [02:01<4:06:53,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 61/7387 [02:03<4:06:45,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 62/7387 [02:05<4:06:43,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 63/7387 [02:07<4:06:43,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 64/7387 [02:09<4:06:45,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 65/7387 [02:11<4:06:58,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 66/7387 [02:13<4:06:48,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 67/7387 [02:15<4:06:47,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 68/7387 [02:17<4:06:37,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 69/7387 [02:19<4:06:44,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 70/7387 [02:21<4:06:44,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 71/7387 [02:23<4:06:41,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 72/7387 [02:26<4:06:35,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 73/7387 [02:28<4:06:32,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 74/7387 [02:30<4:06:26,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 75/7387 [02:32<4:06:19,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 76/7387 [02:34<4:06:19,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 77/7387 [02:36<4:06:18,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 78/7387 [02:38<4:06:16,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 79/7387 [02:40<4:06:12,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 80/7387 [02:42<4:06:08,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 81/7387 [02:44<4:06:14,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 82/7387 [02:46<4:06:09,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 83/7387 [02:48<4:06:02,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 84/7387 [02:50<4:06:04,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 85/7387 [02:52<4:06:02,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 86/7387 [02:54<4:05:53,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 87/7387 [02:56<4:05:49,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 88/7387 [02:58<4:05:51,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 89/7387 [03:00<4:05:56,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 90/7387 [03:02<4:05:55,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 91/7387 [03:04<4:05:46,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 92/7387 [03:06<4:05:44,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 93/7387 [03:08<4:05:42,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 94/7387 [03:10<4:05:45,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 95/7387 [03:12<4:05:42,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 96/7387 [03:14<4:05:41,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 97/7387 [03:16<4:05:34,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 98/7387 [03:18<4:05:35,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 99/7387 [03:20<4:05:34,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 100/7387 [03:22<4:05:26,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 101/7387 [03:24<4:05:28,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 102/7387 [03:26<4:05:29,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 103/7387 [03:28<4:05:28,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 104/7387 [03:30<4:05:29,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 105/7387 [03:32<4:05:22,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 106/7387 [03:34<4:05:24,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 107/7387 [03:36<4:05:21,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 108/7387 [03:38<4:05:17,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 109/7387 [03:40<4:05:13,  2.02s/it]\u001b[A\n",
            "Iteration:   1% 110/7387 [03:42<4:05:20,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 111/7387 [03:44<4:05:15,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 112/7387 [03:46<4:05:07,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 113/7387 [03:48<4:05:08,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 114/7387 [03:50<4:05:01,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 115/7387 [03:52<4:04:59,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 116/7387 [03:54<4:04:51,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 117/7387 [03:56<4:04:52,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 118/7387 [03:58<4:04:58,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 119/7387 [04:01<4:04:57,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 120/7387 [04:03<4:04:51,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 121/7387 [04:05<4:04:44,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 122/7387 [04:07<4:04:40,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 123/7387 [04:09<4:04:43,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 124/7387 [04:11<4:04:41,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 125/7387 [04:13<4:04:38,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 126/7387 [04:15<4:04:39,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 127/7387 [04:17<4:04:45,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 128/7387 [04:19<4:04:39,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 129/7387 [04:21<4:04:35,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 130/7387 [04:23<4:04:39,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 131/7387 [04:25<4:04:36,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 132/7387 [04:27<4:04:31,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 133/7387 [04:29<4:04:35,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 134/7387 [04:31<4:04:29,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 135/7387 [04:33<4:04:27,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 136/7387 [04:35<4:04:22,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 137/7387 [04:37<4:04:20,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 138/7387 [04:39<4:04:18,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 139/7387 [04:41<4:04:11,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 140/7387 [04:43<4:04:09,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 141/7387 [04:45<4:04:10,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 142/7387 [04:47<4:04:05,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 143/7387 [04:49<4:04:03,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 144/7387 [04:51<4:03:58,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 145/7387 [04:53<4:03:58,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 146/7387 [04:55<4:03:56,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 147/7387 [04:57<4:03:58,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 148/7387 [04:59<4:03:53,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 149/7387 [05:01<4:03:53,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 150/7387 [05:03<4:03:51,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 151/7387 [05:05<4:03:46,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 152/7387 [05:07<4:03:44,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 153/7387 [05:09<4:03:45,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 154/7387 [05:11<4:03:46,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 155/7387 [05:13<4:03:37,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 156/7387 [05:15<4:03:37,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 157/7387 [05:17<4:03:34,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 158/7387 [05:19<4:03:31,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 159/7387 [05:21<4:03:27,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 160/7387 [05:23<4:03:29,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 161/7387 [05:25<4:03:26,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 162/7387 [05:27<4:03:25,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 163/7387 [05:29<4:03:24,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 164/7387 [05:31<4:03:17,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 165/7387 [05:34<4:03:16,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 166/7387 [05:36<4:03:13,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 167/7387 [05:38<4:03:09,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 168/7387 [05:40<4:03:07,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 169/7387 [05:42<4:03:05,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 170/7387 [05:44<4:03:14,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 171/7387 [05:46<4:03:15,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 172/7387 [05:48<4:03:14,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 173/7387 [05:50<4:03:08,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 174/7387 [05:52<4:03:02,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 175/7387 [05:54<4:03:01,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 176/7387 [05:56<4:02:58,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 177/7387 [05:58<4:02:52,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 178/7387 [06:00<4:02:45,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 179/7387 [06:02<4:02:53,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 180/7387 [06:04<4:02:51,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 181/7387 [06:06<4:02:53,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 182/7387 [06:08<4:02:48,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 183/7387 [06:10<4:02:53,  2.02s/it]\u001b[A\n",
            "Iteration:   2% 184/7387 [06:12<4:02:42,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 185/7387 [06:14<4:02:47,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 186/7387 [06:16<4:03:02,  2.03s/it]\u001b[A\n",
            "Iteration:   3% 187/7387 [06:18<4:03:00,  2.03s/it]\u001b[A\n",
            "Iteration:   3% 188/7387 [06:20<4:02:55,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 189/7387 [06:22<4:02:46,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 190/7387 [06:24<4:02:41,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 191/7387 [06:26<4:02:31,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 192/7387 [06:28<4:02:30,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 193/7387 [06:30<4:02:29,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 194/7387 [06:32<4:02:22,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 195/7387 [06:34<4:02:27,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 196/7387 [06:36<4:02:22,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 197/7387 [06:38<4:02:12,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 198/7387 [06:40<4:02:11,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 199/7387 [06:42<4:02:11,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 200/7387 [06:44<4:02:05,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 201/7387 [06:46<4:02:05,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 202/7387 [06:48<4:02:05,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 203/7387 [06:50<4:02:06,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 204/7387 [06:52<4:02:03,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 205/7387 [06:54<4:02:00,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 206/7387 [06:56<4:01:51,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 207/7387 [06:58<4:01:50,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 208/7387 [07:00<4:01:46,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 209/7387 [07:02<4:01:45,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 210/7387 [07:05<4:01:44,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 211/7387 [07:07<4:01:40,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 212/7387 [07:09<4:01:38,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 213/7387 [07:11<4:01:43,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 214/7387 [07:13<4:01:38,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 215/7387 [07:15<4:01:32,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 216/7387 [07:17<4:01:27,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 217/7387 [07:19<4:01:28,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 218/7387 [07:21<4:01:27,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 219/7387 [07:23<4:01:20,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 220/7387 [07:25<4:01:27,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 221/7387 [07:27<4:01:35,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 222/7387 [07:29<4:01:54,  2.03s/it]\u001b[A\n",
            "Iteration:   3% 223/7387 [07:31<4:01:49,  2.03s/it]\u001b[A\n",
            "Iteration:   3% 224/7387 [07:33<4:01:43,  2.02s/it]\u001b[A\n",
            "Iteration:   3% 225/7387 [07:35<4:01:49,  2.03s/it]\u001b[A"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-7-d5c11c0f1ba4>\", line 3, in <module>\n",
            "    get_ipython().system('{cmd}')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\", line 96, in system\n",
            "    output = _system_commands._system_compat(self, *args, **kwargs)  # pylint:disable=protected-access\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\", line 438, in _system_compat\n",
            "    shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\", line 195, in _run_command\n",
            "    return _monitor_process(parent_pty, epoll, p, cmd, update_stdin_widget)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\", line 222, in _monitor_process\n",
            "    result = _poll_process(parent_pty, epoll, p, cmd, decoder, state)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\", line 324, in _poll_process\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 739, in getmodule\n",
            "    f = getabsfile(module)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 708, in getabsfile\n",
            "    _filename = getsourcefile(object) or getfile(object)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
            "    if os.path.exists(filename):\n",
            "  File \"/usr/lib/python3.6/genericpath.py\", line 19, in exists\n",
            "    os.stat(path)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5osPeMSwgOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1070fb58-1eac-4f44-c67a-abb0335e5c57"
      },
      "source": [
        "!rm -r transformers\n",
        "!pip uninstall transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling transformers-2.8.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.6/dist-packages/transformers-2.8.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/transformers/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwUGQ5PJ1wCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r cached_train_bert-large-uncased_384 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3FJLBcV1zcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-Ay85QO1pfg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07c64ba4-2eb8-4ef1-9103-22b6d130709b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev-v1.1.json  evaluate-v1.1.py  sample_data  train-v1.1.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IItFk-eXwnwn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "1fd2e9da-41d3-4ffe-88f5-1ea15be0ffda"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-i_t11un7\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-i_t11un7\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.2)\n",
            "Requirement already satisfied: tokenizers==0.7.0rc3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7.0rc3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.12.35)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.0.38)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.35 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.15.35)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->transformers==2.8.0) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->transformers==2.8.0) (0.15.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.8.0-cp36-none-any.whl size=559812 sha256=3a00b9840081053f886566fc988b90067afdcec66b7b3e3c0ced9a4e4af24e67\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sh2cc65s/wheels/33/eb/3b/4bf5dd835e865e472d4fc0754f35ac0edb08fe852e8f21655f\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHdEzpzf7Yem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "325d0817-e28c-48bb-93c1-36dca4d87a9c"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 287, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/287)\u001b[K\rremote: Counting objects:   1% (3/287)\u001b[K\rremote: Counting objects:   2% (6/287)\u001b[K\rremote: Counting objects:   3% (9/287)\u001b[K\rremote: Counting objects:   4% (12/287)\u001b[K\rremote: Counting objects:   5% (15/287)\u001b[K\rremote: Counting objects:   6% (18/287)\u001b[K\rremote: Counting objects:   7% (21/287)\u001b[K\rremote: Counting objects:   8% (23/287)\u001b[K\rremote: Counting objects:   9% (26/287)\u001b[K\rremote: Counting objects:  10% (29/287)\u001b[K\rremote: Counting objects:  11% (32/287)\u001b[K\rremote: Counting objects:  12% (35/287)\u001b[K\rremote: Counting objects:  13% (38/287)\u001b[K\rremote: Counting objects:  14% (41/287)\u001b[K\rremote: Counting objects:  15% (44/287)\u001b[K\rremote: Counting objects:  16% (46/287)\u001b[K\rremote: Counting objects:  17% (49/287)\u001b[K\rremote: Counting objects:  18% (52/287)\u001b[K\rremote: Counting objects:  19% (55/287)\u001b[K\rremote: Counting objects:  20% (58/287)\u001b[K\rremote: Counting objects:  21% (61/287)\u001b[K\rremote: Counting objects:  22% (64/287)\u001b[K\rremote: Counting objects:  23% (67/287)\u001b[K\rremote: Counting objects:  24% (69/287)\u001b[K\rremote: Counting objects:  25% (72/287)\u001b[K\rremote: Counting objects:  26% (75/287)\u001b[K\rremote: Counting objects:  27% (78/287)\u001b[K\rremote: Counting objects:  28% (81/287)\u001b[K\rremote: Counting objects:  29% (84/287)\u001b[K\rremote: Counting objects:  30% (87/287)\u001b[K\rremote: Counting objects:  31% (89/287)\u001b[K\rremote: Counting objects:  32% (92/287)\u001b[K\rremote: Counting objects:  33% (95/287)\u001b[K\rremote: Counting objects:  34% (98/287)\u001b[K\rremote: Counting objects:  35% (101/287)\u001b[K\rremote: Counting objects:  36% (104/287)\u001b[K\rremote: Counting objects:  37% (107/287)\u001b[K\rremote: Counting objects:  38% (110/287)\u001b[K\rremote: Counting objects:  39% (112/287)\u001b[K\rremote: Counting objects:  40% (115/287)\u001b[K\rremote: Counting objects:  41% (118/287)\u001b[K\rremote: Counting objects:  42% (121/287)\u001b[K\rremote: Counting objects:  43% (124/287)\u001b[K\rremote: Counting objects:  44% (127/287)\u001b[K\rremote: Counting objects:  45% (130/287)\u001b[K\rremote: Counting objects:  46% (133/287)\u001b[K\rremote: Counting objects:  47% (135/287)\u001b[K\rremote: Counting objects:  48% (138/287)\u001b[K\rremote: Counting objects:  49% (141/287)\u001b[K\rremote: Counting objects:  50% (144/287)\u001b[K\rremote: Counting objects:  51% (147/287)\u001b[K\rremote: Counting objects:  52% (150/287)\u001b[K\rremote: Counting objects:  53% (153/287)\u001b[K\rremote: Counting objects:  54% (155/287)\u001b[K\rremote: Counting objects:  55% (158/287)\u001b[K\rremote: Counting objects:  56% (161/287)\u001b[K\rremote: Counting objects:  57% (164/287)\u001b[K\rremote: Counting objects:  58% (167/287)\u001b[K\rremote: Counting objects:  59% (170/287)\u001b[K\rremote: Counting objects:  60% (173/287)\u001b[K\rremote: Counting objects:  61% (176/287)\u001b[K\rremote: Counting objects:  62% (178/287)\u001b[K\rremote: Counting objects:  63% (181/287)\u001b[K\rremote: Counting objects:  64% (184/287)\u001b[K\rremote: Counting objects:  65% (187/287)\u001b[K\rremote: Counting objects:  66% (190/287)\u001b[K\rremote: Counting objects:  67% (193/287)\u001b[K\rremote: Counting objects:  68% (196/287)\u001b[K\rremote: Counting objects:  69% (199/287)\u001b[K\rremote: Counting objects:  70% (201/287)\u001b[K\rremote: Counting objects:  71% (204/287)\u001b[K\rremote: Counting objects:  72% (207/287)\u001b[K\rremote: Counting objects:  73% (210/287)\u001b[K\rremote: Counting objects:  74% (213/287)\u001b[K\rremote: Counting objects:  75% (216/287)\u001b[K\rremote: Counting objects:  76% (219/287)\u001b[K\rremote: Counting objects:  77% (221/287)\u001b[K\rremote: Counting objects:  78% (224/287)\u001b[K\rremote: Counting objects:  79% (227/287)\u001b[K\rremote: Counting objects:  80% (230/287)\u001b[K\rremote: Counting objects:  81% (233/287)\u001b[K\rremote: Counting objects:  82% (236/287)\u001b[K\rremote: Counting objects:  83% (239/287)\u001b[K\rremote: Counting objects:  84% (242/287)\u001b[K\rremote: Counting objects:  85% (244/287)\u001b[K\rremote: Counting objects:  86% (247/287)\u001b[K\rremote: Counting objects:  87% (250/287)\u001b[K\rremote: Counting objects:  88% (253/287)\u001b[K\rremote: Counting objects:  89% (256/287)\u001b[K\rremote: Counting objects:  90% (259/287)\u001b[K\rremote: Counting objects:  91% (262/287)\u001b[K\rremote: Counting objects:  92% (265/287)\u001b[K\rremote: Counting objects:  93% (267/287)\u001b[K\rremote: Counting objects:  94% (270/287)\u001b[K\rremote: Counting objects:  95% (273/287)\u001b[K\rremote: Counting objects:  96% (276/287)\u001b[K\rremote: Counting objects:  97% (279/287)\u001b[K\rremote: Counting objects:  98% (282/287)\u001b[K\rremote: Counting objects:  99% (285/287)\u001b[K\rremote: Counting objects: 100% (287/287)\u001b[K\rremote: Counting objects: 100% (287/287), done.\u001b[K\n",
            "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
            "remote: Total 23718 (delta 153), reused 230 (delta 123), pack-reused 23431\u001b[K\n",
            "Receiving objects: 100% (23718/23718), 14.25 MiB | 19.56 MiB/s, done.\n",
            "Resolving deltas: 100% (16766/16766), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iKhtlxO7bUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ff2e781-f299-48c0-c273-d7f5aec5388e"
      },
      "source": [
        "cmd = \"python transformers/examples/run_squad.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-large-uncased \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_lower_case \\\n",
        "    --train_file /content/train-v1.1.json \\\n",
        "    --predict_file /content/dev-v1.1.json \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --max_seq_length 384 \\\n",
        "    --doc_stride 128 \\\n",
        "    --output_dir /models/wwm_uncased_finetuned_squad1/ \\\n",
        "    --per_gpu_eval_batch_size=12   \\\n",
        "    --per_gpu_train_batch_size=12  \"\n",
        "\n",
        "!{cmd}"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-09 16:48:38.033952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "04/09/2020 16:48:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/09/2020 16:48:42 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /root/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.fc076a4d5f1edf25ea3a2bd66e9f6f295dcd64c81dfef5b3f5a3eb2a82751ad1\n",
            "04/09/2020 16:48:42 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/09/2020 16:48:42 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /root/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.fc076a4d5f1edf25ea3a2bd66e9f6f295dcd64c81dfef5b3f5a3eb2a82751ad1\n",
            "04/09/2020 16:48:42 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/09/2020 16:48:43 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /root/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/09/2020 16:48:43 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
            "04/09/2020 16:49:28 - INFO - transformers.modeling_utils -   Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "04/09/2020 16:49:28 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "04/09/2020 16:49:32 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='bert-large-uncased', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=2.0, output_dir='/models/wwm_uncased_finetuned_squad1/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=12, per_gpu_train_batch_size=12, predict_file='/content/dev-v1.1.json', save_steps=500, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file='/content/train-v1.1.json', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
            "04/09/2020 16:49:32 - INFO - __main__ -   Creating features from dataset file at .\n",
            "100% 442/442 [00:28<00:00, 15.52it/s]\n",
            "convert squad examples to features: 100% 87599/87599 [10:55<00:00, 133.73it/s]\n",
            "add example index and unique id: 100% 87599/87599 [00:00<00:00, 879151.70it/s]\n",
            "04/09/2020 17:01:00 - INFO - __main__ -   Saving features into cached file ./cached_train_bert-large-uncased_384\n",
            "tcmalloc: large alloc 1179140096 bytes == 0x7f12ac9ca000 @  0x7f14ee07b2a4 0x592727 0x4dddf7 0x4e2667 0x4e2aab 0x4e30d0 0x4e14d8 0x4e2c5b 0x4e2a3b 0x4e30d0 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50d390 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245\n",
            "04/09/2020 17:03:59 - INFO - __main__ -   ***** Running training *****\n",
            "04/09/2020 17:03:59 - INFO - __main__ -     Num examples = 88641\n",
            "04/09/2020 17:03:59 - INFO - __main__ -     Num Epochs = 2\n",
            "04/09/2020 17:03:59 - INFO - __main__ -     Instantaneous batch size per GPU = 12\n",
            "04/09/2020 17:03:59 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "04/09/2020 17:03:59 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "04/09/2020 17:03:59 - INFO - __main__ -     Total optimization steps = 14774\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/7387 [00:00<?, ?it/s]\u001b[ATraceback (most recent call last):\n",
            "  File \"transformers/examples/run_squad.py\", line 829, in <module>\n",
            "    main()\n",
            "  File \"transformers/examples/run_squad.py\", line 768, in main\n",
            "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
            "  File \"transformers/examples/run_squad.py\", line 204, in train\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\", line 1480, in forward\n",
            "    inputs_embeds=inputs_embeds,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\", line 790, in forward\n",
            "    encoder_attention_mask=encoder_extended_attention_mask,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\", line 407, in forward\n",
            "    hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\", line 379, in forward\n",
            "    intermediate_output = self.intermediate(attention_output)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\", line 331, in forward\n",
            "    hidden_states = self.dense(hidden_states)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\", line 87, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 1372, in linear\n",
            "    output = input.matmul(weight.t())\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 15.90 GiB total capacity; 15.05 GiB already allocated; 51.88 MiB free; 15.06 GiB reserved in total by PyTorch)\n",
            "\n",
            "Epoch:   0% 0/2 [00:01<?, ?it/s]\n",
            "Iteration:   0% 0/7387 [00:01<?, ?it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJuriyfl8-O3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}