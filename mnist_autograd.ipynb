{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_autograd.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRXjapBDdvk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Value:\n",
        "    \"\"\" stores a value and its gradient \"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "            \n",
        "            \n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "    \n",
        "    \n",
        "        \n",
        "        \n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "        def _backward():\n",
        "            \n",
        "            \n",
        "            self.grad += (other * self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "        \n",
        "        out = Value(np.where(self.data < 0, 0, self.data), (self,), 'ReLU')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += np.where(out.data > 0,1,0) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "    def matmul(self,other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(np.matmul(self.data , other.data), (self, other), 'matmul')\n",
        "        def _backward():\n",
        "            self.grad += np.dot(out.grad,other.data.T)\n",
        "            other.grad += np.dot(self.data.T,out.grad)\n",
        "            \n",
        "            \n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "    def softmax(self):\n",
        "\n",
        "        out =  Value(np.exp(self.data) / np.sum(np.exp(self.data), axis=1)[:, None], (self,), 'softmax')\n",
        "        softmax = out.data\n",
        "        def _backward():\n",
        "            self.grad += (out.grad - np.reshape(\n",
        "            np.sum(out.grad * softmax, 1),\n",
        "            [-1, 1]\n",
        "              )) * softmax\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def log(self):\n",
        "        #print(self.data==0.0)\n",
        "        \"\"\"\n",
        "        if len(list(zip(*np.where(self.data == 0.0))))!=0:\n",
        "            print(self.data)\n",
        "        \"\"\"\n",
        "        out = Value(np.log(self.data),(self,),'log')\n",
        "        def _backward():\n",
        "            self.grad += out.grad/self.data\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "    \n",
        "    \n",
        "    def reduce_sum(self,axis = None):\n",
        "        out = Value(np.sum(self.data,axis = axis), (self,), 'REDUCE_SUM')\n",
        "        \n",
        "        def _backward():\n",
        "            output_shape = np.array(self.data.shape)\n",
        "            output_shape[axis] = 1\n",
        "            tile_scaling = self.data.shape // output_shape\n",
        "            grad = np.reshape(out.grad, output_shape)\n",
        "            self.grad += np.tile(grad, tile_scaling)\n",
        "            \n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            #print(v)\n",
        "            v._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRWaB--7d4zs",
        "colab_type": "code",
        "outputId": "f2cd06e1-7700-4a5b-a4d0-42de9c907bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import keras\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfi2rgohd-8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "train_images = np.asarray(x_train, dtype=np.float32) / 255.0\n",
        "test_images = np.asarray(x_test, dtype=np.float32) / 255.0\n",
        "train_images = train_images.reshape(60000,784)\n",
        "test_images = test_images.reshape(10000,784)\n",
        "y_train = keras.utils.to_categorical(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcdeKXE2e0T7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_loss(X,Y,W):\n",
        "  \n",
        "  return -(1/X.shape[0])*np.sum(np.sum(Y*np.log(np.exp(np.matmul(X,W)) / np.sum(np.exp(np.matmul(X,W)),axis=1)[:, None]),axis = 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnjjkOcYd-_c",
        "colab_type": "code",
        "outputId": "8df39745-15c7-4e4f-b331-5b6f39700acf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "batch_size = 32\n",
        "steps = 40000\n",
        "Wb = Value(np.random.randn(784,10))# new initialized weights for gradient descent\n",
        "for step in range(steps):\n",
        "  ri = np.random.permutation(train_images.shape[0])[:batch_size]\n",
        "  Xb, yb = Value(train_images[ri]), Value(y_train[ri])\n",
        "  y_predW = Xb.matmul(Wb)\n",
        "  probs = y_predW.softmax()\n",
        "\n",
        "  log_probs = probs.log()\n",
        "  \n",
        "  zb = yb*log_probs\n",
        "\n",
        "  outb = zb.reduce_sum(axis = 1)\n",
        "  finb = -(1/batch_size)*outb.reduce_sum()  #cross entropy loss\n",
        "  finb.backward()\n",
        "  if step%1000==0:\n",
        "    loss = calculate_loss(train_images,y_train,Wb.data)\n",
        "    print(f'loss in step {step} is {loss}')\n",
        "  Wb.data = Wb.data- 0.01*Wb.grad\n",
        "  Wb.grad = 0\n",
        "loss = calculate_loss(train_images,y_train,Wb.data)\n",
        "print(f'loss in final step {step+1} is {loss}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss in step 0 is 11.965824562043244\n",
            "loss in step 1000 is 3.5340812250235207\n",
            "loss in step 2000 is 2.3862026312303235\n",
            "loss in step 3000 is 1.8992034209586457\n",
            "loss in step 4000 is 1.6277956004915592\n",
            "loss in step 5000 is 1.4518542304774498\n",
            "loss in step 6000 is 1.3296696811452746\n",
            "loss in step 7000 is 1.237307320629226\n",
            "loss in step 8000 is 1.1640020398560544\n",
            "loss in step 9000 is 1.1050946004669735\n",
            "loss in step 10000 is 1.0552722325223898\n",
            "loss in step 11000 is 1.0135529844636009\n",
            "loss in step 12000 is 0.9776317683814056\n",
            "loss in step 13000 is 0.9462063287287965\n",
            "loss in step 14000 is 0.9188650484572274\n",
            "loss in step 15000 is 0.8939651384373788\n",
            "loss in step 16000 is 0.8716843701320222\n",
            "loss in step 17000 is 0.8516149151684732\n",
            "loss in step 18000 is 0.8326553726649465\n",
            "loss in step 19000 is 0.8159560798766764\n",
            "loss in step 20000 is 0.7998413369543754\n",
            "loss in step 21000 is 0.7850434500979272\n",
            "loss in step 22000 is 0.7710236421713172\n",
            "loss in step 23000 is 0.7590027179646724\n",
            "loss in step 24000 is 0.7464614947089783\n",
            "loss in step 25000 is 0.7354912118580088\n",
            "loss in step 26000 is 0.7250937512103576\n",
            "loss in step 27000 is 0.7150674916953877\n",
            "loss in step 28000 is 0.7062630774047437\n",
            "loss in step 29000 is 0.6973919685406473\n",
            "loss in step 30000 is 0.6892837006637575\n",
            "loss in step 31000 is 0.6806314692449069\n",
            "loss in step 32000 is 0.6736668910464348\n",
            "loss in step 33000 is 0.6662304421711187\n",
            "loss in step 34000 is 0.6597650576137385\n",
            "loss in step 35000 is 0.6536405261953652\n",
            "loss in step 36000 is 0.6464428537730077\n",
            "loss in step 37000 is 0.6396620161257461\n",
            "loss in step 38000 is 0.6335093537502491\n",
            "loss in step 39000 is 0.6272556504572979\n",
            "loss in final step 40000 is 0.6233569738744157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_bw9LyVeGPV",
        "colab_type": "code",
        "outputId": "0b30374f-54f2-4e83-9763-bd250fdfcf3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(f'accuracy on test data is {accuracy_score(np.argmax(np.matmul(test_images,Wb.data),axis = 1),y_test)*100} %')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy on test data is 86.74 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un4gCVuTmWtY",
        "colab_type": "code",
        "outputId": "0379c98b-62cc-476d-f43d-d213257fb0d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(np.argmax(np.exp(np.matmul(test_images,Wb.data)) / np.sum(np.exp(np.matmul(test_images,Wb.data)), axis=1)[:, None],axis = 1),y_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8674"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt2vcquCpbTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYxvTB3bpbQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_loss1(X,Y,W1,W2):\n",
        "  y1 = np.where(np.matmul(X,W1)<0,0,np.matmul(X,W1))\n",
        "  prob = np.exp(np.matmul(y1,W2)) / np.sum(np.exp(np.matmul(y1,W2)),axis=1)[:, None]\n",
        "\n",
        "  return -(1/X.shape[0])*np.sum(np.sum(Y*np.log(prob),axis = 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcovk2qlnEmc",
        "colab_type": "code",
        "outputId": "4e494f2a-dbe3-421c-ca85-0b4f7ca78027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "batch_size = 32\n",
        "steps = 20000\n",
        "Wb1 = Value(np.random.randn(784,128))\n",
        "Wb2 = Value(np.random.randn(128,10))# new initialized weights for gradient descent\n",
        "for step in range(steps):\n",
        "  ri = np.random.permutation(train_images.shape[0])[:batch_size]\n",
        "  Xb, yb = Value(train_images[ri]), Value(y_train[ri])\n",
        "  y_predW1 = Xb.matmul(Wb1).relu()\n",
        "  y_predW = y_predW1.matmul(Wb2)\n",
        "  \n",
        "  probs = y_predW.softmax()\n",
        "\n",
        "  log_probs = probs.log()\n",
        "\n",
        "  zb = yb*log_probs\n",
        "\n",
        "  outb = zb.reduce_sum(axis = 1)\n",
        "  finb = -(1/batch_size)*outb.reduce_sum()  #cross entropy loss\n",
        "  finb.backward()\n",
        "  if step%1000==0:\n",
        "    loss = calculate_loss1(train_images,y_train,Wb1.data,Wb2.data)\n",
        "    print(f'loss in step {step} is {loss}')\n",
        "  Wb1.data = Wb1.data- 0.01*Wb1.grad\n",
        "  Wb2.data = Wb2.data- 0.01*Wb2.grad\n",
        "  Wb1.grad = 0\n",
        "  Wb2.grad = 0\n",
        "loss = calculate_loss1(train_images,y_train,Wb1.data,Wb2.data)\n",
        "print(f'loss in final step {step+1} is {loss}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss in step 0 is 104.55093089161471\n",
            "loss in step 1000 is 4.801904007411178\n",
            "loss in step 2000 is 3.2042965135586274\n",
            "loss in step 3000 is 2.4715316092511848\n",
            "loss in step 4000 is 2.025001506053377\n",
            "loss in step 5000 is 1.763347535093437\n",
            "loss in step 6000 is 1.5327570850072534\n",
            "loss in step 7000 is 1.370981805304221\n",
            "loss in step 8000 is 1.2276269130843802\n",
            "loss in step 9000 is 1.1433440024735015\n",
            "loss in step 10000 is 1.0325585242117796\n",
            "loss in step 11000 is 0.9887109268906167\n",
            "loss in step 12000 is 0.9003985043834779\n",
            "loss in step 13000 is 0.8428990153052753\n",
            "loss in step 14000 is 0.7845594497537397\n",
            "loss in step 15000 is 0.7633394953693724\n",
            "loss in step 16000 is 0.6938871874269439\n",
            "loss in step 17000 is 0.6564890083807886\n",
            "loss in step 18000 is 0.649455646441251\n",
            "loss in step 19000 is 0.6222885484394804\n",
            "loss in final step 20000 is 0.5905740356817294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awEB1ChDoC8J",
        "colab_type": "code",
        "outputId": "ed103244-a563-4709-ea4c-dd9f98a267e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y1 = np.where(np.matmul(test_images,Wb1.data)<0,0,np.matmul(test_images,Wb1.data))\n",
        "prob = np.exp(np.matmul(y1,Wb2.data)) / np.sum(np.exp(np.matmul(y1,Wb2.data)),axis=1)[:, None]\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(np.argmax(prob,axis = 1),y_test)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9068"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}