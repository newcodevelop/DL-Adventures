{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Early Stopping XNLI Testings.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BzqiIMgJrq-w"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPy5uglhqQYQ",
        "colab_type": "text"
      },
      "source": [
        "*   **Use iitp.baban Google Drive**\n",
        "*   **SEED = 42** \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1eNCKuMqV_S",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyZMo5A8t2GK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "7ee4cd77-96a0-4ba4-93f3-48fa9aebdf67"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install bert-tensorflow\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3hC_qnvt5SJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ba33382f-d40e-423e-dd50-7626fbae51c4"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSZKm3PjqfKw",
        "colab_type": "text"
      },
      "source": [
        "# BERT Pretrained Model Download "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQsRqD2Ht7JY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6be2f945-235e-4991-9a36-1856863b093b"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
        "!unzip multi_cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-17 18:37:54--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 2404:6800:4008:c01::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662903077 (632M) [application/zip]\n",
            "Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "multi_cased_L-12_H- 100%[===================>] 632.19M  76.2MB/s    in 9.4s    \n",
            "\n",
            "2020-06-17 18:38:05 (67.1 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n",
            "\n",
            "Archive:  multi_cased_L-12_H-768_A-12.zip\n",
            "   creating: multi_cased_L-12_H-768_A-12/\n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuKw9eWfrpv2",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Loading (Text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxdLFOYAolBH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "dad7ed7c-a64f-4ae9-a869-3b44e29eab69"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip\n",
        "!unzip XNLI-1.0.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-17 18:38:25--  https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17865352 (17M) [application/zip]\n",
            "Saving to: ‘XNLI-1.0.zip’\n",
            "\n",
            "XNLI-1.0.zip        100%[===================>]  17.04M  7.02MB/s    in 2.4s    \n",
            "\n",
            "2020-06-17 18:38:28 (7.02 MB/s) - ‘XNLI-1.0.zip’ saved [17865352/17865352]\n",
            "\n",
            "Archive:  XNLI-1.0.zip\n",
            "   creating: XNLI-1.0/\n",
            "  inflating: XNLI-1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/XNLI-1.0/\n",
            "  inflating: __MACOSX/XNLI-1.0/._.DS_Store  \n",
            "  inflating: XNLI-1.0/xnli.dev.tsv   \n",
            "  inflating: __MACOSX/XNLI-1.0/._xnli.dev.tsv  \n",
            "  inflating: XNLI-1.0/xnli.dev.jsonl  \n",
            "  inflating: XNLI-1.0/README.md      \n",
            "  inflating: __MACOSX/XNLI-1.0/._README.md  \n",
            "  inflating: XNLI-1.0/xnli.test.jsonl  \n",
            "  inflating: XNLI-1.0/xnli.test.tsv  \n",
            "  inflating: __MACOSX/XNLI-1.0/._xnli.test.tsv  \n",
            "  inflating: __MACOSX/._XNLI-1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id1H4EarpYaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('XNLI-1.0/xnli.test.tsv',sep = '\\t')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmt_DkfXqzBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_fr = df[(df['language'] == 'fr') ].head(5000)\n",
        "df_de = df[(df['language'] == 'de') ].head(5000)\n",
        "df_tr = df[(df['language'] == 'tr') ].head(5000)\n",
        "df_bg = df[(df['language'] == 'bg') ].head(5000)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JzsFQYRridp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_fr_train,df_fr_test = train_test_split(df_fr, test_size=0.1,random_state = SEED,shuffle = True)\n",
        "df_de_train,df_de_test = train_test_split(df_de, test_size=0.1,random_state = SEED,shuffle = True)\n",
        "df_tr_train,df_tr_test = train_test_split(df_tr, test_size=0.1,random_state = SEED,shuffle = True)\n",
        "df_bg_train,df_bg_test = train_test_split(df_bg, test_size=0.1,random_state = SEED,shuffle = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65ZPFUj2t-86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_for_label(a,b,lang_pair):\n",
        "  pairs = lang_pair.split('-')\n",
        "  b_ = list(a['gold_label'])\n",
        "  assert b_== list(b['gold_label'])\n",
        "  lab = []\n",
        "  \"\"\"\n",
        "  lab  = []\n",
        "  for i in b_:\n",
        "    lab.append(i-1)\n",
        "  \"\"\"\n",
        "  for i in b_:\n",
        "    if i=='contradiction':\n",
        "        lab.append(0)\n",
        "        \n",
        "    elif i=='neutral':\n",
        "        lab.append(1)\n",
        "    elif i== 'entailment':\n",
        "        lab.append(2)\n",
        "    \n",
        "  sentence_1 = list(a['sentence1'])\n",
        "  sentence_2 = list(b['sentence2'])\n",
        "  raw_data_train = {'sentence1_{}'.format(pairs[0]): sentence_1, \n",
        "              'sentence2_{}'.format(pairs[1]): sentence_2,\n",
        "          'label': lab}\n",
        "  df = pd.DataFrame(raw_data_train, columns = ['sentence1_{}'.format(pairs[0]),'sentence2_{}'.format(pairs[1]),'label'])\n",
        "  return df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1afxHh59QYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select Language Pair for Training\n",
        "train_data = get_data_for_label(df_de_train,df_tr_train,'de-tr')\n",
        "test_data =  get_data_for_label(df_de_test,df_tr_test,'de-tr')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn_yth2r9Mxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(a,b,lang_pair):\n",
        "  pairs = lang_pair.split('-')\n",
        "  b_ = list(a['gold_label'])\n",
        "  assert b_== list(b['gold_label'])\n",
        "  lab = []\n",
        "  \"\"\"\n",
        "  lab  = []\n",
        "  for i in b_:\n",
        "    lab.append(i-1)\n",
        "  \"\"\"\n",
        "  for i in b_:\n",
        "    if i=='contradiction':\n",
        "        lab.append(0)\n",
        "        \n",
        "    elif i=='neutral':\n",
        "        lab.append(1)\n",
        "    elif i== 'entailment':\n",
        "        lab.append(2)\n",
        "    \n",
        "  sentence_1 = list(a['sentence1'])\n",
        "  sentence_2 = list(b['sentence2'])\n",
        "  raw_data_train = {'sentence1_{}'.format(pairs[0]): sentence_1, \n",
        "              'sentence2_{}'.format(pairs[1]): sentence_2,\n",
        "          'label': lab}\n",
        "  df = pd.DataFrame(raw_data_train, columns = ['sentence1_{}'.format(pairs[0]),'sentence2_{}'.format(pairs[1]),'label'])\n",
        "  return df\n",
        "\n",
        "def get_features(lang_pair):\n",
        "  features = {}\n",
        "  \n",
        "  features[lang_pair+'_train'] = 0\n",
        "  features[lang_pair+'_test'] = 0\n",
        "  lang_dict = {'fr':[df_fr_train,df_fr_test],'de':[df_de_train,df_de_test],'tr':[df_tr_train,df_tr_test],'bg':[df_bg_train,df_bg_test]}\n",
        "  pairs = lang_pair.split('-')\n",
        "  lang1_train,lang2_train = lang_dict[pairs[0]][0],lang_dict[pairs[1]][0]\n",
        "  lang1_test,lang2_test = lang_dict[pairs[0]][1],lang_dict[pairs[1]][1]\n",
        "  features[lang_pair+'_train'] = get_data(lang1_train,lang2_train,lang_pair)\n",
        "  features[lang_pair+'_test'] = get_data(lang1_test,lang2_test,lang_pair)\n",
        "  features[lang_pair+'_train'] = features[lang_pair+'_train'].apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x['sentence1_{}'.format(pairs[0])], \n",
        "                                                                   text_b = x['sentence2_{}'.format(pairs[1])], \n",
        "                                                                   label = x['label']), axis = 1)\n",
        "  features[lang_pair+'_test'] = features[lang_pair+'_test'].apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x['sentence1_{}'.format(pairs[0])], \n",
        "                                                                   text_b = x['sentence2_{}'.format(pairs[1])], \n",
        "                                                                   label = x['label']), axis = 1)\n",
        "  vocab_file = \"multi_cased_L-12_H-768_A-12/vocab.txt\"\n",
        "  label_list = [0,1,2]\n",
        "  tokenizer = bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=True)\n",
        "  MAX_SEQ_LENGTH = 128\n",
        "\n",
        "  features[lang_pair+'_train'] = bert.run_classifier.convert_examples_to_features(features[lang_pair+'_train'], label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  features[lang_pair+'_test'] = bert.run_classifier.convert_examples_to_features(features[lang_pair+'_test'], label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  return features"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TeXRX6Ajm_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca4db3bd-275a-4177-cc66-7d744a0f7121"
      },
      "source": [
        "features = get_features('de-tr')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Writing example 0 of 4500\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] du vers ##te ##hs ##t bereits die bed ##eutu ##ng von ge ##schicht ##ener ##zahlen , po ##esie , ge ##sang und theater , um empat ##hie , mit ##ge ##fu ##hl und die vor ##stellung ##sk ##raft zu ford ##ern . [SEP] o ##yku an ##lat ##ma , si ##ir , sa ##rk ##ı ve ti ##yatro ve empat ##i kur ##ma , mer ##ham ##et ve haya ##l gu ##cun ##un te ##s ##vik edi ##lmesi ##nde kes ##in ##likle one ##ms ##iz ##dir ve ise ya ##rama ##z . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10168 12576 10216 22394 10123 13506 10128 30113 82039 10376 10166 46503 110534 36446 106717 117 10514 86258 117 46503 45952 10130 33462 117 10293 30593 72287 117 10221 10525 20758 17054 10130 10128 11190 29526 11478 49448 10304 57821 13979 119 102 183 83496 10151 16698 10369 117 10294 10835 117 10148 16299 10713 10323 14382 73279 10323 30593 10116 19830 10369 117 13697 13196 10308 10323 57069 10161 75980 48917 11107 10361 10107 21533 105861 80008 11382 21388 10245 92067 10464 12387 15834 11957 10323 14680 10549 46582 10305 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] . . . die aufmerksam ##sten und motivi ##eren ##sten natura ##utore ##n der wel ##t . [SEP] hi ##c kim ##se dog ##ay ##la ilgili ya ##zı ya ##zm ##ı ##yor . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 119 119 119 10128 110837 12587 10130 58456 13653 12587 30009 97402 10115 10118 16829 10123 119 102 11520 10350 38516 10341 17835 13998 10330 48009 10549 44047 10549 37661 10713 26101 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] h ##mm , nei ##n , ehr ##lich ge ##sagt habe ich noch nie eines der bu ##cher gel ##esen , die ich lesen sollte . [SEP] 100 say ##fa ##dan daha uzun hi ##c ##bir kita ##p ok ##uma ##dı ##m . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 176 17525 117 11888 10115 117 92197 11666 46503 74849 21404 12979 11230 11058 11655 10118 11499 13396 74458 25012 117 10128 12979 106234 17799 119 102 10407 23763 13369 12146 13878 31975 11520 10350 29241 40091 10410 14302 16746 17532 10147 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] du mach ##st zu viel davon . [SEP] bir se ##y ya ##rat ##ı ##yor ##sun . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10168 109664 10562 10304 23596 20816 119 102 10561 10126 10157 10549 12553 10713 26101 24883 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 2 (id = 2)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] beide konnen vera ##nder ##t werden , ohne den antico ##don - co ##don - an ##pass ##ung ##sme ##chan ##ismus zu vera ##nder ##n . [SEP] kar ##sı ##t diz ##gi - diz ##gi es ##les ##tir ##me me ##kan ##izma ##sı de ##gist ##iri ##lme ##den bun ##ların hi ##c ##bir ##i de ##gist ##iri ##lemez . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 17222 73782 25948 16497 10123 10615 117 15032 10140 36232 15081 118 11170 15081 118 10151 36388 10716 17228 47019 22747 10304 25948 16497 10115 119 102 25085 13836 10123 36897 11210 118 36897 11210 10196 11268 18330 10627 10911 10706 84063 13836 10104 77362 19334 53048 10633 59230 15628 11520 10350 29241 10116 10104 77362 19334 94168 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Writing example 0 of 500\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] wie kann ein el ##tern ##teil den unter ##schi ##ed zwischen einer sprach ##stor ##ung und der normale ##n sprach ##ent ##wicklung fest ##stellen ? [SEP] bir e ##be ##vey ##n , iki ya ##sında ##ki co ##cu ##kla konu ##sama ##manı ##n ano ##rmal oldu ##gun ##u dus ##unu ##r . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10953 12382 10290 10125 26999 29517 10140 11124 63565 10336 11788 10599 74841 42396 10716 10130 10118 29923 10115 74841 11405 56817 34519 28211 136 102 10561 173 11044 50038 10115 117 12322 10549 25666 10506 11170 12352 22444 98881 62009 68455 10115 12797 66619 17312 32657 10138 20239 28041 10129 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] die board ab ##satz 605 ( b ) ze ##rti ##fik ##ate wurden dem chef ##bera ##ter fur int ##ressen berat ##ung der klein unter ##nehmen ##s ver ##walt ##ung ( s ##ba ) nicht se ##pera ##t vor ##gelegt , mein ##te ein board offizielle ##r [SEP] kuru ##l , s ##ba sert ##if ##ikal ##arın ##ı ver ##medi ve bunu de ##ger ##lendi ##rici ##nin of ##isin ##e b ##ır ##akt ##ı . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10128 17936 11357 39403 48141 113 170 114 10941 28304 51892 12436 10995 10268 15158 62183 10877 61001 26391 105258 60954 10716 10118 29185 11124 36771 10107 16719 57710 10716 113 187 10537 114 10726 10126 37097 10123 11190 32534 117 70273 10216 10290 17936 69573 10129 102 48350 10161 117 187 10537 38005 13918 52564 88638 10713 16719 96092 10323 79471 10104 11446 72384 30558 11802 10108 33560 10112 170 17145 45522 10713 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] schließlich scheint die post ##dic ##hte ein wichtiger ##er faktor fur die vers ##and ##kosten der ein ##heit ##sstraße zu sein als das volumen u ##ber die tat ##sa ##chliche ##n ent ##fern ##ungen in fra ##nk ##reich und den usa . [SEP] posta yo ##gun ##lug ##u ul ##ast ##ır ##ma mali ##yet ##lerinin buy ##uk ##lug ##une iki kat etki edi ##yor . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 19123 66373 10128 11841 55170 24819 10290 88239 10165 52221 61001 10128 12576 14752 92316 10118 10290 15543 75983 10304 11479 10223 10242 56889 189 12212 10128 22795 10466 83220 10115 61047 59798 13692 10106 10628 17761 31922 10130 10140 15610 119 102 29175 13672 32657 67381 10138 16600 15171 17145 10369 38981 33878 31268 47715 13013 67381 19659 12322 27689 47033 105861 26101 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] jede hoher ##e form des denken ##s , so beton ##te er , erscheint zuerst in der sozialen kommun ##ikat ##ion , zwischen dem kind und den vert ##rete ##rn seiner kultur , wenn sie eine gemeinsame aktiv ##itat aus ##uben . [SEP] ortak aktivite ##leri pay ##las ##mak daha us ##t du ##zey dus ##un ##mele ##ri pay ##las ##mak ici ##n yardım ##cı ol ##uy ##or . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 45278 61207 10112 12188 10139 91682 10107 117 10380 105375 10216 10163 117 47519 48676 10106 10118 64203 15484 49295 11046 117 11788 10268 22282 10130 10140 20900 34942 13061 11411 46774 117 16082 10632 10359 65823 22275 16313 10441 66918 119 102 95045 39720 12538 16868 13983 23892 13878 19626 10123 10168 68060 20239 11107 81612 10401 16868 13983 23892 42032 10115 79346 18311 30668 53452 10667 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 2 (id = 2)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] und es machte mir immer ##no ##ch ang ##st [SEP] sadece bir ##az ##cı ##k kor ##km ##ust ##um . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10130 10196 24607 36077 15967 10343 10269 10488 10562 102 39958 10561 16724 18311 10174 33705 23440 19265 10465 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccNTm7l5j-KT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2b01676-cddc-4403-8e5e-76a1ed6e24f7"
      },
      "source": [
        "features.keys()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['de-tr_train', 'de-tr_test'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzqiIMgJrq-w",
        "colab_type": "text"
      },
      "source": [
        "# CLTE-BERT Custom Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVk6Z7w4uo8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = bert.run_classifier.modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use model.get_sequence_output()\n",
        "  # instead.\n",
        "  output_layer = model.get_pooled_output()\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities,predicted_labels,output_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities,predicted_labels,hidden_context) = create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = bert.run_classifier.modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    \"\"\"\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "    \"\"\"\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "       \n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(per_example_loss, label_ids, logits, is_real_example)\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities,\"labels\": predicted_labels, \"hidden_context\": hidden_context})\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxarj7wkrq5j",
        "colab_type": "text"
      },
      "source": [
        "# CLTE-Progressive-BERT Custom Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESrCwHX2usTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_progressive(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings,hidden_context):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = bert.run_classifier.modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use model.get_sequence_output()\n",
        "  # instead.\n",
        "  output_layer = model.get_pooled_output()\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "  #kd_loss_weight = tf.get_variable('KLD', initializer=tf.constant(0.2))\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "\n",
        "    output_layer_probs = tf.nn.softmax(output_layer,axis = -1)\n",
        "    #loss = y_true * log(y_true / y_pred)\n",
        "    hidden_context = tf.nn.softmax(hidden_context,axis = -1)\n",
        "    per_example_kd_loss = tf.keras.losses.KLD(hidden_context,output_layer_probs)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "\n",
        "    kd_loss_weight = 0.2 #hyperparameter\n",
        "    per_example_kd_loss = kd_loss_weight*per_example_kd_loss\n",
        "\n",
        "    per_example_loss += per_example_kd_loss\n",
        "\n",
        "    \n",
        "\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities,predicted_labels,kd_loss_weight)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def model_fn_builder_progressive(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    hidden_context = features[\"hidden_context\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities,predicted_labels,KLD) = create_model_progressive(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings,hidden_context)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = bert.run_classifier.modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    \"\"\"\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    \"\"\"\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(per_example_loss, label_ids, logits, is_real_example)\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities,\"labels\": predicted_labels,\"kl-param\": KLD})\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uNc6dcsrqia",
        "colab_type": "text"
      },
      "source": [
        "# Input Functions\n",
        "\n",
        "1.   CLTE-BERT\n",
        "2.   CLTE-BERT with Image\n",
        "3.   CLTE-BERT-Progressive with Image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n8-XpQ5uxFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn_builder(features, hidden_context,seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "    hidden_shape = hidden_context.shape[-1]\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "\n",
        "        \"hidden_context\":\n",
        "            tf.constant(hidden_context, shape = [num_examples,hidden_shape], dtype = tf.float32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "\n",
        "def input_fn_builder_img(img_features,features,seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "    hidden_shape_img = img_features.shape[-1]\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "\n",
        "        \"img_features\":\n",
        "            tf.constant(img_features, shape = [num_examples,hidden_shape_img], dtype = tf.float32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "def input_fn_builder_pr_img(img_features,features,hidden_context,seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "    hidden_shape_img = img_features.shape[-1]\n",
        "    hidden_shape = hidden_context.shape[-1]\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "\n",
        "        \"img_features\":\n",
        "            tf.constant(img_features, shape = [num_examples,hidden_shape_img], dtype = tf.float32),\n",
        "\n",
        "        \"hidden_context\":\n",
        "            tf.constant(hidden_context, shape = [num_examples,hidden_shape], dtype = tf.float32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiMTHcFauCz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L = [] #global list of eval accuracy for early stopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11ROr-6Eh3KL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ValidationHook(tf.train.SessionRunHook):\n",
        "    def __init__(self, estimator, input_fn,\n",
        "                 every_n_secs=None, every_n_steps=None):\n",
        "        self._iter_count = 0\n",
        "        self._estimator = estimator\n",
        "        self._input_fn = input_fn\n",
        "        self._timer = tf.train.SecondOrStepTimer(every_n_secs, every_n_steps)\n",
        "        self._should_trigger = False\n",
        "\n",
        "    def begin(self):\n",
        "        self._timer.reset()\n",
        "        self._iter_count = 0\n",
        "\n",
        "    def before_run(self, run_context):\n",
        "        self._should_trigger = self._timer.should_trigger_for_step(self._iter_count)\n",
        "\n",
        "    def after_run(self, run_context, run_values):\n",
        "        if self._should_trigger:\n",
        "            vals = self._estimator.evaluate(\n",
        "                self._input_fn\n",
        "            )\n",
        "            vals = vals['eval_accuracy']\n",
        "            print(f'************EVAL ACC  {vals}*****************')\n",
        "            print(f'***********EVAL STEP COUNT {self._iter_count}****************')\n",
        "            if vals > 0.52:\n",
        "              print(f'vals is {vals}')\n",
        "              raise ValueError(\"model stopped training from earlystoppig\")\n",
        "            self._timer.update_last_triggered_step(self._iter_count)\n",
        "        self._iter_count += 1"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg6XMNnzrKn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf out_dir_train_eng_pr"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJqVRXAurqgj",
        "colab_type": "text"
      },
      "source": [
        "# Trainer Functions for BERT (With and Without Image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trvxhdrguz2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epochs = 10 # Number of Training Epochs \n",
        "\n",
        "\n",
        "def train(output_dir,input_fn,eval_input_fn,input_fn_builder_progressive = False,hidden_context = None):\n",
        "  CONFIG_FILE = \"multi_cased_L-12_H-768_A-12/bert_config.json\"\n",
        "  INIT_CHECKPOINT = \"multi_cased_L-12_H-768_A-12/bert_model.ckpt\"\n",
        "  MAX_SEQ_LENGTH = 128\n",
        "  BATCH_SIZE = 28\n",
        "  LEARNING_RATE = 2e-5\n",
        "  NUM_TRAIN_EPOCHS = Epochs\n",
        "  # Warmup is a period of time where hte learning rate \n",
        "  # is small and gradually increases--usually helps training.\n",
        "  WARMUP_PROPORTION = 0.1\n",
        "  # Model configs\n",
        "  \n",
        "  SAVE_SUMMARY_STEPS = 100\n",
        "  OUTPUT_DIR = output_dir\n",
        "  # Compute # train and warmup steps from batch size\n",
        "  num_train_steps = int(len(input_fn) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "  #num_warmup_steps = 80\n",
        "  print(f'num warmup steps {num_warmup_steps }')\n",
        "  SAVE_CHECKPOINTS_STEPS = num_train_steps/NUM_TRAIN_EPOCHS\n",
        "  print(num_train_steps)\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      model_dir=OUTPUT_DIR,\n",
        "      save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "\n",
        "  # Specify outpit directory and number of checkpoint steps to save\n",
        "  if input_fn_builder_progressive==False:\n",
        "  \n",
        "\n",
        "\n",
        "    model_fn = model_fn_builder(\n",
        "      bert_config=bert.run_classifier.modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "      num_labels=4, #number of unique labels\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=False,\n",
        "      use_one_hot_embeddings=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "  \n",
        "  \n",
        "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "        features=input_fn,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "    \n",
        "\n",
        "  else:\n",
        "\n",
        "    model_fn_pr = model_fn_builder_progressive(\n",
        "      bert_config=bert.run_classifier.modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "      num_labels=4, #number of unique labels\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=False,\n",
        "      use_one_hot_embeddings=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn_pr,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "  \n",
        "    train_input_fn = input_fn_builder(\n",
        "        features=input_fn,\n",
        "        hidden_context=hidden_context,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "    test_input_fn = input_fn_builder(\n",
        "      features=eval_input_fn,\n",
        "      hidden_context=np.random.randn(500,768),\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "\n",
        "  print(f'Beginning Training!')\n",
        "  %timeit\n",
        "\n",
        "  #estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  try:\n",
        "    estimator.train(input_fn=train_input_fn,\n",
        "                hooks=[ValidationHook(estimator,test_input_fn,every_n_steps =SAVE_CHECKPOINTS_STEPS)])\n",
        "  except:\n",
        "    print('stopped')\n",
        "  return estimator\n",
        "\n",
        "\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdTSGWdNeRwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNzleH_urqeW",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Functions for BERT (With and Without Image)\n",
        "\n",
        "*   CTX = 0 for English Premise and Hindi Hypothesis\n",
        "*   CTX = 1 for Hindi Premise and English Hypothesis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbhGSEfgu2DE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def evaluate_and_get_hidden_context(estimator,input_fn_for_test,input_fn_for_hidden,is_progressive = False,hidden_context=None):\n",
        "  MAX_SEQ_LENGTH = 128\n",
        "  if not is_progressive:\n",
        "    test_input_fn = run_classifier.input_fn_builder(\n",
        "      features=input_fn_for_test,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "    actual_labels = []\n",
        "    for i in test_data['label']:\n",
        "      actual_labels.append(i)\n",
        "\n",
        "    res = estimator.predict(test_input_fn)\n",
        "    predicted_labels = []\n",
        "    temp = 0\n",
        "    for i in res:\n",
        "      if temp==0:\n",
        "        print(f\"KL-Param is {i['kl-param']}\")\n",
        "        temp+=1\n",
        "\n",
        "      predicted_labels.append(i['labels'])\n",
        "    assert temp==1\n",
        "    print(f'acc {accuracy_score(actual_labels,predicted_labels)} ')\n",
        "    estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "    hidden_input_fn = run_classifier.input_fn_builder(\n",
        "        features=input_fn_for_hidden,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)\n",
        "    estimator.evaluate(input_fn=hidden_input_fn, steps=None)\n",
        "    res_ = estimator.predict(hidden_input_fn)\n",
        "    hidden_context = []\n",
        "    k = 0\n",
        "    try:\n",
        "      for i in res_:\n",
        "        #print(i['hidden_context'])\n",
        "        \n",
        "        hidden_context.append(i[\"hidden_context\"])\n",
        "        k+=1\n",
        "    except:\n",
        "      print(f'k is {k}')\n",
        "    hidden_context = np.array(hidden_context)\n",
        "    return hidden_context,actual_labels,predicted_labels\n",
        "  else:\n",
        "    test_input_fn = input_fn_builder(\n",
        "      features=input_fn_for_test,\n",
        "      hidden_context=hidden_context,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "    estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "    actual_labels = []\n",
        "    for i in test_data['label']:\n",
        "      actual_labels.append(i)\n",
        "\n",
        "    res = estimator.predict(test_input_fn)\n",
        "    predicted_labels = []\n",
        "\n",
        "    for i in res:\n",
        "      predicted_labels.append(i['labels'])\n",
        "    return actual_labels,predicted_labels\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjdlPxj0s1wD",
        "colab_type": "text"
      },
      "source": [
        "# Progressive Training on Chosen Premise and Hypothesis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFQdoMANvEq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82482ed9-7b92-485b-cd2e-783de46258af"
      },
      "source": [
        "#get hidden context from drive\n",
        "hidden_context_data = np.load('/content/gdrive/My Drive/XNLI Hidden Contexts/Hidden_Context_fr-bg_Normal.npy', allow_pickle=True)\n",
        "#get features using get_features\n",
        "features = get_features('de-tr')\n",
        "estimator = train('out_dir_train_eng_pr2', features['de-tr_train'],features['de-tr_test'], input_fn_builder_progressive = True, hidden_context = hidden_context_data)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 4500\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] du vers ##te ##hs ##t bereits die bed ##eutu ##ng von ge ##schicht ##ener ##zahlen , po ##esie , ge ##sang und theater , um empat ##hie , mit ##ge ##fu ##hl und die vor ##stellung ##sk ##raft zu ford ##ern . [SEP] o ##yku an ##lat ##ma , si ##ir , sa ##rk ##ı ve ti ##yatro ve empat ##i kur ##ma , mer ##ham ##et ve haya ##l gu ##cun ##un te ##s ##vik edi ##lmesi ##nde kes ##in ##likle one ##ms ##iz ##dir ve ise ya ##rama ##z . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10168 12576 10216 22394 10123 13506 10128 30113 82039 10376 10166 46503 110534 36446 106717 117 10514 86258 117 46503 45952 10130 33462 117 10293 30593 72287 117 10221 10525 20758 17054 10130 10128 11190 29526 11478 49448 10304 57821 13979 119 102 183 83496 10151 16698 10369 117 10294 10835 117 10148 16299 10713 10323 14382 73279 10323 30593 10116 19830 10369 117 13697 13196 10308 10323 57069 10161 75980 48917 11107 10361 10107 21533 105861 80008 11382 21388 10245 92067 10464 12387 15834 11957 10323 14680 10549 46582 10305 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] . . . die aufmerksam ##sten und motivi ##eren ##sten natura ##utore ##n der wel ##t . [SEP] hi ##c kim ##se dog ##ay ##la ilgili ya ##zı ya ##zm ##ı ##yor . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 119 119 119 10128 110837 12587 10130 58456 13653 12587 30009 97402 10115 10118 16829 10123 119 102 11520 10350 38516 10341 17835 13998 10330 48009 10549 44047 10549 37661 10713 26101 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] h ##mm , nei ##n , ehr ##lich ge ##sagt habe ich noch nie eines der bu ##cher gel ##esen , die ich lesen sollte . [SEP] 100 say ##fa ##dan daha uzun hi ##c ##bir kita ##p ok ##uma ##dı ##m . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 176 17525 117 11888 10115 117 92197 11666 46503 74849 21404 12979 11230 11058 11655 10118 11499 13396 74458 25012 117 10128 12979 106234 17799 119 102 10407 23763 13369 12146 13878 31975 11520 10350 29241 40091 10410 14302 16746 17532 10147 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] du mach ##st zu viel davon . [SEP] bir se ##y ya ##rat ##ı ##yor ##sun . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10168 109664 10562 10304 23596 20816 119 102 10561 10126 10157 10549 12553 10713 26101 24883 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 2 (id = 2)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] beide konnen vera ##nder ##t werden , ohne den antico ##don - co ##don - an ##pass ##ung ##sme ##chan ##ismus zu vera ##nder ##n . [SEP] kar ##sı ##t diz ##gi - diz ##gi es ##les ##tir ##me me ##kan ##izma ##sı de ##gist ##iri ##lme ##den bun ##ların hi ##c ##bir ##i de ##gist ##iri ##lemez . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 17222 73782 25948 16497 10123 10615 117 15032 10140 36232 15081 118 11170 15081 118 10151 36388 10716 17228 47019 22747 10304 25948 16497 10115 119 102 25085 13836 10123 36897 11210 118 36897 11210 10196 11268 18330 10627 10911 10706 84063 13836 10104 77362 19334 53048 10633 59230 15628 11520 10350 29241 10116 10104 77362 19334 94168 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Writing example 0 of 500\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] wie kann ein el ##tern ##teil den unter ##schi ##ed zwischen einer sprach ##stor ##ung und der normale ##n sprach ##ent ##wicklung fest ##stellen ? [SEP] bir e ##be ##vey ##n , iki ya ##sında ##ki co ##cu ##kla konu ##sama ##manı ##n ano ##rmal oldu ##gun ##u dus ##unu ##r . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10953 12382 10290 10125 26999 29517 10140 11124 63565 10336 11788 10599 74841 42396 10716 10130 10118 29923 10115 74841 11405 56817 34519 28211 136 102 10561 173 11044 50038 10115 117 12322 10549 25666 10506 11170 12352 22444 98881 62009 68455 10115 12797 66619 17312 32657 10138 20239 28041 10129 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] die board ab ##satz 605 ( b ) ze ##rti ##fik ##ate wurden dem chef ##bera ##ter fur int ##ressen berat ##ung der klein unter ##nehmen ##s ver ##walt ##ung ( s ##ba ) nicht se ##pera ##t vor ##gelegt , mein ##te ein board offizielle ##r [SEP] kuru ##l , s ##ba sert ##if ##ikal ##arın ##ı ver ##medi ve bunu de ##ger ##lendi ##rici ##nin of ##isin ##e b ##ır ##akt ##ı . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10128 17936 11357 39403 48141 113 170 114 10941 28304 51892 12436 10995 10268 15158 62183 10877 61001 26391 105258 60954 10716 10118 29185 11124 36771 10107 16719 57710 10716 113 187 10537 114 10726 10126 37097 10123 11190 32534 117 70273 10216 10290 17936 69573 10129 102 48350 10161 117 187 10537 38005 13918 52564 88638 10713 16719 96092 10323 79471 10104 11446 72384 30558 11802 10108 33560 10112 170 17145 45522 10713 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] schließlich scheint die post ##dic ##hte ein wichtiger ##er faktor fur die vers ##and ##kosten der ein ##heit ##sstraße zu sein als das volumen u ##ber die tat ##sa ##chliche ##n ent ##fern ##ungen in fra ##nk ##reich und den usa . [SEP] posta yo ##gun ##lug ##u ul ##ast ##ır ##ma mali ##yet ##lerinin buy ##uk ##lug ##une iki kat etki edi ##yor . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 19123 66373 10128 11841 55170 24819 10290 88239 10165 52221 61001 10128 12576 14752 92316 10118 10290 15543 75983 10304 11479 10223 10242 56889 189 12212 10128 22795 10466 83220 10115 61047 59798 13692 10106 10628 17761 31922 10130 10140 15610 119 102 29175 13672 32657 67381 10138 16600 15171 17145 10369 38981 33878 31268 47715 13013 67381 19659 12322 27689 47033 105861 26101 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] jede hoher ##e form des denken ##s , so beton ##te er , erscheint zuerst in der sozialen kommun ##ikat ##ion , zwischen dem kind und den vert ##rete ##rn seiner kultur , wenn sie eine gemeinsame aktiv ##itat aus ##uben . [SEP] ortak aktivite ##leri pay ##las ##mak daha us ##t du ##zey dus ##un ##mele ##ri pay ##las ##mak ici ##n yardım ##cı ol ##uy ##or . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 45278 61207 10112 12188 10139 91682 10107 117 10380 105375 10216 10163 117 47519 48676 10106 10118 64203 15484 49295 11046 117 11788 10268 22282 10130 10140 20900 34942 13061 11411 46774 117 16082 10632 10359 65823 22275 16313 10441 66918 119 102 95045 39720 12538 16868 13983 23892 13878 19626 10123 10168 68060 20239 11107 81612 10401 16868 13983 23892 42032 10115 79346 18311 30668 53452 10667 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 2 (id = 2)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] und es machte mir immer ##no ##ch ang ##st [SEP] sadece bir ##az ##cı ##k kor ##km ##ust ##um . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10130 10196 24607 36077 15967 10343 10269 10488 10562 102 39958 10561 16724 18311 10174 33705 23440 19265 10465 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "num warmup steps 160\n",
            "1607\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'out_dir_train_eng_pr2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 160.7, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4e4d94c208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "Beginning Training!\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into out_dir_train_eng_pr2/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-06-17T20:53:12Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from out_dir_train_eng_pr2/model.ckpt-0\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-06-17-20:53:17\n",
            "INFO:tensorflow:Saving dict for global step 0: eval_accuracy = 0.092, eval_loss = 1.5561895, global_step = 0, loss = 1.5562462\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 0: out_dir_train_eng_pr2/model.ckpt-0\n",
            "************EVAL ACC  0.09200000017881393*****************\n",
            "***********EVAL STEP COUNT 0****************\n",
            "INFO:tensorflow:loss = 1.4698446, step = 0\n",
            "INFO:tensorflow:global_step/sec: 1.32135\n",
            "INFO:tensorflow:loss = 1.1061138, step = 100 (67.554 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 161 into out_dir_train_eng_pr2/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-06-17T20:55:18Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from out_dir_train_eng_pr2/model.ckpt-161\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-06-17-20:55:24\n",
            "INFO:tensorflow:Saving dict for global step 161: eval_accuracy = 0.456, eval_loss = 1.1704673, global_step = 161, loss = 1.1696782\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 161: out_dir_train_eng_pr2/model.ckpt-161\n",
            "************EVAL ACC  0.4560000002384186*****************\n",
            "***********EVAL STEP COUNT 161****************\n",
            "INFO:tensorflow:global_step/sec: 1.28364\n",
            "INFO:tensorflow:loss = 1.0460231, step = 200 (77.904 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.10351\n",
            "INFO:tensorflow:loss = 1.1207392, step = 300 (47.539 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 322 into out_dir_train_eng_pr2/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-06-17T20:57:03Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from out_dir_train_eng_pr2/model.ckpt-322\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-06-17-20:57:08\n",
            "INFO:tensorflow:Saving dict for global step 322: eval_accuracy = 0.498, eval_loss = 1.1661189, global_step = 322, loss = 1.1649842\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 322: out_dir_train_eng_pr2/model.ckpt-322\n",
            "************EVAL ACC  0.49799999594688416*****************\n",
            "***********EVAL STEP COUNT 322****************\n",
            "INFO:tensorflow:global_step/sec: 1.32346\n",
            "INFO:tensorflow:loss = 0.9801122, step = 400 (75.559 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 483 into out_dir_train_eng_pr2/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-06-17T20:58:48Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from out_dir_train_eng_pr2/model.ckpt-483\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-06-17-20:58:53\n",
            "INFO:tensorflow:Saving dict for global step 483: eval_accuracy = 0.488, eval_loss = 1.2483411, global_step = 483, loss = 1.2464695\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 483: out_dir_train_eng_pr2/model.ckpt-483\n",
            "************EVAL ACC  0.4880000054836273*****************\n",
            "***********EVAL STEP COUNT 483****************\n",
            "INFO:tensorflow:global_step/sec: 1.32609\n",
            "INFO:tensorflow:loss = 0.5798545, step = 500 (75.410 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.10389\n",
            "INFO:tensorflow:loss = 0.5990144, step = 600 (47.531 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 644 into out_dir_train_eng_pr2/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-06-17T21:00:32Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from out_dir_train_eng_pr2/model.ckpt-644\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-06-17-21:00:37\n",
            "INFO:tensorflow:Saving dict for global step 644: eval_accuracy = 0.506, eval_loss = 1.41885, global_step = 644, loss = 1.4158916\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 644: out_dir_train_eng_pr2/model.ckpt-644\n",
            "************EVAL ACC  0.5059999823570251*****************\n",
            "***********EVAL STEP COUNT 644****************\n",
            "INFO:tensorflow:global_step/sec: 1.32441\n",
            "INFO:tensorflow:loss = 0.32298616, step = 700 (75.505 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.10524\n",
            "INFO:tensorflow:loss = 0.57163525, step = 800 (47.501 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 805 into out_dir_train_eng_pr2/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-06-17T21:02:17Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from out_dir_train_eng_pr2/model.ckpt-805\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-06-17-21:02:22\n",
            "INFO:tensorflow:Saving dict for global step 805: eval_accuracy = 0.494, eval_loss = 1.6509459, global_step = 805, loss = 1.647623\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 805: out_dir_train_eng_pr2/model.ckpt-805\n",
            "************EVAL ACC  0.49399998784065247*****************\n",
            "***********EVAL STEP COUNT 805****************\n",
            "INFO:tensorflow:global_step/sec: 1.31531\n",
            "INFO:tensorflow:loss = 0.2975803, step = 900 (76.028 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 966 into out_dir_train_eng_pr2/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-06-17T21:04:04Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from out_dir_train_eng_pr2/model.ckpt-966\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-06-17-21:04:09\n",
            "INFO:tensorflow:Saving dict for global step 966: eval_accuracy = 0.492, eval_loss = 1.7791896, global_step = 966, loss = 1.7755299\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 966: out_dir_train_eng_pr2/model.ckpt-966\n",
            "************EVAL ACC  0.492000013589859*****************\n",
            "***********EVAL STEP COUNT 966****************\n",
            "INFO:tensorflow:global_step/sec: 1.28029\n",
            "INFO:tensorflow:loss = 0.6029519, step = 1000 (78.108 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.10572\n",
            "INFO:tensorflow:loss = 0.2118324, step = 1100 (47.489 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1127 into out_dir_train_eng_pr2/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-06-17T21:05:53Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from out_dir_train_eng_pr2/model.ckpt-1127\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-06-17-21:05:58\n",
            "INFO:tensorflow:Saving dict for global step 1127: eval_accuracy = 0.498, eval_loss = 1.9190505, global_step = 1127, loss = 1.915371\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1127: out_dir_train_eng_pr2/model.ckpt-1127\n",
            "************EVAL ACC  0.49799999594688416*****************\n",
            "***********EVAL STEP COUNT 1127****************\n",
            "INFO:tensorflow:global_step/sec: 1.25466\n",
            "INFO:tensorflow:loss = 0.2676795, step = 1200 (79.703 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1288 into out_dir_train_eng_pr2/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-06-17T21:07:41Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from out_dir_train_eng_pr2/model.ckpt-1288\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-06-17-21:07:46\n",
            "INFO:tensorflow:Saving dict for global step 1288: eval_accuracy = 0.48, eval_loss = 2.0450857, global_step = 1288, loss = 2.0430117\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1288: out_dir_train_eng_pr2/model.ckpt-1288\n",
            "************EVAL ACC  0.47999998927116394*****************\n",
            "***********EVAL STEP COUNT 1288****************\n",
            "INFO:tensorflow:global_step/sec: 1.2699\n",
            "INFO:tensorflow:loss = 0.15505865, step = 1300 (78.746 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.10519\n",
            "INFO:tensorflow:loss = 0.09927767, step = 1400 (47.502 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1449 into out_dir_train_eng_pr2/model.ckpt.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-06-17T21:09:26Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from out_dir_train_eng_pr2/model.ckpt-1449\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-06-17-21:09:31\n",
            "INFO:tensorflow:Saving dict for global step 1449: eval_accuracy = 0.484, eval_loss = 2.207944, global_step = 1449, loss = 2.2053876\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1449: out_dir_train_eng_pr2/model.ckpt-1449\n",
            "************EVAL ACC  0.48399999737739563*****************\n",
            "***********EVAL STEP COUNT 1449****************\n",
            "INFO:tensorflow:global_step/sec: 1.30351\n",
            "INFO:tensorflow:loss = 0.07108705, step = 1500 (76.716 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.10445\n",
            "INFO:tensorflow:loss = 0.17116344, step = 1600 (47.518 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1610 into out_dir_train_eng_pr2/model.ckpt.\n",
            "stopped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1pYkArxza3e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "965d0c70-faf9-4caa-b7a3-bc6bb172b984"
      },
      "source": [
        "Test_batch_size = 500\n",
        "dummy = np.random.randn(Test_batch_size,768)\n",
        "act_lab, pred_lab = evaluate_and_get_hidden_context(estimator,input_fn_for_test = features['de-tr_test'], input_fn_for_hidden = features['de-tr_train'],is_progressive = True, hidden_context = dummy)\n",
        "\n",
        "#Classification Report\n",
        "target_names = ['Contradiction', 'Neutral', 'Entailment']\n",
        "print(classification_report(act_lab, pred_lab, target_names=target_names))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-06-17T19:04:17Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from out_dir_train_eng_pr2/model.ckpt-803\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2020-06-17-19:04:21\n",
            "INFO:tensorflow:Saving dict for global step 803: eval_accuracy = 0.49, eval_loss = 1.2571745, global_step = 803, loss = 1.2544389\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 803: out_dir_train_eng_pr2/model.ckpt-803\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-29aa7f7a56ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTest_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mact_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_lab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_and_get_hidden_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_fn_for_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'de-tr_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn_for_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'de-tr_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_progressive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Classification Report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d546b5e8983a>\u001b[0m in \u001b[0;36mevaluate_and_get_hidden_context\u001b[0;34m(estimator, input_fn_for_test, input_fn_for_hidden, is_progressive, hidden_context)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m       \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mactual_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    620\u001b[0m             input_fn, ModeKeys.PREDICT)\n\u001b[1;32m    621\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 622\u001b[0;31m             features, None, ModeKeys.PREDICT, self.config)\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m# Call to warm_start has to be after model_fn is called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-0dd89b3f3f4d>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    148\u001b[0m       output_spec = tf.estimator.EstimatorSpec(\n\u001b[1;32m    149\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m           predictions={\"probabilities\": probabilities,\"labels\": predicted_labels,\"kl-param\": KLD})\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/model_fn.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, mode, predictions, loss, train_op, eval_metric_ops, export_outputs, training_chief_hooks, training_hooks, scaffold, evaluation_hooks, prediction_hooks)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_estimator_spec_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     export_outputs = _validate_estimator_spec_export_outputs(\n\u001b[0;32m--> 167\u001b[0;31m         export_outputs, predictions, mode)\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mtraining_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_estimator_spec_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mevaluation_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_estimator_spec_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/model_fn.py\u001b[0m in \u001b[0;36m_validate_estimator_spec_export_outputs\u001b[0;34m(export_outputs, predictions, mode)\u001b[0m\n\u001b[1;32m    418\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     export_outputs = export_utils.get_export_outputs(export_outputs,\n\u001b[0;32m--> 420\u001b[0;31m                                                      predictions)\n\u001b[0m\u001b[1;32m    421\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexport_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/model_utils/export_utils.py\u001b[0m in \u001b[0;36mget_export_outputs\u001b[0;34m(export_outputs, predictions)\u001b[0m\n\u001b[1;32m    307\u001b[0m   \"\"\"\n\u001b[1;32m    308\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mexport_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mdefault_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexport_output_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredictOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     export_outputs = {\n\u001b[1;32m    311\u001b[0m         signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: default_output}\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/model_utils/export_output.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     self._outputs = self._wrap_and_check_outputs(\n\u001b[0;32m--> 224\u001b[0;31m         outputs, self._SINGLE_OUTPUT_DEFAULT_NAME, error_label='Prediction')\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/model_utils/export_output.py\u001b[0m in \u001b[0;36m_wrap_and_check_outputs\u001b[0;34m(self, outputs, single_output_default_name, error_label)\u001b[0m\n\u001b[1;32m     94\u001b[0m         raise ValueError(\n\u001b[1;32m     95\u001b[0m             '{} output value must be a Tensor; got {}.'.format(\n\u001b[0;32m---> 96\u001b[0;31m                 error_name, value))\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m       \u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Prediction output value must be a Tensor; got <tf.Variable 'KLD:0' shape=() dtype=float32_ref>."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMg-oCWVVD7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}